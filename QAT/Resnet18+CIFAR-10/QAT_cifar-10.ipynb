{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.quantization import fuse_modules\n",
    "from torch.nn.quantized import FloatFunctional\n",
    "from torch import Tensor\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from resnet import resnet18\n",
    "\n",
    "# Set up warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.ao.quantization'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(random_seed=0):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform) \n",
    "    # We will use test set for validation and test in this project.\n",
    "    # Do not use test set for validation in practice!\n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=train_batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, criterion=None):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    eval_loss = running_loss / len(test_loader.dataset)\n",
    "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    return eval_loss, eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=200):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
    "    print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(-1, eval_loss, eval_accuracy))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
    "\n",
    "        # Set learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _ = model(inputs)\n",
    "\n",
    "def measure_inference_latency(model,\n",
    "                              device,\n",
    "                              input_size=(1, 3, 32, 32),\n",
    "                              num_samples=100,\n",
    "                              num_warmups=10):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmups):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_samples):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_ave = elapsed_time / num_samples\n",
    "\n",
    "    return elapsed_time_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.save(model.state_dict(), model_filepath)\n",
    "\n",
    "def load_model(model, model_filepath, device):\n",
    "\n",
    "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torchscript_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
    "    \n",
    "def load_torchscript_model(model_filepath, device):\n",
    "\n",
    "    model = torch.jit.load(model_filepath, map_location=device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes=10):\n",
    "\n",
    "    model = resnet18(num_classes=num_classes, pretrained=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedResNet18(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedResNet18, self).__init__()\n",
    "        \n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        self.model_fp32 = model_fp32\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
    "\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "num_classes = 10\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "model_dir = \"saved_models\"\n",
    "model_filename = \"resnet18_cifar10.pt\"\n",
    "quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
    "set_random_seeds(random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model...\n",
      "Epoch: -1 Eval Loss: 2.325 Eval Acc: 0.098\n",
      "Epoch: 000 Train Loss: 2.127 Train Acc: 0.275 Eval Loss: 1.653 Eval Acc: 0.397\n",
      "Epoch: 001 Train Loss: 1.553 Train Acc: 0.423 Eval Loss: 1.374 Eval Acc: 0.494\n",
      "Epoch: 002 Train Loss: 1.385 Train Acc: 0.496 Eval Loss: 1.245 Eval Acc: 0.543\n",
      "Epoch: 003 Train Loss: 1.232 Train Acc: 0.552 Eval Loss: 1.144 Eval Acc: 0.594\n",
      "Epoch: 004 Train Loss: 1.121 Train Acc: 0.600 Eval Loss: 1.030 Eval Acc: 0.634\n",
      "Epoch: 005 Train Loss: 1.027 Train Acc: 0.635 Eval Loss: 0.998 Eval Acc: 0.660\n",
      "Epoch: 006 Train Loss: 0.968 Train Acc: 0.658 Eval Loss: 0.958 Eval Acc: 0.662\n",
      "Epoch: 007 Train Loss: 0.910 Train Acc: 0.679 Eval Loss: 0.875 Eval Acc: 0.700\n",
      "Epoch: 008 Train Loss: 0.869 Train Acc: 0.696 Eval Loss: 0.885 Eval Acc: 0.696\n",
      "Epoch: 009 Train Loss: 0.821 Train Acc: 0.713 Eval Loss: 0.829 Eval Acc: 0.715\n",
      "Epoch: 010 Train Loss: 0.783 Train Acc: 0.726 Eval Loss: 0.839 Eval Acc: 0.720\n",
      "Epoch: 011 Train Loss: 0.756 Train Acc: 0.738 Eval Loss: 0.788 Eval Acc: 0.729\n",
      "Epoch: 012 Train Loss: 0.721 Train Acc: 0.750 Eval Loss: 0.699 Eval Acc: 0.760\n",
      "Epoch: 013 Train Loss: 0.701 Train Acc: 0.758 Eval Loss: 0.712 Eval Acc: 0.754\n",
      "Epoch: 014 Train Loss: 0.679 Train Acc: 0.764 Eval Loss: 0.710 Eval Acc: 0.755\n",
      "Epoch: 015 Train Loss: 0.659 Train Acc: 0.770 Eval Loss: 0.728 Eval Acc: 0.747\n",
      "Epoch: 016 Train Loss: 0.640 Train Acc: 0.779 Eval Loss: 0.662 Eval Acc: 0.777\n",
      "Epoch: 017 Train Loss: 0.625 Train Acc: 0.784 Eval Loss: 0.696 Eval Acc: 0.764\n",
      "Epoch: 018 Train Loss: 0.612 Train Acc: 0.786 Eval Loss: 0.675 Eval Acc: 0.766\n",
      "Epoch: 019 Train Loss: 0.597 Train Acc: 0.795 Eval Loss: 0.689 Eval Acc: 0.764\n",
      "Epoch: 020 Train Loss: 0.586 Train Acc: 0.795 Eval Loss: 0.653 Eval Acc: 0.784\n",
      "Epoch: 021 Train Loss: 0.570 Train Acc: 0.803 Eval Loss: 0.610 Eval Acc: 0.794\n",
      "Epoch: 022 Train Loss: 0.561 Train Acc: 0.804 Eval Loss: 0.649 Eval Acc: 0.778\n",
      "Epoch: 023 Train Loss: 0.555 Train Acc: 0.808 Eval Loss: 0.630 Eval Acc: 0.783\n",
      "Epoch: 024 Train Loss: 0.542 Train Acc: 0.813 Eval Loss: 0.629 Eval Acc: 0.783\n",
      "Epoch: 025 Train Loss: 0.536 Train Acc: 0.814 Eval Loss: 0.601 Eval Acc: 0.795\n",
      "Epoch: 026 Train Loss: 0.529 Train Acc: 0.817 Eval Loss: 0.620 Eval Acc: 0.787\n",
      "Epoch: 027 Train Loss: 0.520 Train Acc: 0.819 Eval Loss: 0.649 Eval Acc: 0.783\n",
      "Epoch: 028 Train Loss: 0.508 Train Acc: 0.824 Eval Loss: 0.608 Eval Acc: 0.798\n",
      "Epoch: 029 Train Loss: 0.511 Train Acc: 0.823 Eval Loss: 0.623 Eval Acc: 0.796\n",
      "Epoch: 030 Train Loss: 0.501 Train Acc: 0.825 Eval Loss: 0.599 Eval Acc: 0.798\n",
      "Epoch: 031 Train Loss: 0.496 Train Acc: 0.827 Eval Loss: 0.626 Eval Acc: 0.787\n",
      "Epoch: 032 Train Loss: 0.485 Train Acc: 0.831 Eval Loss: 0.661 Eval Acc: 0.792\n",
      "Epoch: 033 Train Loss: 0.474 Train Acc: 0.835 Eval Loss: 0.577 Eval Acc: 0.809\n",
      "Epoch: 034 Train Loss: 0.480 Train Acc: 0.832 Eval Loss: 0.597 Eval Acc: 0.803\n",
      "Epoch: 035 Train Loss: 0.472 Train Acc: 0.837 Eval Loss: 0.570 Eval Acc: 0.810\n",
      "Epoch: 036 Train Loss: 0.462 Train Acc: 0.840 Eval Loss: 0.561 Eval Acc: 0.814\n",
      "Epoch: 037 Train Loss: 0.462 Train Acc: 0.839 Eval Loss: 0.587 Eval Acc: 0.809\n",
      "Epoch: 038 Train Loss: 0.455 Train Acc: 0.842 Eval Loss: 0.535 Eval Acc: 0.819\n",
      "Epoch: 039 Train Loss: 0.452 Train Acc: 0.843 Eval Loss: 0.594 Eval Acc: 0.808\n",
      "Epoch: 040 Train Loss: 0.451 Train Acc: 0.843 Eval Loss: 0.578 Eval Acc: 0.805\n",
      "Epoch: 041 Train Loss: 0.442 Train Acc: 0.846 Eval Loss: 0.556 Eval Acc: 0.812\n",
      "Epoch: 042 Train Loss: 0.443 Train Acc: 0.847 Eval Loss: 0.558 Eval Acc: 0.815\n",
      "Epoch: 043 Train Loss: 0.435 Train Acc: 0.849 Eval Loss: 0.573 Eval Acc: 0.813\n",
      "Epoch: 044 Train Loss: 0.431 Train Acc: 0.849 Eval Loss: 0.589 Eval Acc: 0.807\n",
      "Epoch: 045 Train Loss: 0.430 Train Acc: 0.852 Eval Loss: 0.595 Eval Acc: 0.803\n",
      "Epoch: 046 Train Loss: 0.427 Train Acc: 0.852 Eval Loss: 0.558 Eval Acc: 0.815\n",
      "Epoch: 047 Train Loss: 0.423 Train Acc: 0.854 Eval Loss: 0.567 Eval Acc: 0.810\n",
      "Epoch: 048 Train Loss: 0.420 Train Acc: 0.855 Eval Loss: 0.591 Eval Acc: 0.810\n",
      "Epoch: 049 Train Loss: 0.411 Train Acc: 0.858 Eval Loss: 0.591 Eval Acc: 0.810\n",
      "Epoch: 050 Train Loss: 0.417 Train Acc: 0.856 Eval Loss: 0.544 Eval Acc: 0.818\n",
      "Epoch: 051 Train Loss: 0.409 Train Acc: 0.858 Eval Loss: 0.564 Eval Acc: 0.818\n",
      "Epoch: 052 Train Loss: 0.407 Train Acc: 0.857 Eval Loss: 0.557 Eval Acc: 0.814\n",
      "Epoch: 053 Train Loss: 0.410 Train Acc: 0.858 Eval Loss: 0.567 Eval Acc: 0.816\n",
      "Epoch: 054 Train Loss: 0.410 Train Acc: 0.859 Eval Loss: 0.596 Eval Acc: 0.807\n",
      "Epoch: 055 Train Loss: 0.403 Train Acc: 0.859 Eval Loss: 0.550 Eval Acc: 0.821\n",
      "Epoch: 056 Train Loss: 0.398 Train Acc: 0.861 Eval Loss: 0.584 Eval Acc: 0.811\n",
      "Epoch: 057 Train Loss: 0.397 Train Acc: 0.862 Eval Loss: 0.579 Eval Acc: 0.818\n",
      "Epoch: 058 Train Loss: 0.392 Train Acc: 0.862 Eval Loss: 0.539 Eval Acc: 0.820\n",
      "Epoch: 059 Train Loss: 0.393 Train Acc: 0.863 Eval Loss: 0.554 Eval Acc: 0.819\n",
      "Epoch: 060 Train Loss: 0.387 Train Acc: 0.867 Eval Loss: 0.561 Eval Acc: 0.822\n",
      "Epoch: 061 Train Loss: 0.390 Train Acc: 0.866 Eval Loss: 0.572 Eval Acc: 0.816\n",
      "Epoch: 062 Train Loss: 0.391 Train Acc: 0.864 Eval Loss: 0.573 Eval Acc: 0.811\n",
      "Epoch: 063 Train Loss: 0.384 Train Acc: 0.866 Eval Loss: 0.553 Eval Acc: 0.823\n",
      "Epoch: 064 Train Loss: 0.379 Train Acc: 0.867 Eval Loss: 0.581 Eval Acc: 0.811\n",
      "Epoch: 065 Train Loss: 0.382 Train Acc: 0.869 Eval Loss: 0.571 Eval Acc: 0.820\n",
      "Epoch: 066 Train Loss: 0.376 Train Acc: 0.870 Eval Loss: 0.588 Eval Acc: 0.815\n",
      "Epoch: 067 Train Loss: 0.375 Train Acc: 0.870 Eval Loss: 0.563 Eval Acc: 0.822\n",
      "Epoch: 068 Train Loss: 0.376 Train Acc: 0.869 Eval Loss: 0.539 Eval Acc: 0.827\n",
      "Epoch: 069 Train Loss: 0.374 Train Acc: 0.870 Eval Loss: 0.539 Eval Acc: 0.828\n",
      "Epoch: 070 Train Loss: 0.377 Train Acc: 0.868 Eval Loss: 0.581 Eval Acc: 0.809\n",
      "Epoch: 071 Train Loss: 0.368 Train Acc: 0.871 Eval Loss: 0.608 Eval Acc: 0.809\n",
      "Epoch: 072 Train Loss: 0.372 Train Acc: 0.871 Eval Loss: 0.536 Eval Acc: 0.824\n",
      "Epoch: 073 Train Loss: 0.366 Train Acc: 0.873 Eval Loss: 0.581 Eval Acc: 0.820\n",
      "Epoch: 074 Train Loss: 0.368 Train Acc: 0.874 Eval Loss: 0.607 Eval Acc: 0.810\n",
      "Epoch: 075 Train Loss: 0.364 Train Acc: 0.873 Eval Loss: 0.595 Eval Acc: 0.809\n",
      "Epoch: 076 Train Loss: 0.364 Train Acc: 0.873 Eval Loss: 0.555 Eval Acc: 0.821\n",
      "Epoch: 077 Train Loss: 0.360 Train Acc: 0.874 Eval Loss: 0.569 Eval Acc: 0.821\n",
      "Epoch: 078 Train Loss: 0.362 Train Acc: 0.875 Eval Loss: 0.583 Eval Acc: 0.816\n",
      "Epoch: 079 Train Loss: 0.356 Train Acc: 0.876 Eval Loss: 0.525 Eval Acc: 0.833\n",
      "Epoch: 080 Train Loss: 0.360 Train Acc: 0.874 Eval Loss: 0.557 Eval Acc: 0.822\n",
      "Epoch: 081 Train Loss: 0.357 Train Acc: 0.874 Eval Loss: 0.570 Eval Acc: 0.818\n",
      "Epoch: 082 Train Loss: 0.361 Train Acc: 0.873 Eval Loss: 0.535 Eval Acc: 0.826\n",
      "Epoch: 083 Train Loss: 0.355 Train Acc: 0.877 Eval Loss: 0.564 Eval Acc: 0.821\n",
      "Epoch: 084 Train Loss: 0.348 Train Acc: 0.879 Eval Loss: 0.569 Eval Acc: 0.818\n",
      "Epoch: 085 Train Loss: 0.353 Train Acc: 0.876 Eval Loss: 0.669 Eval Acc: 0.800\n",
      "Epoch: 086 Train Loss: 0.351 Train Acc: 0.877 Eval Loss: 0.584 Eval Acc: 0.823\n",
      "Epoch: 087 Train Loss: 0.348 Train Acc: 0.878 Eval Loss: 0.549 Eval Acc: 0.827\n",
      "Epoch: 088 Train Loss: 0.354 Train Acc: 0.876 Eval Loss: 0.569 Eval Acc: 0.822\n",
      "Epoch: 089 Train Loss: 0.346 Train Acc: 0.881 Eval Loss: 0.570 Eval Acc: 0.823\n",
      "Epoch: 090 Train Loss: 0.348 Train Acc: 0.879 Eval Loss: 0.593 Eval Acc: 0.809\n",
      "Epoch: 091 Train Loss: 0.342 Train Acc: 0.881 Eval Loss: 0.581 Eval Acc: 0.813\n",
      "Epoch: 092 Train Loss: 0.344 Train Acc: 0.879 Eval Loss: 0.558 Eval Acc: 0.825\n",
      "Epoch: 093 Train Loss: 0.344 Train Acc: 0.879 Eval Loss: 0.513 Eval Acc: 0.832\n",
      "Epoch: 094 Train Loss: 0.344 Train Acc: 0.880 Eval Loss: 0.550 Eval Acc: 0.822\n",
      "Epoch: 095 Train Loss: 0.340 Train Acc: 0.881 Eval Loss: 0.631 Eval Acc: 0.810\n",
      "Epoch: 096 Train Loss: 0.341 Train Acc: 0.881 Eval Loss: 0.588 Eval Acc: 0.817\n",
      "Epoch: 097 Train Loss: 0.342 Train Acc: 0.880 Eval Loss: 0.610 Eval Acc: 0.813\n",
      "Epoch: 098 Train Loss: 0.342 Train Acc: 0.882 Eval Loss: 0.579 Eval Acc: 0.818\n",
      "Epoch: 099 Train Loss: 0.336 Train Acc: 0.882 Eval Loss: 0.590 Eval Acc: 0.819\n",
      "Epoch: 100 Train Loss: 0.224 Train Acc: 0.923 Eval Loss: 0.452 Eval Acc: 0.863\n",
      "Epoch: 101 Train Loss: 0.183 Train Acc: 0.937 Eval Loss: 0.459 Eval Acc: 0.866\n",
      "Epoch: 102 Train Loss: 0.168 Train Acc: 0.942 Eval Loss: 0.458 Eval Acc: 0.865\n",
      "Epoch: 103 Train Loss: 0.156 Train Acc: 0.946 Eval Loss: 0.466 Eval Acc: 0.866\n",
      "Epoch: 104 Train Loss: 0.151 Train Acc: 0.948 Eval Loss: 0.470 Eval Acc: 0.866\n",
      "Epoch: 105 Train Loss: 0.139 Train Acc: 0.951 Eval Loss: 0.470 Eval Acc: 0.870\n",
      "Epoch: 106 Train Loss: 0.139 Train Acc: 0.952 Eval Loss: 0.470 Eval Acc: 0.870\n",
      "Epoch: 107 Train Loss: 0.128 Train Acc: 0.955 Eval Loss: 0.477 Eval Acc: 0.870\n",
      "Epoch: 108 Train Loss: 0.127 Train Acc: 0.956 Eval Loss: 0.471 Eval Acc: 0.872\n",
      "Epoch: 109 Train Loss: 0.123 Train Acc: 0.957 Eval Loss: 0.473 Eval Acc: 0.871\n",
      "Epoch: 110 Train Loss: 0.118 Train Acc: 0.959 Eval Loss: 0.485 Eval Acc: 0.871\n",
      "Epoch: 111 Train Loss: 0.115 Train Acc: 0.960 Eval Loss: 0.489 Eval Acc: 0.869\n",
      "Epoch: 112 Train Loss: 0.111 Train Acc: 0.961 Eval Loss: 0.491 Eval Acc: 0.871\n",
      "Epoch: 113 Train Loss: 0.105 Train Acc: 0.963 Eval Loss: 0.487 Eval Acc: 0.873\n",
      "Epoch: 114 Train Loss: 0.106 Train Acc: 0.963 Eval Loss: 0.496 Eval Acc: 0.871\n",
      "Epoch: 115 Train Loss: 0.101 Train Acc: 0.964 Eval Loss: 0.495 Eval Acc: 0.871\n",
      "Epoch: 116 Train Loss: 0.098 Train Acc: 0.965 Eval Loss: 0.507 Eval Acc: 0.868\n",
      "Epoch: 117 Train Loss: 0.098 Train Acc: 0.966 Eval Loss: 0.516 Eval Acc: 0.870\n",
      "Epoch: 118 Train Loss: 0.097 Train Acc: 0.966 Eval Loss: 0.511 Eval Acc: 0.870\n",
      "Epoch: 119 Train Loss: 0.091 Train Acc: 0.968 Eval Loss: 0.516 Eval Acc: 0.870\n",
      "Epoch: 120 Train Loss: 0.091 Train Acc: 0.968 Eval Loss: 0.510 Eval Acc: 0.872\n",
      "Epoch: 121 Train Loss: 0.089 Train Acc: 0.969 Eval Loss: 0.522 Eval Acc: 0.867\n",
      "Epoch: 122 Train Loss: 0.086 Train Acc: 0.970 Eval Loss: 0.533 Eval Acc: 0.870\n",
      "Epoch: 123 Train Loss: 0.084 Train Acc: 0.970 Eval Loss: 0.537 Eval Acc: 0.869\n",
      "Epoch: 124 Train Loss: 0.084 Train Acc: 0.971 Eval Loss: 0.540 Eval Acc: 0.867\n",
      "Epoch: 125 Train Loss: 0.082 Train Acc: 0.971 Eval Loss: 0.534 Eval Acc: 0.870\n",
      "Epoch: 126 Train Loss: 0.078 Train Acc: 0.972 Eval Loss: 0.548 Eval Acc: 0.868\n",
      "Epoch: 127 Train Loss: 0.078 Train Acc: 0.973 Eval Loss: 0.548 Eval Acc: 0.870\n",
      "Epoch: 128 Train Loss: 0.075 Train Acc: 0.974 Eval Loss: 0.550 Eval Acc: 0.868\n",
      "Epoch: 129 Train Loss: 0.075 Train Acc: 0.974 Eval Loss: 0.556 Eval Acc: 0.868\n",
      "Epoch: 130 Train Loss: 0.071 Train Acc: 0.975 Eval Loss: 0.562 Eval Acc: 0.868\n",
      "Epoch: 131 Train Loss: 0.070 Train Acc: 0.976 Eval Loss: 0.569 Eval Acc: 0.869\n",
      "Epoch: 132 Train Loss: 0.072 Train Acc: 0.975 Eval Loss: 0.560 Eval Acc: 0.872\n",
      "Epoch: 133 Train Loss: 0.069 Train Acc: 0.975 Eval Loss: 0.577 Eval Acc: 0.867\n",
      "Epoch: 134 Train Loss: 0.066 Train Acc: 0.977 Eval Loss: 0.570 Eval Acc: 0.867\n",
      "Epoch: 135 Train Loss: 0.066 Train Acc: 0.976 Eval Loss: 0.573 Eval Acc: 0.866\n",
      "Epoch: 136 Train Loss: 0.064 Train Acc: 0.978 Eval Loss: 0.566 Eval Acc: 0.871\n",
      "Epoch: 137 Train Loss: 0.066 Train Acc: 0.977 Eval Loss: 0.580 Eval Acc: 0.864\n",
      "Epoch: 138 Train Loss: 0.066 Train Acc: 0.977 Eval Loss: 0.605 Eval Acc: 0.864\n",
      "Epoch: 139 Train Loss: 0.064 Train Acc: 0.978 Eval Loss: 0.575 Eval Acc: 0.868\n",
      "Epoch: 140 Train Loss: 0.062 Train Acc: 0.978 Eval Loss: 0.589 Eval Acc: 0.864\n",
      "Epoch: 141 Train Loss: 0.067 Train Acc: 0.976 Eval Loss: 0.591 Eval Acc: 0.863\n",
      "Epoch: 142 Train Loss: 0.062 Train Acc: 0.978 Eval Loss: 0.584 Eval Acc: 0.866\n",
      "Epoch: 143 Train Loss: 0.059 Train Acc: 0.980 Eval Loss: 0.595 Eval Acc: 0.866\n",
      "Epoch: 144 Train Loss: 0.061 Train Acc: 0.979 Eval Loss: 0.608 Eval Acc: 0.867\n",
      "Epoch: 145 Train Loss: 0.058 Train Acc: 0.979 Eval Loss: 0.607 Eval Acc: 0.865\n",
      "Epoch: 146 Train Loss: 0.059 Train Acc: 0.979 Eval Loss: 0.587 Eval Acc: 0.869\n",
      "Epoch: 147 Train Loss: 0.059 Train Acc: 0.980 Eval Loss: 0.602 Eval Acc: 0.867\n",
      "Epoch: 148 Train Loss: 0.058 Train Acc: 0.979 Eval Loss: 0.598 Eval Acc: 0.867\n",
      "Epoch: 149 Train Loss: 0.057 Train Acc: 0.980 Eval Loss: 0.594 Eval Acc: 0.868\n",
      "Epoch: 150 Train Loss: 0.046 Train Acc: 0.984 Eval Loss: 0.581 Eval Acc: 0.870\n",
      "Epoch: 151 Train Loss: 0.041 Train Acc: 0.986 Eval Loss: 0.579 Eval Acc: 0.872\n",
      "Epoch: 152 Train Loss: 0.041 Train Acc: 0.986 Eval Loss: 0.574 Eval Acc: 0.872\n",
      "Epoch: 153 Train Loss: 0.038 Train Acc: 0.987 Eval Loss: 0.577 Eval Acc: 0.873\n",
      "Epoch: 154 Train Loss: 0.038 Train Acc: 0.987 Eval Loss: 0.578 Eval Acc: 0.872\n",
      "Epoch: 155 Train Loss: 0.036 Train Acc: 0.988 Eval Loss: 0.581 Eval Acc: 0.873\n",
      "Epoch: 156 Train Loss: 0.035 Train Acc: 0.988 Eval Loss: 0.582 Eval Acc: 0.872\n",
      "Epoch: 157 Train Loss: 0.037 Train Acc: 0.988 Eval Loss: 0.586 Eval Acc: 0.873\n",
      "Epoch: 158 Train Loss: 0.034 Train Acc: 0.989 Eval Loss: 0.584 Eval Acc: 0.873\n",
      "Epoch: 159 Train Loss: 0.034 Train Acc: 0.989 Eval Loss: 0.580 Eval Acc: 0.875\n",
      "Epoch: 160 Train Loss: 0.035 Train Acc: 0.988 Eval Loss: 0.585 Eval Acc: 0.875\n",
      "Epoch: 161 Train Loss: 0.033 Train Acc: 0.989 Eval Loss: 0.589 Eval Acc: 0.874\n",
      "Epoch: 162 Train Loss: 0.032 Train Acc: 0.989 Eval Loss: 0.587 Eval Acc: 0.875\n",
      "Epoch: 163 Train Loss: 0.033 Train Acc: 0.989 Eval Loss: 0.587 Eval Acc: 0.875\n",
      "Epoch: 164 Train Loss: 0.032 Train Acc: 0.989 Eval Loss: 0.591 Eval Acc: 0.873\n",
      "Epoch: 165 Train Loss: 0.031 Train Acc: 0.990 Eval Loss: 0.596 Eval Acc: 0.874\n",
      "Epoch: 166 Train Loss: 0.033 Train Acc: 0.989 Eval Loss: 0.598 Eval Acc: 0.874\n",
      "Epoch: 167 Train Loss: 0.030 Train Acc: 0.990 Eval Loss: 0.596 Eval Acc: 0.875\n",
      "Epoch: 168 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.599 Eval Acc: 0.876\n",
      "Epoch: 169 Train Loss: 0.030 Train Acc: 0.990 Eval Loss: 0.599 Eval Acc: 0.875\n",
      "Epoch: 170 Train Loss: 0.031 Train Acc: 0.990 Eval Loss: 0.598 Eval Acc: 0.875\n",
      "Epoch: 171 Train Loss: 0.030 Train Acc: 0.990 Eval Loss: 0.602 Eval Acc: 0.876\n",
      "Epoch: 172 Train Loss: 0.030 Train Acc: 0.990 Eval Loss: 0.603 Eval Acc: 0.874\n",
      "Epoch: 173 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.606 Eval Acc: 0.874\n",
      "Epoch: 174 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.606 Eval Acc: 0.874\n",
      "Epoch: 175 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.604 Eval Acc: 0.876\n",
      "Epoch: 176 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.606 Eval Acc: 0.876\n",
      "Epoch: 177 Train Loss: 0.031 Train Acc: 0.990 Eval Loss: 0.609 Eval Acc: 0.876\n",
      "Epoch: 178 Train Loss: 0.028 Train Acc: 0.991 Eval Loss: 0.610 Eval Acc: 0.874\n",
      "Epoch: 179 Train Loss: 0.029 Train Acc: 0.991 Eval Loss: 0.613 Eval Acc: 0.874\n",
      "Epoch: 180 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.612 Eval Acc: 0.875\n",
      "Epoch: 181 Train Loss: 0.028 Train Acc: 0.991 Eval Loss: 0.613 Eval Acc: 0.874\n",
      "Epoch: 182 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.615 Eval Acc: 0.876\n",
      "Epoch: 183 Train Loss: 0.028 Train Acc: 0.991 Eval Loss: 0.618 Eval Acc: 0.874\n",
      "Epoch: 184 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.614 Eval Acc: 0.875\n",
      "Epoch: 185 Train Loss: 0.026 Train Acc: 0.992 Eval Loss: 0.617 Eval Acc: 0.875\n",
      "Epoch: 186 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.623 Eval Acc: 0.875\n",
      "Epoch: 187 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.626 Eval Acc: 0.875\n",
      "Epoch: 188 Train Loss: 0.026 Train Acc: 0.991 Eval Loss: 0.631 Eval Acc: 0.874\n",
      "Epoch: 189 Train Loss: 0.025 Train Acc: 0.991 Eval Loss: 0.625 Eval Acc: 0.874\n",
      "Epoch: 190 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.626 Eval Acc: 0.874\n",
      "Epoch: 191 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.638 Eval Acc: 0.873\n",
      "Epoch: 192 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.630 Eval Acc: 0.874\n",
      "Epoch: 193 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.628 Eval Acc: 0.874\n",
      "Epoch: 194 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.629 Eval Acc: 0.875\n",
      "Epoch: 195 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.635 Eval Acc: 0.875\n",
      "Epoch: 196 Train Loss: 0.025 Train Acc: 0.991 Eval Loss: 0.632 Eval Acc: 0.876\n",
      "Epoch: 197 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.635 Eval Acc: 0.873\n",
      "Epoch: 198 Train Loss: 0.025 Train Acc: 0.991 Eval Loss: 0.634 Eval Acc: 0.874\n",
      "Epoch: 199 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.636 Eval Acc: 0.874\n"
     ]
    }
   ],
   "source": [
    "# Create an untrained model.\n",
    "model = create_model(num_classes=num_classes)\n",
    "\n",
    "print(\"Training Model...\")\n",
    "model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-1, num_epochs=200)\n",
    "\n",
    "# Save model.\n",
    "save_model(model=model, model_dir=model_dir, model_filename=model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. floating point 타입으로 모델을 학습하거나 pre-trained 모델 불러오기\n",
    "model = load_model(model=create_model(num_classes=num_classes), model_filepath=model_filepath, device=cuda_device)\n",
    "\n",
    "# 2. 모델을 CPU 상태로 두고 학습 모드로 변환\n",
    "model.to(cpu_device)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a copy of the model for layer fusion\n",
    "fused_model = copy.deepcopy(model)\n",
    "\n",
    "# The model has to be switched to training mode before any layer fusion.\n",
    "# Otherwise the quantization aware training will not work correctly.\n",
    "fused_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. layer fusion 적용\n",
    "# Fuse the model in place rather manually.\n",
    "\n",
    "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
    "for module_name, module in fused_model.named_children():\n",
    "    if \"layer\" in module_name:\n",
    "        for basic_block_name, basic_block in module.named_children():\n",
    "            torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
    "            for sub_block_name, sub_block in basic_block.named_children():\n",
    "                if sub_block_name == \"downsample\":\n",
    "                    torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 모델을 평가 모드로 변환 후 layer fusion이 잘 적용되었는지 확인\n",
    "\n",
    "model.eval()\n",
    "fused_model.eval()\n",
    "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devLupin\\Miniconda3\\envs\\qat_env\\lib\\site-packages\\torch\\quantization\\observer.py:121: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedResNet18(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (model_fp32): ResNet(\n",
       "    (conv1): ConvBnReLU2d(\n",
       "      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "    (bn1): Identity()\n",
       "    (relu): Identity()\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(\n",
       "      in_features=512, out_features=10, bias=True\n",
       "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. input에는 torch.quantization.QuantStub() 적용\n",
    "#    output에는 torch.quantization.DeQuantStub() 적용\n",
    "\n",
    "quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
    "\n",
    "# 6. quantization configuration 지정\n",
    "quantized_model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "\n",
    "torch.quantization.prepare_qat(quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training QAT Model...\n",
      "Epoch: -1 Eval Loss: 0.636 Eval Acc: 0.874\n",
      "Epoch: 000 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.636 Eval Acc: 0.874\n",
      "Epoch: 001 Train Loss: 0.025 Train Acc: 0.991 Eval Loss: 0.642 Eval Acc: 0.873\n",
      "Epoch: 002 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.638 Eval Acc: 0.873\n",
      "Epoch: 003 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.642 Eval Acc: 0.874\n",
      "Epoch: 004 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.647 Eval Acc: 0.872\n",
      "Epoch: 005 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.643 Eval Acc: 0.874\n",
      "Epoch: 006 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.646 Eval Acc: 0.874\n",
      "Epoch: 007 Train Loss: 0.023 Train Acc: 0.992 Eval Loss: 0.647 Eval Acc: 0.874\n",
      "Epoch: 008 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.645 Eval Acc: 0.875\n",
      "Epoch: 009 Train Loss: 0.023 Train Acc: 0.992 Eval Loss: 0.648 Eval Acc: 0.875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedResNet18(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (model_fp32): ResNet(\n",
       "    (conv1): ConvBnReLU2d(\n",
       "      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "        min_val=tensor([-2.4343e-02, -5.5356e-02, -1.6924e-01, -2.2380e-02, -1.0226e-02,\n",
       "                -2.5949e-01, -1.4813e-02, -3.3876e-01, -1.4831e-01, -2.5777e-02,\n",
       "                -1.2778e-01, -1.2449e-01, -6.8747e-03, -2.0834e-02, -3.2141e-01,\n",
       "                -6.0829e-02, -1.1857e-01, -3.3610e-01, -3.3183e-02, -7.0651e-02,\n",
       "                -1.4683e-05, -1.4628e-01, -3.6514e-01, -8.4780e-02, -2.1904e-01,\n",
       "                -2.5619e-01, -7.9354e-02, -2.3710e-01, -5.7769e-02, -1.0227e-03,\n",
       "                -5.1797e-02, -3.8092e-02, -1.1359e-01, -8.0964e-02, -1.6891e-02,\n",
       "                -1.3513e-01, -2.3006e-01, -1.0520e-01, -3.7972e-04, -9.7140e-04,\n",
       "                -4.8827e-02, -2.3777e-01, -9.1403e-03, -1.2513e-01, -3.4156e-01,\n",
       "                -1.5833e-01, -3.9198e-01, -1.8174e-01, -8.4873e-02, -2.6192e-04,\n",
       "                -9.2009e-02, -3.6104e-05, -1.4636e-01, -3.2934e-01, -6.1371e-02,\n",
       "                -1.7374e-02, -8.6006e-02, -3.8384e-01, -9.1473e-02, -1.0461e-01,\n",
       "                -4.8237e-03, -3.5448e-02, -1.8701e-01, -2.9669e-01], device='cuda:0'), max_val=tensor([3.6786e-02, 4.5084e-02, 2.0956e-01, 3.1291e-02, 1.6758e-02, 1.2168e-01,\n",
       "                2.2538e-02, 3.3592e-01, 1.1723e-01, 5.1752e-02, 9.8125e-02, 1.4418e-01,\n",
       "                3.4087e-03, 1.4910e-02, 3.5002e-01, 4.1075e-02, 1.0285e-01, 3.8955e-01,\n",
       "                4.3898e-02, 5.2318e-02, 4.1269e-05, 1.3108e-01, 3.0526e-01, 5.3994e-02,\n",
       "                1.9795e-01, 2.3957e-01, 1.0017e-01, 2.0457e-01, 7.6196e-02, 8.3913e-04,\n",
       "                2.9658e-02, 4.4313e-02, 1.1877e-01, 9.0686e-02, 3.3857e-02, 1.6375e-01,\n",
       "                1.3339e-01, 9.1547e-02, 3.2528e-04, 1.5644e-03, 3.8363e-02, 2.0090e-01,\n",
       "                1.5555e-02, 1.3219e-01, 3.6355e-01, 2.2100e-01, 4.4403e-01, 2.9151e-01,\n",
       "                7.4496e-02, 1.4326e-04, 7.9857e-02, 5.4095e-05, 1.1790e-01, 3.2726e-01,\n",
       "                5.6822e-02, 1.6643e-02, 1.1640e-01, 2.6604e-01, 1.1041e-01, 1.1709e-01,\n",
       "                9.8772e-03, 4.5817e-02, 1.2495e-01, 3.0917e-01], device='cuda:0')\n",
       "      )\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "    (bn1): Identity()\n",
       "    (relu): Identity()\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.0649, -0.0497, -0.0639, -0.0780, -0.0922, -0.1233, -0.0786, -0.0743,\n",
       "                    -0.1683, -0.0491, -0.0438, -0.0686, -0.2234, -0.0464, -0.1292, -0.0452,\n",
       "                    -0.0383, -0.0300, -0.0684, -0.0537, -0.0632, -0.1086, -0.0867, -0.1247,\n",
       "                    -0.0784, -0.0742, -0.0546, -0.0701, -0.0974, -0.0493, -0.1037, -0.0894,\n",
       "                    -0.0502, -0.0757, -0.1206, -0.0510, -0.0532, -0.0696, -0.0901, -0.0739,\n",
       "                    -0.0826, -0.1014, -0.0807, -0.0542, -0.0630, -0.1266, -0.0855, -0.0974,\n",
       "                    -0.0769, -0.0875, -0.0700, -0.0623, -0.0397, -0.0324, -0.1150, -0.1314,\n",
       "                    -0.1286, -0.1281, -0.1139, -0.0564, -0.0976, -0.1196, -0.1452, -0.0934],\n",
       "                   device='cuda:0'), max_val=tensor([0.1040, 0.0553, 0.0886, 0.0414, 0.0708, 0.0743, 0.0754, 0.0723, 0.0495,\n",
       "                    0.1175, 0.0449, 0.0321, 0.0544, 0.1033, 0.0569, 0.0430, 0.0574, 0.1336,\n",
       "                    0.0683, 0.0661, 0.0623, 0.0811, 0.0933, 0.0754, 0.0521, 0.0654, 0.1045,\n",
       "                    0.0609, 0.0923, 0.0435, 0.0764, 0.0794, 0.0584, 0.0593, 0.0915, 0.0349,\n",
       "                    0.0612, 0.0823, 0.0775, 0.0475, 0.0764, 0.0485, 0.0469, 0.0353, 0.0569,\n",
       "                    0.0732, 0.0944, 0.0585, 0.0428, 0.0497, 0.0528, 0.1264, 0.1007, 0.0591,\n",
       "                    0.1136, 0.0534, 0.0614, 0.0477, 0.0826, 0.1204, 0.0514, 0.1106, 0.0606,\n",
       "                    0.0581], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.1349, -0.1499, -0.0940, -0.1430, -0.1758, -0.0981, -0.1464, -0.0750,\n",
       "                    -0.1920, -0.2320, -0.2525, -0.1435, -0.1886, -0.4001, -0.0372, -0.1753,\n",
       "                    -0.1885, -0.1667, -0.2311, -0.1751, -0.1701, -0.2751, -0.0349, -0.1031,\n",
       "                    -0.1866, -0.1674, -0.1534, -0.0888, -0.1622, -0.2968, -0.2511, -0.2883,\n",
       "                    -0.1564, -0.1681, -0.1913, -0.1618, -0.1747, -0.3056, -0.2732, -0.3571,\n",
       "                    -0.2368, -0.2279, -0.2298, -0.1690, -0.1738, -0.1076, -0.1181, -0.1347,\n",
       "                    -0.2349, -0.3448, -0.2790, -0.4100, -0.1322, -0.0958, -0.2767, -0.2495,\n",
       "                    -0.1313, -0.0545, -0.2706, -0.1295, -0.2202, -0.3353, -0.1474, -0.0804],\n",
       "                   device='cuda:0'), max_val=tensor([0.2229, 0.3706, 0.0458, 0.1507, 0.2432, 0.1295, 0.2324, 0.0793, 0.2128,\n",
       "                    0.3213, 0.1925, 0.1019, 0.2448, 0.4542, 0.0311, 0.1987, 0.1449, 0.1205,\n",
       "                    0.2468, 0.1296, 0.2421, 0.2379, 0.0294, 0.1599, 0.1998, 0.1289, 0.1893,\n",
       "                    0.0428, 0.2395, 0.2768, 0.2700, 0.1668, 0.0829, 0.1777, 0.2709, 0.0915,\n",
       "                    0.1384, 0.2312, 0.2932, 0.3786, 0.3151, 0.2143, 0.3271, 0.1091, 0.1102,\n",
       "                    0.1127, 0.0932, 0.1474, 0.1754, 0.3415, 0.2161, 0.4628, 0.1311, 0.1073,\n",
       "                    0.2705, 0.2616, 0.1741, 0.0744, 0.2367, 0.1532, 0.3659, 0.3654, 0.1269,\n",
       "                    0.0897], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.0779, -0.0740, -0.0734, -0.1088, -0.0783, -0.0900, -0.1203, -0.1132,\n",
       "                    -0.0832, -0.0631, -0.0815, -0.0465, -0.0864, -0.0939, -0.1001, -0.1055,\n",
       "                    -0.0965, -0.0939, -0.0377, -0.0530, -0.0729, -0.0570, -0.0679, -0.0739,\n",
       "                    -0.0978, -0.0786, -0.0526, -0.1214, -0.1104, -0.0647, -0.0522, -0.0719,\n",
       "                    -0.0784, -0.0878, -0.1350, -0.0840, -0.0593, -0.0757, -0.0859, -0.1046,\n",
       "                    -0.1305, -0.1021, -0.0869, -0.0878, -0.0663, -0.0423, -0.1104, -0.0690,\n",
       "                    -0.0871, -0.1454, -0.0485, -0.0684, -0.0587, -0.0829, -0.0669, -0.0830,\n",
       "                    -0.0594, -0.0802, -0.0685, -0.1199, -0.1046, -0.1857, -0.0558, -0.0743],\n",
       "                   device='cuda:0'), max_val=tensor([0.1304, 0.0706, 0.1088, 0.1032, 0.1197, 0.0637, 0.0589, 0.0684, 0.0661,\n",
       "                    0.0795, 0.0568, 0.0679, 0.0598, 0.1469, 0.0908, 0.0599, 0.0915, 0.0630,\n",
       "                    0.0685, 0.1695, 0.0660, 0.0729, 0.0774, 0.0780, 0.1180, 0.0611, 0.1174,\n",
       "                    0.0643, 0.0826, 0.0622, 0.0742, 0.0517, 0.0452, 0.0909, 0.1318, 0.0984,\n",
       "                    0.0892, 0.0975, 0.0589, 0.1273, 0.1663, 0.0940, 0.1057, 0.0815, 0.0904,\n",
       "                    0.0414, 0.0821, 0.0818, 0.0787, 0.1509, 0.0676, 0.0725, 0.1341, 0.0947,\n",
       "                    0.0883, 0.0740, 0.0719, 0.0848, 0.0890, 0.0670, 0.0700, 0.3616, 0.0501,\n",
       "                    0.0760], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.1354, -0.1574, -0.4089, -0.1352, -0.1469, -0.1846, -0.1124, -0.1370,\n",
       "                    -0.2401, -0.1741, -0.2736, -0.1249, -0.1904, -0.2956, -0.1173, -0.3256,\n",
       "                    -0.2489, -0.1749, -0.2677, -0.1829, -0.2623, -0.2517, -0.0999, -0.0971,\n",
       "                    -0.2010, -0.2096, -0.2372, -0.1860, -0.2894, -0.2975, -0.1686, -0.2794,\n",
       "                    -0.1660, -0.1333, -0.2612, -0.1020, -0.3362, -0.2727, -0.3873, -0.3337,\n",
       "                    -0.3857, -0.3185, -0.3145, -0.3002, -0.0915, -0.1130, -0.2197, -0.1454,\n",
       "                    -0.2886, -0.3619, -0.2217, -0.2433, -0.3144, -0.1607, -0.3723, -0.2646,\n",
       "                    -0.2148, -0.0369, -0.2491, -0.1278, -0.1081, -0.2872, -0.1485, -0.1265],\n",
       "                   device='cuda:0'), max_val=tensor([0.1500, 0.2076, 0.1114, 0.1636, 0.2098, 0.1838, 0.1581, 0.1061, 0.2499,\n",
       "                    0.2245, 0.1920, 0.1166, 0.2085, 0.3321, 0.0948, 0.6482, 0.1804, 0.1518,\n",
       "                    0.3437, 0.2198, 0.1898, 0.2611, 0.1063, 0.1193, 0.2189, 0.1753, 0.1920,\n",
       "                    0.1573, 0.2973, 0.3280, 0.2291, 0.3346, 0.1381, 0.1419, 0.3270, 0.0540,\n",
       "                    0.3139, 0.3735, 0.3426, 0.2796, 0.3095, 0.3219, 0.4152, 0.2033, 0.1047,\n",
       "                    0.0945, 0.1618, 0.1555, 0.2794, 0.4219, 0.2317, 0.2554, 0.2400, 0.1911,\n",
       "                    0.3876, 0.2771, 0.2270, 0.0279, 0.3253, 0.1269, 0.2142, 0.2801, 0.1125,\n",
       "                    0.1295], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.0465, -0.0781, -0.0614, -0.0627, -0.0465, -0.0610, -0.0659, -0.0662,\n",
       "                    -0.0718, -0.0482, -0.0601, -0.0617, -0.0609, -0.0756, -0.0607, -0.0938,\n",
       "                    -0.0887, -0.0520, -0.0765, -0.0718, -0.0839, -0.0563, -0.0984, -0.0586,\n",
       "                    -0.0613, -0.0704, -0.0545, -0.0574, -0.0486, -0.0617, -0.0812, -0.0463,\n",
       "                    -0.0467, -0.0697, -0.0647, -0.0608, -0.1036, -0.0561, -0.0718, -0.0582,\n",
       "                    -0.0823, -0.0638, -0.0732, -0.1041, -0.0641, -0.0574, -0.0437, -0.1605,\n",
       "                    -0.0441, -0.0773, -0.0738, -0.0536, -0.0730, -0.0605, -0.0681, -0.0853,\n",
       "                    -0.0593, -0.0560, -0.0668, -0.0689, -0.0734, -0.0455, -0.0698, -0.0597,\n",
       "                    -0.0843, -0.0630, -0.0725, -0.0623, -0.0523, -0.0507, -0.0608, -0.0575,\n",
       "                    -0.0729, -0.0671, -0.0585, -0.0603, -0.0576, -0.0625, -0.0641, -0.0611,\n",
       "                    -0.0614, -0.0607, -0.0553, -0.0649, -0.0780, -0.1126, -0.0695, -0.0530,\n",
       "                    -0.0911, -0.0792, -0.0798, -0.0660, -0.0623, -0.0717, -0.0708, -0.0851,\n",
       "                    -0.0476, -0.0665, -0.0838, -0.0577, -0.0544, -0.0573, -0.0600, -0.0602,\n",
       "                    -0.0981, -0.0642, -0.0564, -0.0727, -0.0620, -0.0584, -0.0673, -0.1330,\n",
       "                    -0.0828, -0.0559, -0.0419, -0.0572, -0.0590, -0.1137, -0.0830, -0.0541,\n",
       "                    -0.0638, -0.0647, -0.0606, -0.0497, -0.0751, -0.0535, -0.0640, -0.0794],\n",
       "                   device='cuda:0'), max_val=tensor([0.0761, 0.1070, 0.0657, 0.0752, 0.0604, 0.0719, 0.0836, 0.0658, 0.0700,\n",
       "                    0.0811, 0.0744, 0.0637, 0.0963, 0.0768, 0.0636, 0.0790, 0.1222, 0.0742,\n",
       "                    0.0538, 0.1033, 0.1424, 0.0831, 0.1182, 0.0737, 0.1430, 0.1002, 0.0575,\n",
       "                    0.0689, 0.0917, 0.0635, 0.0524, 0.0537, 0.0820, 0.1090, 0.0794, 0.0660,\n",
       "                    0.0574, 0.0994, 0.0603, 0.0620, 0.0626, 0.0829, 0.1222, 0.0661, 0.0816,\n",
       "                    0.0805, 0.0717, 0.1509, 0.1178, 0.0561, 0.0758, 0.0777, 0.0836, 0.0679,\n",
       "                    0.0742, 0.0912, 0.0744, 0.0818, 0.0636, 0.0883, 0.0660, 0.0707, 0.0770,\n",
       "                    0.0714, 0.0781, 0.1137, 0.0671, 0.1197, 0.0645, 0.0768, 0.0714, 0.0485,\n",
       "                    0.0862, 0.0752, 0.0634, 0.0654, 0.0953, 0.0987, 0.0702, 0.0670, 0.0884,\n",
       "                    0.0722, 0.0767, 0.0735, 0.0703, 0.2232, 0.0783, 0.0722, 0.0684, 0.0943,\n",
       "                    0.1452, 0.0655, 0.0792, 0.1078, 0.0459, 0.1105, 0.0939, 0.0962, 0.0589,\n",
       "                    0.0753, 0.0664, 0.0652, 0.0754, 0.0864, 0.0639, 0.0466, 0.0692, 0.1524,\n",
       "                    0.0915, 0.0922, 0.0750, 0.1117, 0.0938, 0.0636, 0.0735, 0.0817, 0.0546,\n",
       "                    0.0564, 0.0558, 0.0669, 0.0868, 0.0798, 0.0718, 0.0662, 0.1209, 0.0661,\n",
       "                    0.0808, 0.0871], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.1420, -0.1421, -0.2042, -0.1669, -0.1513, -0.0999, -0.1476, -0.1366,\n",
       "                    -0.1653, -0.1581, -0.1706, -0.1468, -0.1677, -0.1464, -0.2063, -0.1437,\n",
       "                    -0.1627, -0.1343, -0.1344, -0.1158, -0.1770, -0.1491, -0.1309, -0.2364,\n",
       "                    -0.1808, -0.1429, -0.2031, -0.1435, -0.1569, -0.2053, -0.2281, -0.1638,\n",
       "                    -0.1475, -0.1574, -0.1226, -0.1888, -0.1522, -0.1438, -0.1484, -0.1404,\n",
       "                    -0.1549, -0.1709, -0.2582, -0.1385, -0.1618, -0.1543, -0.1548, -0.1297,\n",
       "                    -0.1442, -0.2048, -0.1938, -0.2918, -0.1494, -0.1309, -0.1702, -0.1886,\n",
       "                    -0.1101, -0.1984, -0.1797, -0.1555, -0.1453, -0.1729, -0.1504, -0.1522,\n",
       "                    -0.1319, -0.1634, -0.1883, -0.1390, -0.2074, -0.1331, -0.2014, -0.1991,\n",
       "                    -0.1343, -0.1597, -0.1627, -0.1467, -0.1739, -0.1578, -0.1301, -0.1548,\n",
       "                    -0.1584, -0.1612, -0.1750, -0.1501, -0.1735, -0.1296, -0.1614, -0.1438,\n",
       "                    -0.1735, -0.1548, -0.1196, -0.1136, -0.1562, -0.1263, -0.1731, -0.1515,\n",
       "                    -0.1442, -0.1224, -0.1424, -0.1266, -0.1600, -0.1599, -0.1392, -0.1421,\n",
       "                    -0.1696, -0.1103, -0.1598, -0.1245, -0.1427, -0.1518, -0.1198, -0.1624,\n",
       "                    -0.1448, -0.1600, -0.1560, -0.1575, -0.1456, -0.1196, -0.1561, -0.1557,\n",
       "                    -0.1696, -0.1550, -0.1467, -0.1312, -0.1451, -0.1149, -0.1618, -0.1468],\n",
       "                   device='cuda:0'), max_val=tensor([0.1666, 0.1584, 0.2574, 0.2344, 0.1133, 0.1521, 0.1842, 0.1727, 0.2334,\n",
       "                    0.2688, 0.3261, 0.2307, 0.1903, 0.2143, 0.2611, 0.2059, 0.1633, 0.2125,\n",
       "                    0.1791, 0.1457, 0.1902, 0.1963, 0.1932, 0.2656, 0.1999, 0.2422, 0.2631,\n",
       "                    0.1761, 0.2337, 0.1815, 0.1765, 0.1618, 0.2564, 0.2092, 0.1624, 0.1632,\n",
       "                    0.1662, 0.2559, 0.2080, 0.1757, 0.2239, 0.2006, 0.3191, 0.2286, 0.2204,\n",
       "                    0.2485, 0.1626, 0.1548, 0.1485, 0.2262, 0.1443, 0.2736, 0.1824, 0.2320,\n",
       "                    0.1445, 0.2282, 0.1263, 0.2351, 0.2111, 0.2277, 0.1490, 0.1459, 0.1465,\n",
       "                    0.1726, 0.1111, 0.2604, 0.2050, 0.1818, 0.1689, 0.1666, 0.2272, 0.2886,\n",
       "                    0.1918, 0.2356, 0.2762, 0.2267, 0.1569, 0.1471, 0.1946, 0.1858, 0.1348,\n",
       "                    0.1956, 0.1811, 0.1615, 0.2080, 0.1430, 0.2012, 0.1923, 0.1741, 0.2485,\n",
       "                    0.1779, 0.1803, 0.1889, 0.1314, 0.1946, 0.1671, 0.1856, 0.1395, 0.2294,\n",
       "                    0.2110, 0.2409, 0.2200, 0.1862, 0.1804, 0.1897, 0.1419, 0.2048, 0.1324,\n",
       "                    0.1649, 0.1925, 0.1223, 0.1880, 0.1930, 0.2075, 0.2019, 0.1697, 0.1972,\n",
       "                    0.1439, 0.1678, 0.1671, 0.1901, 0.1802, 0.1919, 0.1605, 0.2440, 0.1408,\n",
       "                    0.1843, 0.1635], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "              min_val=tensor([-0.0879, -0.1326, -0.1056, -0.0599, -0.2793, -0.0767, -0.1828, -0.0494,\n",
       "                      -0.0729, -0.1462, -0.0997, -0.0752, -0.0806, -0.0835, -0.1419, -0.0717,\n",
       "                      -0.0827, -0.0978, -0.1373, -0.0971, -0.1413, -0.0916, -0.0993, -0.1019,\n",
       "                      -0.1106, -0.0622, -0.0945, -0.0999, -0.0657, -0.0903, -0.1668, -0.1363,\n",
       "                      -0.0803, -0.0656, -0.1055, -0.0649, -0.0847, -0.0903, -0.0491, -0.1002,\n",
       "                      -0.1191, -0.1702, -0.1260, -0.0948, -0.0830, -0.0726, -0.1387, -0.1478,\n",
       "                      -0.1489, -0.1172, -0.1715, -0.2220, -0.0976, -0.1316, -0.1607, -0.0736,\n",
       "                      -0.1722, -0.0995, -0.0982, -0.0687, -0.0979, -0.1186, -0.0857, -0.1121,\n",
       "                      -0.2934, -0.0815, -0.0831, -0.1498, -0.0907, -0.0847, -0.0716, -0.0434,\n",
       "                      -0.1166, -0.0791, -0.1279, -0.0649, -0.1193, -0.1102, -0.1494, -0.0818,\n",
       "                      -0.2213, -0.0840, -0.0829, -0.0977, -0.1026, -0.1407, -0.1362, -0.1015,\n",
       "                      -0.1474, -0.0911, -0.0281, -0.1037, -0.0901, -0.1065, -0.0939, -0.0965,\n",
       "                      -0.1386, -0.1153, -0.1150, -0.1001, -0.0680, -0.0440, -0.0830, -0.0667,\n",
       "                      -0.0454, -0.1385, -0.0696, -0.1007, -0.0760, -0.0980, -0.1731, -0.0917,\n",
       "                      -0.0846, -0.1010, -0.0903, -0.0914, -0.1676, -0.1344, -0.0954, -0.1431,\n",
       "                      -0.0944, -0.1206, -0.0803, -0.1346, -0.0581, -0.1772, -0.1112, -0.1603],\n",
       "                     device='cuda:0'), max_val=tensor([0.0683, 0.1171, 0.1659, 0.0670, 0.0903, 0.1577, 0.3653, 0.1675, 0.1425,\n",
       "                      0.1562, 0.1226, 0.1223, 0.0838, 0.1072, 0.0949, 0.0633, 0.1643, 0.1169,\n",
       "                      0.1182, 0.1417, 0.0942, 0.1089, 0.0752, 0.1269, 0.1548, 0.0655, 0.1108,\n",
       "                      0.1424, 0.1183, 0.1647, 0.2122, 0.1115, 0.0795, 0.1491, 0.1358, 0.0560,\n",
       "                      0.0942, 0.0969, 0.0671, 0.1101, 0.1679, 0.1532, 0.0889, 0.2012, 0.0648,\n",
       "                      0.0847, 0.1477, 0.1352, 0.1105, 0.1558, 0.0915, 0.3977, 0.1280, 0.1510,\n",
       "                      0.2564, 0.1051, 0.0705, 0.1173, 0.2011, 0.0533, 0.1185, 0.1860, 0.0874,\n",
       "                      0.1876, 0.2068, 0.0848, 0.0712, 0.1263, 0.0773, 0.0771, 0.1601, 0.0846,\n",
       "                      0.2008, 0.0947, 0.1305, 0.0819, 0.1138, 0.1295, 0.1344, 0.0484, 0.1212,\n",
       "                      0.0983, 0.0931, 0.0571, 0.1033, 0.0937, 0.1161, 0.0536, 0.1455, 0.0960,\n",
       "                      0.0182, 0.1578, 0.0571, 0.0849, 0.1311, 0.1383, 0.1523, 0.0566, 0.0755,\n",
       "                      0.1552, 0.1234, 0.1113, 0.1238, 0.0464, 0.0500, 0.1226, 0.1061, 0.1076,\n",
       "                      0.0855, 0.1596, 0.1054, 0.0925, 0.0979, 0.0847, 0.1094, 0.0885, 0.0731,\n",
       "                      0.0952, 0.1272, 0.1134, 0.1153, 0.0945, 0.1346, 0.1521, 0.0655, 0.2019,\n",
       "                      0.1406, 0.2744], device='cuda:0')\n",
       "            )\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.1092, -0.0953, -0.1127, -0.1108, -0.1037, -0.1058, -0.0905, -0.1223,\n",
       "                    -0.1021, -0.1275, -0.1039, -0.1100, -0.1126, -0.1217, -0.1037, -0.1136,\n",
       "                    -0.1056, -0.1063, -0.0993, -0.0809, -0.1259, -0.0949, -0.0797, -0.0741,\n",
       "                    -0.1120, -0.1002, -0.1066, -0.1190, -0.0968, -0.1066, -0.1056, -0.1381,\n",
       "                    -0.0761, -0.1236, -0.1023, -0.1061, -0.1022, -0.1236, -0.0996, -0.0764,\n",
       "                    -0.0983, -0.0939, -0.1144, -0.0838, -0.0987, -0.0808, -0.0988, -0.1086,\n",
       "                    -0.1231, -0.0848, -0.1044, -0.0988, -0.0996, -0.0971, -0.1245, -0.1036,\n",
       "                    -0.1018, -0.1396, -0.0807, -0.1019, -0.0964, -0.1152, -0.0932, -0.1108,\n",
       "                    -0.1005, -0.0947, -0.1118, -0.0817, -0.1135, -0.0876, -0.1163, -0.0993,\n",
       "                    -0.0853, -0.0796, -0.0921, -0.0829, -0.0806, -0.0809, -0.1306, -0.0923,\n",
       "                    -0.1223, -0.1313, -0.1021, -0.0813, -0.1140, -0.1003, -0.0994, -0.1101,\n",
       "                    -0.1221, -0.1032, -0.0874, -0.0999, -0.1093, -0.1074, -0.0674, -0.0982,\n",
       "                    -0.0814, -0.0918, -0.0825, -0.0813, -0.0913, -0.1179, -0.1023, -0.0949,\n",
       "                    -0.0925, -0.1059, -0.0906, -0.0984, -0.1148, -0.0781, -0.1037, -0.0749,\n",
       "                    -0.1284, -0.1257, -0.0941, -0.1006, -0.0938, -0.0924, -0.0866, -0.0942,\n",
       "                    -0.1006, -0.0622, -0.0826, -0.0868, -0.0793, -0.0920, -0.0786, -0.0842],\n",
       "                   device='cuda:0'), max_val=tensor([0.1289, 0.1556, 0.1431, 0.1750, 0.1366, 0.1270, 0.1067, 0.2086, 0.1252,\n",
       "                    0.1345, 0.1477, 0.1119, 0.1370, 0.1438, 0.1473, 0.1345, 0.0922, 0.1215,\n",
       "                    0.0877, 0.0982, 0.1277, 0.1247, 0.1143, 0.1395, 0.1378, 0.1561, 0.1498,\n",
       "                    0.1279, 0.1126, 0.1054, 0.1179, 0.1403, 0.1029, 0.1195, 0.0866, 0.1353,\n",
       "                    0.1079, 0.1175, 0.1261, 0.1150, 0.1423, 0.1421, 0.1277, 0.1497, 0.1660,\n",
       "                    0.1176, 0.1242, 0.1250, 0.1350, 0.1013, 0.1146, 0.1514, 0.1280, 0.1093,\n",
       "                    0.1420, 0.1060, 0.1244, 0.1241, 0.1205, 0.1286, 0.1023, 0.1478, 0.1258,\n",
       "                    0.1345, 0.1091, 0.1272, 0.1190, 0.0946, 0.0910, 0.1371, 0.1123, 0.0935,\n",
       "                    0.1117, 0.1194, 0.1571, 0.1023, 0.0988, 0.1121, 0.1236, 0.0877, 0.1310,\n",
       "                    0.1626, 0.1486, 0.1444, 0.1154, 0.1230, 0.1248, 0.1163, 0.1296, 0.1014,\n",
       "                    0.1080, 0.1132, 0.1106, 0.1534, 0.0971, 0.1145, 0.1436, 0.1341, 0.1686,\n",
       "                    0.1134, 0.1139, 0.1305, 0.1162, 0.1284, 0.1224, 0.1112, 0.1333, 0.1441,\n",
       "                    0.1351, 0.1136, 0.1272, 0.1530, 0.1559, 0.1212, 0.1222, 0.1152, 0.1043,\n",
       "                    0.1331, 0.1202, 0.1485, 0.1414, 0.1048, 0.1143, 0.0997, 0.1036, 0.1303,\n",
       "                    0.0961, 0.0941], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.1778, -0.2268, -0.2466, -0.2695, -0.1509, -0.2312, -0.1639, -0.1657,\n",
       "                    -0.2822, -0.2422, -0.2680, -0.2417, -0.2126, -0.1599, -0.2545, -0.2093,\n",
       "                    -0.2152, -0.2092, -0.2250, -0.4331, -0.1833, -0.0883, -0.1986, -0.3478,\n",
       "                    -0.1939, -0.1862, -0.2859, -0.2752, -0.1806, -0.2889, -0.1654, -0.2445,\n",
       "                    -0.2107, -0.2269, -0.3645, -0.2633, -0.2842, -0.2029, -0.3234, -0.2444,\n",
       "                    -0.2503, -0.2006, -0.3043, -0.2074, -0.2948, -0.1102, -0.2717, -0.3317,\n",
       "                    -0.1611, -0.2061, -0.2274, -0.1727, -0.1985, -0.2874, -0.2765, -0.2759,\n",
       "                    -0.1610, -0.2743, -0.2298, -0.2609, -0.2670, -0.1878, -0.2163, -0.2495,\n",
       "                    -0.2280, -0.1555, -0.1641, -0.1848, -0.1591, -0.2479, -0.3374, -0.2406,\n",
       "                    -0.1760, -0.2824, -0.2100, -0.2789, -0.1801, -0.2634, -0.1750, -0.2747,\n",
       "                    -0.2091, -0.1958, -0.2702, -0.2558, -0.3111, -0.2900, -0.2297, -0.2412,\n",
       "                    -0.2420, -0.3829, -0.3454, -0.1894, -0.2040, -0.2117, -0.2489, -0.2889,\n",
       "                    -0.2208, -0.2214, -0.2735, -0.3572, -0.2208, -0.2240, -0.2411, -0.3022,\n",
       "                    -0.1906, -0.2391, -0.2594, -0.3141, -0.2277, -0.2866, -0.2619, -0.3730,\n",
       "                    -0.2315, -0.2755, -0.2283, -0.1847, -0.2214, -0.1827, -0.2697, -0.2122,\n",
       "                    -0.2554, -0.2191, -0.1859, -0.1255, -0.3422, -0.1424, -0.1293, -0.2714],\n",
       "                   device='cuda:0'), max_val=tensor([0.1876, 0.2522, 0.3114, 0.3026, 0.1651, 0.1973, 0.2530, 0.1685, 0.2070,\n",
       "                    0.3705, 0.3985, 0.3189, 0.2683, 0.2072, 0.3311, 0.3257, 0.3236, 0.2246,\n",
       "                    0.2160, 0.2884, 0.2043, 0.0728, 0.3575, 0.4684, 0.2482, 0.2139, 0.3886,\n",
       "                    0.2482, 0.2758, 0.3547, 0.2234, 0.2975, 0.2274, 0.3159, 0.2117, 0.3186,\n",
       "                    0.2551, 0.3295, 0.4327, 0.3160, 0.3481, 0.1446, 0.4404, 0.2277, 0.2738,\n",
       "                    0.1439, 0.2977, 0.4263, 0.2162, 0.2381, 0.2766, 0.1749, 0.2239, 0.3692,\n",
       "                    0.3118, 0.4623, 0.2337, 0.3284, 0.3052, 0.3603, 0.3729, 0.2162, 0.2654,\n",
       "                    0.3517, 0.2249, 0.1481, 0.2016, 0.2421, 0.1996, 0.3292, 0.4695, 0.3134,\n",
       "                    0.2429, 0.2830, 0.2088, 0.2679, 0.1733, 0.2519, 0.1951, 0.2636, 0.2624,\n",
       "                    0.2357, 0.3504, 0.2416, 0.2867, 0.2569, 0.2350, 0.3937, 0.2328, 0.4800,\n",
       "                    0.3938, 0.2200, 0.2105, 0.1677, 0.3043, 0.2418, 0.2297, 0.2875, 0.3207,\n",
       "                    0.3185, 0.4851, 0.4608, 0.2628, 0.3351, 0.2924, 0.2413, 0.3236, 0.3187,\n",
       "                    0.2683, 0.3252, 0.2712, 0.6133, 0.3227, 0.3674, 0.3381, 0.1840, 0.1885,\n",
       "                    0.2242, 0.2596, 0.1839, 0.3587, 0.1890, 0.2790, 0.1147, 0.4262, 0.1583,\n",
       "                    0.1434, 0.2319], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.0894, -0.0699, -0.0682, -0.0622, -0.0821, -0.0971, -0.0686, -0.0735,\n",
       "                    -0.0614, -0.0613, -0.0737, -0.0611, -0.0817, -0.0578, -0.0597, -0.0731,\n",
       "                    -0.0752, -0.0788, -0.0782, -0.0620, -0.0932, -0.0647, -0.0667, -0.0722,\n",
       "                    -0.0756, -0.0556, -0.0593, -0.0423, -0.0463, -0.0560, -0.0722, -0.0724,\n",
       "                    -0.0714, -0.0770, -0.0576, -0.0712, -0.0634, -0.0673, -0.0850, -0.0596,\n",
       "                    -0.0677, -0.0958, -0.0768, -0.0799, -0.0577, -0.0656, -0.0752, -0.0615,\n",
       "                    -0.0739, -0.0872, -0.0541, -0.0649, -0.0760, -0.0804, -0.0611, -0.0838,\n",
       "                    -0.0596, -0.0657, -0.0944, -0.0497, -0.0627, -0.0876, -0.0759, -0.0617,\n",
       "                    -0.0663, -0.0790, -0.0762, -0.0733, -0.0947, -0.1204, -0.0703, -0.0531,\n",
       "                    -0.0933, -0.0614, -0.0929, -0.0775, -0.0659, -0.0661, -0.0731, -0.0790,\n",
       "                    -0.0609, -0.0864, -0.0547, -0.0640, -0.0694, -0.0595, -0.0705, -0.0704,\n",
       "                    -0.0831, -0.0758, -0.0774, -0.0748, -0.1106, -0.0952, -0.0645, -0.0572,\n",
       "                    -0.0549, -0.0840, -0.0908, -0.0657, -0.0570, -0.0811, -0.0756, -0.0709,\n",
       "                    -0.0632, -0.0668, -0.0693, -0.0674, -0.0758, -0.0627, -0.0775, -0.0884,\n",
       "                    -0.0776, -0.0822, -0.0789, -0.0600, -0.0841, -0.0646, -0.0705, -0.0720,\n",
       "                    -0.0593, -0.0797, -0.0636, -0.0753, -0.0817, -0.0570, -0.0558, -0.0591,\n",
       "                    -0.0606, -0.0693, -0.0668, -0.0752, -0.0639, -0.0908, -0.0649, -0.0628,\n",
       "                    -0.0621, -0.0862, -0.0706, -0.0687, -0.0821, -0.0618, -0.0433, -0.0618,\n",
       "                    -0.0768, -0.0705, -0.0556, -0.0802, -0.0847, -0.0686, -0.0315, -0.0595,\n",
       "                    -0.0708, -0.0583, -0.0722, -0.0756, -0.0777, -0.0790, -0.0789, -0.0713,\n",
       "                    -0.0620, -0.0593, -0.0638, -0.0676, -0.0821, -0.0562, -0.0774, -0.0620,\n",
       "                    -0.0630, -0.0924, -0.0798, -0.0804, -0.0721, -0.0684, -0.0776, -0.0875,\n",
       "                    -0.0665, -0.0710, -0.1014, -0.1222, -0.0557, -0.0487, -0.0764, -0.0873,\n",
       "                    -0.0775, -0.0669, -0.0738, -0.0618, -0.0759, -0.0538, -0.0763, -0.0530,\n",
       "                    -0.0468, -0.0511, -0.1012, -0.0754, -0.1078, -0.0691, -0.0504, -0.0622,\n",
       "                    -0.0819, -0.0845, -0.0691, -0.0876, -0.0696, -0.0588, -0.0678, -0.0660,\n",
       "                    -0.0750, -0.0778, -0.0753, -0.0606, -0.0622, -0.0624, -0.0717, -0.0887,\n",
       "                    -0.0605, -0.1060, -0.0858, -0.0546, -0.0681, -0.0794, -0.1017, -0.0475,\n",
       "                    -0.0683, -0.0800, -0.0678, -0.0844, -0.0833, -0.0741, -0.0608, -0.0875,\n",
       "                    -0.0739, -0.0783, -0.0397, -0.1011, -0.0690, -0.0695, -0.0574, -0.0584,\n",
       "                    -0.0862, -0.0501, -0.0577, -0.0740, -0.0910, -0.0823, -0.0465, -0.0691,\n",
       "                    -0.0622, -0.0710, -0.0738, -0.0796, -0.0577, -0.0665, -0.0510, -0.0745],\n",
       "                   device='cuda:0'), max_val=tensor([0.0902, 0.0792, 0.1206, 0.0872, 0.1342, 0.1037, 0.0692, 0.0908, 0.1309,\n",
       "                    0.1017, 0.0591, 0.1111, 0.0900, 0.0728, 0.0839, 0.0878, 0.0961, 0.1010,\n",
       "                    0.0931, 0.0751, 0.0860, 0.0956, 0.1415, 0.1237, 0.0813, 0.1159, 0.0629,\n",
       "                    0.0524, 0.0621, 0.0856, 0.0935, 0.1052, 0.0935, 0.1086, 0.0860, 0.0688,\n",
       "                    0.0851, 0.0740, 0.1491, 0.0967, 0.0708, 0.1127, 0.1041, 0.0884, 0.1006,\n",
       "                    0.0961, 0.0840, 0.0796, 0.0918, 0.1018, 0.0645, 0.0678, 0.0901, 0.1063,\n",
       "                    0.0897, 0.0745, 0.0782, 0.0863, 0.0905, 0.0905, 0.0676, 0.1411, 0.1343,\n",
       "                    0.0901, 0.0612, 0.0826, 0.1098, 0.0861, 0.1011, 0.0803, 0.0761, 0.1044,\n",
       "                    0.0903, 0.0735, 0.1006, 0.0817, 0.0971, 0.0710, 0.0800, 0.0817, 0.0635,\n",
       "                    0.0893, 0.0762, 0.0757, 0.1111, 0.0635, 0.0609, 0.0710, 0.0753, 0.0856,\n",
       "                    0.0783, 0.0864, 0.0878, 0.1632, 0.0918, 0.0758, 0.0765, 0.0936, 0.0923,\n",
       "                    0.1114, 0.1015, 0.0834, 0.1290, 0.0851, 0.0792, 0.0902, 0.1181, 0.0870,\n",
       "                    0.0899, 0.0663, 0.0792, 0.0845, 0.0822, 0.0902, 0.1279, 0.0904, 0.0861,\n",
       "                    0.1011, 0.0771, 0.1043, 0.0989, 0.0944, 0.0756, 0.0855, 0.0857, 0.0851,\n",
       "                    0.0740, 0.0681, 0.0749, 0.1121, 0.0921, 0.0964, 0.0874, 0.0974, 0.0775,\n",
       "                    0.0703, 0.0699, 0.0806, 0.1154, 0.0770, 0.1041, 0.0948, 0.0507, 0.1276,\n",
       "                    0.1214, 0.0903, 0.0951, 0.0868, 0.1012, 0.0870, 0.0437, 0.0930, 0.0780,\n",
       "                    0.0672, 0.0814, 0.0991, 0.0790, 0.1059, 0.0882, 0.0765, 0.1426, 0.0877,\n",
       "                    0.0669, 0.0830, 0.1066, 0.0831, 0.0863, 0.1008, 0.0969, 0.0961, 0.1117,\n",
       "                    0.0781, 0.0795, 0.0798, 0.1462, 0.1077, 0.0940, 0.0994, 0.0985, 0.0657,\n",
       "                    0.0550, 0.0698, 0.0811, 0.0705, 0.1219, 0.0809, 0.0804, 0.0837, 0.0937,\n",
       "                    0.1027, 0.1277, 0.0926, 0.0881, 0.0724, 0.0917, 0.1083, 0.1133, 0.0776,\n",
       "                    0.0753, 0.0643, 0.1088, 0.1228, 0.0576, 0.1370, 0.0700, 0.0950, 0.0843,\n",
       "                    0.0743, 0.1012, 0.1218, 0.1098, 0.0844, 0.0937, 0.0650, 0.1161, 0.0712,\n",
       "                    0.0885, 0.0791, 0.0865, 0.0800, 0.1400, 0.0502, 0.1023, 0.0640, 0.0785,\n",
       "                    0.0791, 0.0696, 0.0978, 0.0982, 0.0897, 0.0729, 0.1082, 0.0916, 0.0997,\n",
       "                    0.0429, 0.0974, 0.1004, 0.0745, 0.0852, 0.1015, 0.0813, 0.0531, 0.0506,\n",
       "                    0.0849, 0.1028, 0.0770, 0.0609, 0.0849, 0.0918, 0.0817, 0.0913, 0.0939,\n",
       "                    0.0694, 0.0716, 0.0637, 0.0968], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.1026, -0.1496, -0.0614, -0.1009, -0.0811, -0.1035, -0.0896, -0.0527,\n",
       "                    -0.0872, -0.1293, -0.1080, -0.1618, -0.2030, -0.2492, -0.0245, -0.2693,\n",
       "                    -0.1986, -0.2971, -0.1225, -0.2102, -0.1226, -0.1727, -0.1514, -0.1787,\n",
       "                    -0.1738, -0.0826, -0.3277, -0.0446, -0.1109, -0.3061, -0.0808, -0.0721,\n",
       "                    -0.0582, -0.1070, -0.0745, -0.0484, -0.1068, -0.0598, -0.0624, -0.2069,\n",
       "                    -0.2064, -0.1808, -0.1429, -0.1092, -0.0728, -0.1498, -0.1724, -0.1507,\n",
       "                    -0.0728, -0.0797, -0.0468, -0.1738, -0.2699, -0.0614, -0.1588, -0.1299,\n",
       "                    -0.0161, -0.1295, -0.1358, -0.0814, -0.1279, -0.0931, -0.1443, -0.2607,\n",
       "                    -0.0843, -0.1971, -0.1093, -0.2534, -0.1291, -0.1302, -0.0967, -0.0934,\n",
       "                    -0.1533, -0.1506, -0.0724, -0.0988, -0.0914, -0.1097, -0.1026, -0.1804,\n",
       "                    -0.0917, -0.1160, -0.1412, -0.1006, -0.1454, -0.1759, -0.0928, -0.0649,\n",
       "                    -0.1381, -0.1973, -0.1236, -0.1136, -0.1056, -0.0859, -0.1883, -0.1826,\n",
       "                    -0.1290, -0.0708, -0.1269, -0.1925, -0.1540, -0.0840, -0.1626, -0.1258,\n",
       "                    -0.1109, -0.0764, -0.1384, -0.0604, -0.0827, -0.0425, -0.2482, -0.1872,\n",
       "                    -0.1089, -0.1305, -0.0558, -0.1230, -0.0667, -0.1830, -0.2481, -0.1142,\n",
       "                    -0.0580, -0.3058, -0.1424, -0.1156, -0.0977, -0.1836, -0.1425, -0.0568,\n",
       "                    -0.1307, -0.1274, -0.1420, -0.1284, -0.1100, -0.1463, -0.0461, -0.1743,\n",
       "                    -0.1409, -0.1177, -0.1595, -0.1129, -0.1038, -0.1232, -0.2123, -0.1901,\n",
       "                    -0.0945, -0.0946, -0.0957, -0.1127, -0.1283, -0.0840, -0.1390, -0.1248,\n",
       "                    -0.0745, -0.1600, -0.2426, -0.1592, -0.1351, -0.1975, -0.1086, -0.2025,\n",
       "                    -0.1539, -0.0716, -0.1280, -0.2070, -0.0912, -0.1980, -0.1075, -0.1567,\n",
       "                    -0.1307, -0.1365, -0.0502, -0.1328, -0.2002, -0.1211, -0.1678, -0.1103,\n",
       "                    -0.0956, -0.0664, -0.1340, -0.0966, -0.1359, -0.1454, -0.1452, -0.1943,\n",
       "                    -0.1144, -0.1764, -0.1470, -0.0372, -0.0861, -0.0781, -0.1503, -0.1094,\n",
       "                    -0.1321, -0.1262, -0.0854, -0.1282, -0.0787, -0.1337, -0.0694, -0.0547,\n",
       "                    -0.0671, -0.0864, -0.1440, -0.1018, -0.0448, -0.1006, -0.1918, -0.0793,\n",
       "                    -0.1209, -0.1193, -0.1400, -0.1617, -0.1309, -0.1847, -0.3487, -0.2163,\n",
       "                    -0.1732, -0.0810, -0.2020, -0.1333, -0.1148, -0.1211, -0.1177, -0.1263,\n",
       "                    -0.1078, -0.1708, -0.1608, -0.1026, -0.1087, -0.1235, -0.1058, -0.1144,\n",
       "                    -0.1676, -0.1361, -0.0755, -0.1202, -0.1386, -0.1231, -0.1534, -0.1260,\n",
       "                    -0.0732, -0.0976, -0.1931, -0.0472, -0.1674, -0.1961, -0.0700, -0.1232,\n",
       "                    -0.1732, -0.0660, -0.1628, -0.1762, -0.1180, -0.0505, -0.0858, -0.0920],\n",
       "                   device='cuda:0'), max_val=tensor([0.0683, 0.1500, 0.0816, 0.0792, 0.0687, 0.1220, 0.0918, 0.0772, 0.1263,\n",
       "                    0.1244, 0.1646, 0.1453, 0.1505, 0.2029, 0.0370, 0.3122, 0.1820, 0.1820,\n",
       "                    0.1909, 0.1799, 0.1502, 0.1334, 0.1485, 0.2933, 0.1570, 0.0915, 0.2429,\n",
       "                    0.0607, 0.1124, 0.2688, 0.0992, 0.0766, 0.0753, 0.1332, 0.0949, 0.0638,\n",
       "                    0.0789, 0.0914, 0.0639, 0.1002, 0.2131, 0.2097, 0.1772, 0.1772, 0.0671,\n",
       "                    0.1721, 0.1651, 0.1344, 0.0806, 0.0656, 0.0382, 0.1541, 0.2884, 0.0699,\n",
       "                    0.1533, 0.1938, 0.0185, 0.1536, 0.1510, 0.0805, 0.1035, 0.0783, 0.1587,\n",
       "                    0.3447, 0.0982, 0.1581, 0.1674, 0.3077, 0.1415, 0.1820, 0.0762, 0.1226,\n",
       "                    0.1466, 0.0869, 0.0728, 0.0907, 0.1411, 0.1224, 0.1436, 0.2198, 0.0991,\n",
       "                    0.1312, 0.1587, 0.1011, 0.1417, 0.1074, 0.0754, 0.0939, 0.1586, 0.2871,\n",
       "                    0.1881, 0.1002, 0.1425, 0.1458, 0.1892, 0.2321, 0.1351, 0.0738, 0.1567,\n",
       "                    0.0989, 0.1713, 0.1259, 0.1534, 0.1065, 0.1184, 0.0755, 0.1501, 0.0424,\n",
       "                    0.0892, 0.0437, 0.2781, 0.2178, 0.1101, 0.1776, 0.0683, 0.1582, 0.0826,\n",
       "                    0.2244, 0.2520, 0.0827, 0.0451, 0.2951, 0.1360, 0.1088, 0.0841, 0.1372,\n",
       "                    0.2004, 0.0560, 0.1153, 0.1366, 0.1250, 0.1300, 0.1073, 0.2136, 0.0443,\n",
       "                    0.1350, 0.2172, 0.1354, 0.1451, 0.1187, 0.1309, 0.1357, 0.3531, 0.2042,\n",
       "                    0.1007, 0.0964, 0.1491, 0.1170, 0.1125, 0.0880, 0.1500, 0.2858, 0.0838,\n",
       "                    0.1609, 0.2678, 0.1416, 0.1283, 0.1764, 0.1581, 0.1869, 0.1290, 0.0902,\n",
       "                    0.1245, 0.3445, 0.0683, 0.2510, 0.1603, 0.1512, 0.1144, 0.1566, 0.0469,\n",
       "                    0.1320, 0.2208, 0.1409, 0.2091, 0.1172, 0.0756, 0.0801, 0.1487, 0.1723,\n",
       "                    0.1555, 0.1177, 0.1850, 0.1930, 0.1383, 0.1889, 0.1702, 0.0376, 0.0905,\n",
       "                    0.0519, 0.1411, 0.1109, 0.0998, 0.1112, 0.0885, 0.1376, 0.0940, 0.1287,\n",
       "                    0.0969, 0.0596, 0.0809, 0.0919, 0.1831, 0.0912, 0.0465, 0.1230, 0.1944,\n",
       "                    0.0823, 0.0978, 0.1023, 0.1112, 0.0855, 0.1534, 0.1335, 0.3125, 0.2563,\n",
       "                    0.1651, 0.0851, 0.1572, 0.1376, 0.0822, 0.1288, 0.1056, 0.1873, 0.1520,\n",
       "                    0.1241, 0.1350, 0.0890, 0.1365, 0.1434, 0.1202, 0.1082, 0.1449, 0.1182,\n",
       "                    0.0901, 0.1401, 0.0936, 0.1578, 0.1229, 0.1159, 0.0606, 0.0818, 0.2225,\n",
       "                    0.0409, 0.1805, 0.1818, 0.0880, 0.1211, 0.1897, 0.0991, 0.1697, 0.1384,\n",
       "                    0.1343, 0.0390, 0.1004, 0.0962], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "              min_val=tensor([-0.0724, -0.0707, -0.0702, -0.0772, -0.0784, -0.0711, -0.0875, -0.0478,\n",
       "                      -0.0716, -0.0325, -0.0875, -0.0103, -0.0536, -0.0830, -0.0801, -0.0771,\n",
       "                      -0.0026, -0.0903, -0.0804, -0.0516, -0.0617, -0.0060, -0.0759, -0.0738,\n",
       "                      -0.1023, -0.0086, -0.1505, -0.0698, -0.0907, -0.0471, -0.0711, -0.0898,\n",
       "                      -0.1363, -0.0200, -0.1200, -0.0900, -0.0829, -0.0508, -0.0983, -0.0767,\n",
       "                      -0.0670, -0.0841, -0.0613, -0.0381, -0.0601, -0.0880, -0.0476, -0.0609,\n",
       "                      -0.0451, -0.1190, -0.0516, -0.0571, -0.1022, -0.0708, -0.1081, -0.0972,\n",
       "                      -0.0568, -0.1367, -0.0753, -0.1125, -0.0168, -0.0514, -0.1129, -0.0908,\n",
       "                      -0.0877, -0.1110, -0.0617, -0.0738, -0.0003, -0.0696, -0.0606, -0.1122,\n",
       "                      -0.0133, -0.0671, -0.0579, -0.1191, -0.0483, -0.0607, -0.0817, -0.1351,\n",
       "                      -0.1099, -0.0280, -0.0653, -0.0972, -0.1034, -0.0079, -0.1428, -0.0038,\n",
       "                      -0.0663, -0.0829, -0.1152, -0.1565, -0.0747, -0.0473, -0.0730, -0.0702,\n",
       "                      -0.1070, -0.0559, -0.1160, -0.0144, -0.0135, -0.0949, -0.1058, -0.0768,\n",
       "                      -0.0902, -0.0565, -0.1089, -0.0478, -0.0656, -0.0772, -0.1097, -0.0485,\n",
       "                      -0.0963, -0.0803, -0.0681, -0.1211, -0.0607, -0.0385, -0.0684, -0.0052,\n",
       "                      -0.0377, -0.1380, -0.0851, -0.1269, -0.0624, -0.0014, -0.0693, -0.0660,\n",
       "                      -0.0119, -0.0761, -0.0885, -0.0093, -0.1046, -0.1086, -0.0403, -0.0102,\n",
       "                      -0.0943, -0.0937, -0.0632, -0.0032, -0.0868, -0.1059, -0.1016, -0.0931,\n",
       "                      -0.1032, -0.0778, -0.0742, -0.1037, -0.0728, -0.0745, -0.0794, -0.0912,\n",
       "                      -0.0645, -0.0476, -0.1100, -0.0881, -0.0447, -0.0544, -0.0781, -0.0522,\n",
       "                      -0.0829, -0.0784, -0.0669, -0.0913, -0.0510, -0.0721, -0.1260, -0.0144,\n",
       "                      -0.0050, -0.0965, -0.0652, -0.0877, -0.0923, -0.0980, -0.0801, -0.0010,\n",
       "                      -0.0978, -0.0569, -0.0078, -0.0978, -0.0933, -0.0113, -0.0762, -0.0101,\n",
       "                      -0.1177, -0.1282, -0.0483, -0.0984, -0.0346, -0.0401, -0.0603, -0.0521,\n",
       "                      -0.0074, -0.0797, -0.0099, -0.0767, -0.0042, -0.1325, -0.0635, -0.0802,\n",
       "                      -0.0122, -0.1014, -0.1106, -0.0679, -0.0605, -0.0797, -0.0698, -0.0731,\n",
       "                      -0.0012, -0.0846, -0.0401, -0.0021, -0.0788, -0.0963, -0.1017, -0.0693,\n",
       "                      -0.0465, -0.0744, -0.0667, -0.0510, -0.0656, -0.0436, -0.0019, -0.0121,\n",
       "                      -0.0661, -0.0828, -0.0906, -0.0916, -0.0729, -0.1329, -0.0888, -0.0041,\n",
       "                      -0.0159, -0.0023, -0.1010, -0.0941, -0.1020, -0.0595, -0.0575, -0.0021,\n",
       "                      -0.1246, -0.0747, -0.0594, -0.0745, -0.0688, -0.1031, -0.1002, -0.0042,\n",
       "                      -0.0975, -0.0829, -0.0451, -0.0237, -0.0843, -0.0633, -0.0053, -0.0477],\n",
       "                     device='cuda:0'), max_val=tensor([0.0706, 0.0882, 0.0889, 0.0914, 0.0875, 0.0882, 0.1163, 0.0811, 0.1038,\n",
       "                      0.0329, 0.0796, 0.0163, 0.0606, 0.0870, 0.1132, 0.1142, 0.0012, 0.0864,\n",
       "                      0.0427, 0.0686, 0.1120, 0.0041, 0.1301, 0.0725, 0.1184, 0.0070, 0.1027,\n",
       "                      0.0885, 0.1015, 0.0599, 0.0967, 0.0952, 0.1616, 0.0238, 0.1196, 0.1187,\n",
       "                      0.0928, 0.0794, 0.1629, 0.0376, 0.0675, 0.1245, 0.0682, 0.0428, 0.1103,\n",
       "                      0.1145, 0.0617, 0.0881, 0.0414, 0.1111, 0.0627, 0.0478, 0.1056, 0.0635,\n",
       "                      0.1363, 0.1073, 0.0820, 0.1453, 0.0961, 0.1045, 0.0182, 0.0821, 0.0906,\n",
       "                      0.1085, 0.1126, 0.1890, 0.1260, 0.0556, 0.0003, 0.0616, 0.0456, 0.1845,\n",
       "                      0.0067, 0.0396, 0.0609, 0.1087, 0.0664, 0.0708, 0.0803, 0.1975, 0.0992,\n",
       "                      0.0103, 0.0953, 0.1441, 0.0584, 0.0087, 0.1493, 0.0065, 0.0733, 0.1134,\n",
       "                      0.0828, 0.1083, 0.1339, 0.0829, 0.1157, 0.0654, 0.0791, 0.0449, 0.1622,\n",
       "                      0.0092, 0.0108, 0.1065, 0.0624, 0.1092, 0.0800, 0.0745, 0.1170, 0.0602,\n",
       "                      0.0920, 0.1454, 0.0932, 0.0228, 0.0918, 0.0845, 0.1113, 0.1322, 0.0684,\n",
       "                      0.0301, 0.1256, 0.0071, 0.0419, 0.1285, 0.1191, 0.0948, 0.0653, 0.0007,\n",
       "                      0.0813, 0.0750, 0.0057, 0.0807, 0.1581, 0.0131, 0.0763, 0.1189, 0.0319,\n",
       "                      0.0155, 0.1420, 0.0956, 0.0546, 0.0056, 0.0791, 0.1271, 0.1321, 0.1008,\n",
       "                      0.0954, 0.1129, 0.1290, 0.1033, 0.0695, 0.0516, 0.1151, 0.1217, 0.0707,\n",
       "                      0.0538, 0.0982, 0.1011, 0.0483, 0.0745, 0.1145, 0.0784, 0.0750, 0.1238,\n",
       "                      0.0663, 0.1001, 0.0277, 0.1280, 0.1361, 0.0162, 0.0039, 0.1269, 0.1089,\n",
       "                      0.0739, 0.1207, 0.1218, 0.1201, 0.0008, 0.1144, 0.0697, 0.0068, 0.1313,\n",
       "                      0.0843, 0.0079, 0.0753, 0.0072, 0.1215, 0.1603, 0.0424, 0.0797, 0.0372,\n",
       "                      0.0460, 0.0516, 0.0400, 0.0067, 0.0966, 0.0048, 0.0885, 0.0027, 0.0919,\n",
       "                      0.0748, 0.0937, 0.0057, 0.0847, 0.1035, 0.1009, 0.0948, 0.0519, 0.0602,\n",
       "                      0.0880, 0.0020, 0.0773, 0.0324, 0.0044, 0.1077, 0.1378, 0.0919, 0.0541,\n",
       "                      0.0443, 0.0603, 0.0673, 0.0664, 0.0717, 0.0271, 0.0052, 0.0131, 0.0716,\n",
       "                      0.0745, 0.1119, 0.0959, 0.0469, 0.1665, 0.0859, 0.0026, 0.0254, 0.0016,\n",
       "                      0.1215, 0.1341, 0.1043, 0.0894, 0.0604, 0.0047, 0.1914, 0.1062, 0.0810,\n",
       "                      0.0699, 0.0664, 0.0814, 0.1196, 0.0089, 0.1239, 0.0744, 0.0579, 0.0255,\n",
       "                      0.0872, 0.0578, 0.0034, 0.0750], device='cuda:0')\n",
       "            )\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.0044, -0.0104, -0.0016, -0.0008, -0.0034, -0.0027, -0.0212, -0.0131,\n",
       "                    -0.0024, -0.0121, -0.0020, -0.0032, -0.0182, -0.0024, -0.0021, -0.0042,\n",
       "                    -0.0027, -0.0020, -0.0135, -0.0081, -0.0052, -0.0021, -0.0007, -0.0145,\n",
       "                    -0.0098, -0.0010, -0.0058, -0.0078, -0.0047, -0.0052, -0.0189, -0.0120,\n",
       "                    -0.0253, -0.0189, -0.0106, -0.0090, -0.0046, -0.0046, -0.0282, -0.0073,\n",
       "                    -0.0017, -0.0013, -0.0058, -0.0110, -0.0048, -0.0209, -0.0102, -0.0010,\n",
       "                    -0.0053, -0.0010, -0.0236, -0.0042, -0.0147, -0.0045, -0.0004, -0.0016,\n",
       "                    -0.0015, -0.0093, -0.0085, -0.0099, -0.0034, -0.0232, -0.0055, -0.0008,\n",
       "                    -0.0150, -0.0081, -0.0074, -0.0086, -0.0123, -0.0201, -0.0084, -0.0014,\n",
       "                    -0.0296, -0.0248, -0.0113, -0.0151, -0.0127, -0.0011, -0.0024, -0.0015,\n",
       "                    -0.0018, -0.0031, -0.0020, -0.0017, -0.0049, -0.0007, -0.0195, -0.0107,\n",
       "                    -0.0025, -0.0009, -0.0032, -0.0094, -0.0078, -0.0010, -0.0106, -0.0008,\n",
       "                    -0.0032, -0.0061, -0.0065, -0.0048, -0.0155, -0.0023, -0.0060, -0.0059,\n",
       "                    -0.0016, -0.0006, -0.0237, -0.0614, -0.0044, -0.0108, -0.0011, -0.0111,\n",
       "                    -0.0014, -0.0324, -0.0056, -0.0022, -0.0010, -0.0014, -0.0041, -0.0144,\n",
       "                    -0.0053, -0.0046, -0.0063, -0.0036, -0.0134, -0.0079, -0.0033, -0.0035,\n",
       "                    -0.0025, -0.0156, -0.0063, -0.0011, -0.0069, -0.0083, -0.0042, -0.0178,\n",
       "                    -0.0047, -0.0020, -0.0116, -0.0086, -0.0077, -0.0290, -0.0033, -0.0120,\n",
       "                    -0.0132, -0.0026, -0.0031, -0.0124, -0.0036, -0.0117, -0.0126, -0.0049,\n",
       "                    -0.0081, -0.0093, -0.0109, -0.0225, -0.0148, -0.0185, -0.0276, -0.0058,\n",
       "                    -0.0061, -0.0036, -0.0142, -0.0008, -0.0192, -0.0047, -0.0393, -0.0080,\n",
       "                    -0.0091, -0.0005, -0.0053, -0.0095, -0.0025, -0.0033, -0.0033, -0.0011,\n",
       "                    -0.0036, -0.0108, -0.0085, -0.0096, -0.0270, -0.0048, -0.0173, -0.0038,\n",
       "                    -0.0111, -0.0039, -0.0038, -0.0287, -0.0011, -0.0214, -0.0015, -0.0009,\n",
       "                    -0.0006, -0.0073, -0.0005, -0.0222, -0.0015, -0.0036, -0.0107, -0.0206,\n",
       "                    -0.0135, -0.0159, -0.0085, -0.0172, -0.0132, -0.0060, -0.0126, -0.0271,\n",
       "                    -0.0005, -0.0309, -0.0046, -0.0094, -0.0150, -0.0069, -0.0189, -0.0055,\n",
       "                    -0.0010, -0.0064, -0.0056, -0.0017, -0.0047, -0.0159, -0.0108, -0.0006,\n",
       "                    -0.0057, -0.0047, -0.0230, -0.0021, -0.0198, -0.0019, -0.0237, -0.0201,\n",
       "                    -0.0340, -0.0045, -0.0041, -0.0153, -0.0018, -0.0130, -0.0006, -0.0023,\n",
       "                    -0.0126, -0.0013, -0.0045, -0.0028, -0.0032, -0.0018, -0.0236, -0.0033,\n",
       "                    -0.0056, -0.0009, -0.0077, -0.0029, -0.0014, -0.0011, -0.0156, -0.0139],\n",
       "                   device='cuda:0'), max_val=tensor([0.0037, 0.0103, 0.0022, 0.0009, 0.0041, 0.0033, 0.0263, 0.0151, 0.0023,\n",
       "                    0.0105, 0.0023, 0.0035, 0.0219, 0.0022, 0.0026, 0.0040, 0.0027, 0.0017,\n",
       "                    0.0162, 0.0102, 0.0059, 0.0023, 0.0012, 0.0264, 0.0087, 0.0013, 0.0070,\n",
       "                    0.0079, 0.0067, 0.0055, 0.0279, 0.0088, 0.0235, 0.0144, 0.0157, 0.0071,\n",
       "                    0.0058, 0.0045, 0.0352, 0.0078, 0.0018, 0.0014, 0.0051, 0.0109, 0.0075,\n",
       "                    0.0374, 0.0124, 0.0010, 0.0042, 0.0012, 0.0280, 0.0051, 0.0156, 0.0053,\n",
       "                    0.0004, 0.0016, 0.0016, 0.0212, 0.0118, 0.0096, 0.0038, 0.0161, 0.0084,\n",
       "                    0.0011, 0.0156, 0.0059, 0.0128, 0.0082, 0.0129, 0.0222, 0.0121, 0.0014,\n",
       "                    0.0357, 0.0250, 0.0153, 0.0212, 0.0172, 0.0016, 0.0024, 0.0017, 0.0019,\n",
       "                    0.0025, 0.0022, 0.0021, 0.0081, 0.0006, 0.0322, 0.0094, 0.0014, 0.0010,\n",
       "                    0.0029, 0.0120, 0.0086, 0.0013, 0.0111, 0.0010, 0.0046, 0.0061, 0.0108,\n",
       "                    0.0050, 0.0192, 0.0038, 0.0054, 0.0056, 0.0015, 0.0009, 0.0232, 0.0610,\n",
       "                    0.0072, 0.0187, 0.0012, 0.0136, 0.0019, 0.0661, 0.0087, 0.0032, 0.0013,\n",
       "                    0.0015, 0.0036, 0.0103, 0.0037, 0.0062, 0.0067, 0.0042, 0.0132, 0.0063,\n",
       "                    0.0025, 0.0035, 0.0021, 0.0210, 0.0067, 0.0015, 0.0068, 0.0105, 0.0027,\n",
       "                    0.0252, 0.0048, 0.0015, 0.0163, 0.0128, 0.0083, 0.0197, 0.0032, 0.0111,\n",
       "                    0.0177, 0.0029, 0.0037, 0.0186, 0.0037, 0.0142, 0.0107, 0.0048, 0.0081,\n",
       "                    0.0129, 0.0167, 0.0226, 0.0191, 0.0284, 0.0227, 0.0054, 0.0048, 0.0040,\n",
       "                    0.0137, 0.0009, 0.0278, 0.0054, 0.0361, 0.0084, 0.0067, 0.0009, 0.0074,\n",
       "                    0.0106, 0.0026, 0.0040, 0.0023, 0.0012, 0.0039, 0.0117, 0.0124, 0.0122,\n",
       "                    0.0443, 0.0048, 0.0157, 0.0035, 0.0153, 0.0046, 0.0039, 0.0371, 0.0016,\n",
       "                    0.0259, 0.0017, 0.0009, 0.0007, 0.0060, 0.0007, 0.0316, 0.0015, 0.0035,\n",
       "                    0.0105, 0.0246, 0.0111, 0.0143, 0.0075, 0.0160, 0.0218, 0.0061, 0.0221,\n",
       "                    0.0469, 0.0006, 0.0315, 0.0055, 0.0102, 0.0175, 0.0097, 0.0187, 0.0058,\n",
       "                    0.0011, 0.0093, 0.0051, 0.0026, 0.0039, 0.0188, 0.0087, 0.0006, 0.0068,\n",
       "                    0.0037, 0.0400, 0.0018, 0.0263, 0.0021, 0.0378, 0.0271, 0.0415, 0.0040,\n",
       "                    0.0047, 0.0177, 0.0016, 0.0201, 0.0008, 0.0031, 0.0098, 0.0013, 0.0037,\n",
       "                    0.0043, 0.0045, 0.0024, 0.0402, 0.0056, 0.0059, 0.0012, 0.0101, 0.0025,\n",
       "                    0.0016, 0.0011, 0.0141, 0.0148], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-1.1414e-03, -7.9345e-02, -1.2323e-02, -1.3268e-01, -3.3607e-02,\n",
       "                    -6.4945e-03, -2.9688e-01, -6.3902e-02, -2.6420e-02, -9.4013e-02,\n",
       "                    -1.1188e-01, -5.8596e-02, -4.1975e-01, -6.0443e-03, -3.1474e-02,\n",
       "                    -1.0502e-01, -1.1841e-01, -1.5543e-02, -7.8817e-02, -1.7799e-01,\n",
       "                    -2.0572e-02, -2.9040e-02, -9.9538e-02, -2.7340e-01, -6.6274e-02,\n",
       "                    -1.1405e-01, -3.4328e-01, -2.6416e-03, -4.6001e-03, -2.7294e-02,\n",
       "                    -2.6689e-02, -2.8131e-02, -1.1205e-01, -5.0678e-02, -4.5121e-03,\n",
       "                    -7.4554e-02, -8.6945e-02, -1.7296e-02, -7.1134e-02, -8.6935e-02,\n",
       "                    -3.1523e-02, -5.3821e-02, -8.1825e-02, -9.2045e-03, -1.5650e-02,\n",
       "                    -7.8501e-02, -6.2503e-01, -1.8226e-02, -2.4269e-02, -2.9170e-03,\n",
       "                    -3.6566e-02, -2.7739e-02, -2.9460e-01, -1.8712e-01, -1.6094e-01,\n",
       "                    -1.9326e-01, -3.9947e-02, -2.0054e-02, -2.1255e-02, -1.1611e-02,\n",
       "                    -3.1492e-01, -9.1533e-03, -2.7005e-01, -2.7445e-01, -8.0504e-02,\n",
       "                    -8.0682e-02, -2.1345e-03, -9.2483e-03, -7.7882e-03, -3.0055e-02,\n",
       "                    -7.1568e-02, -1.0378e-01, -3.4039e-02, -1.9170e-02, -2.5963e-04,\n",
       "                    -1.3303e-02, -6.8843e-03, -1.4628e-02, -8.0193e-02, -1.3110e-01,\n",
       "                    -1.3036e-02, -5.1230e-02, -5.7343e-03, -3.1140e-02, -1.3850e-01,\n",
       "                    -1.3682e-01, -1.9209e-02, -2.1468e-02, -7.8728e-03, -2.8536e-02,\n",
       "                    -9.6409e-03, -9.6751e-02, -8.7562e-02, -8.8279e-02, -1.2216e-01,\n",
       "                    -1.0160e-01, -3.3853e-02, -2.5886e-02, -1.2938e-01, -9.0580e-02,\n",
       "                    -9.6775e-02, -2.1482e-02, -2.0092e-02, -8.7419e-03, -1.0788e-01,\n",
       "                    -9.0101e-03, -4.9926e-02, -1.3176e-01, -8.1903e-02, -2.0246e-02,\n",
       "                    -1.7216e-01, -9.9391e-02, -1.2911e-01, -5.8478e-02, -2.3566e-02,\n",
       "                    -6.0680e-02, -2.7404e-03, -1.2793e-03, -5.2361e-01, -1.7715e-01,\n",
       "                    -1.2910e-02, -2.9553e-01, -8.3536e-02, -2.5530e-02, -8.9860e-03,\n",
       "                    -4.1932e-01, -1.0638e-01, -1.1840e-02, -1.5685e-01, -1.0538e-01,\n",
       "                    -6.6325e-03, -1.0883e-01, -1.2753e-01, -1.0096e-01, -3.4052e-03,\n",
       "                    -1.2559e-02, -8.2036e-03, -3.1943e-02, -1.0372e-01, -3.8397e-02,\n",
       "                    -2.3594e-02, -5.0859e-03, -2.3878e-01, -1.1390e-01, -5.5252e-03,\n",
       "                    -7.9993e-02, -1.9697e-01, -2.6938e-02, -7.5658e-03, -6.1712e-02,\n",
       "                    -2.7576e-02, -1.5246e-01, -1.0425e-01, -2.2827e-02, -2.9575e-01,\n",
       "                    -4.1096e-02, -1.0970e-01, -3.1008e-03, -4.1225e-02, -4.1693e-02,\n",
       "                    -1.4351e-03, -8.9332e-02, -3.9317e-02, -3.0717e-01, -4.5783e-02,\n",
       "                    -2.6498e-01, -6.2953e-02, -5.3485e-02, -4.5959e-02, -9.0820e-02,\n",
       "                    -5.1004e-02, -9.3300e-02, -2.1138e-02, -1.8570e-03, -4.5021e-02,\n",
       "                    -3.4220e-02, -6.4668e-02, -4.2596e-02, -4.2903e-02, -2.0163e-01,\n",
       "                    -1.8488e-02, -1.0917e-01, -1.9455e-01, -2.0209e-02, -8.1437e-02,\n",
       "                    -4.9006e-02, -2.6974e-01, -6.5549e-03, -1.0108e-02, -3.4925e-02,\n",
       "                    -1.7044e-02, -8.4232e-02, -5.2048e-02, -8.1791e-02, -1.0261e-01,\n",
       "                    -9.3328e-02, -1.2484e-01, -1.3295e-01, -8.5453e-02, -7.9199e-03,\n",
       "                    -1.5067e-01, -7.8729e-02, -1.2400e-02, -2.4357e-02, -3.5826e-02,\n",
       "                    -1.7854e-02, -2.2133e-01, -6.0191e-02, -2.9965e-01, -8.2939e-02,\n",
       "                    -9.2551e-02, -1.0458e-02, -2.8158e-03, -9.2420e-02, -1.6004e-01,\n",
       "                    -7.6719e-02, -8.8944e-03, -8.0822e-02, -4.4532e-02, -3.2363e-02,\n",
       "                    -4.6204e-02, -1.0036e-01, -5.3714e-02, -1.9028e-02, -7.2447e-02,\n",
       "                    -6.8045e-02, -1.3663e-01, -1.2487e-01, -8.0721e-02, -1.0032e-02,\n",
       "                    -1.0859e-03, -5.6021e-03, -2.6484e-02, -1.0789e-01, -3.0648e-03,\n",
       "                    -2.6650e-02, -2.0298e-01, -3.1573e-02, -3.5926e-02, -1.2918e-01,\n",
       "                    -3.8127e-02, -4.1347e-03, -1.4802e-02, -3.1652e-02, -1.7767e-01,\n",
       "                    -8.7386e-02, -9.2876e-03, -7.0242e-02, -6.3829e-02, -2.8201e-02,\n",
       "                    -3.3015e-03, -1.3176e-02, -1.0741e-01, -9.2479e-03, -2.3592e-02,\n",
       "                    -7.3367e-02], device='cuda:0'), max_val=tensor([1.1112e-03, 9.9697e-02, 1.0611e-02, 8.9904e-02, 3.2527e-02, 4.7896e-03,\n",
       "                    1.5322e-01, 1.9411e-01, 1.8213e-02, 9.7360e-02, 1.1203e-01, 6.9089e-02,\n",
       "                    2.6435e-01, 6.6359e-03, 4.0041e-02, 2.6064e-01, 1.8712e-01, 1.2575e-02,\n",
       "                    1.5673e-01, 4.8319e-02, 3.4899e-02, 2.6063e-02, 5.4774e-02, 1.9549e-01,\n",
       "                    3.1187e-02, 9.3582e-02, 8.5028e-01, 2.8635e-03, 3.5033e-03, 1.3715e-02,\n",
       "                    1.6085e-02, 3.5555e-02, 1.5246e-01, 3.9576e-02, 4.1941e-03, 7.1930e-02,\n",
       "                    9.7016e-02, 2.1324e-02, 6.3676e-02, 7.3700e-02, 3.0358e-02, 3.6272e-02,\n",
       "                    7.7988e-02, 7.3699e-03, 1.5485e-02, 1.2107e-01, 1.9485e-01, 2.5156e-02,\n",
       "                    3.7572e-02, 6.4638e-03, 2.9851e-02, 2.0655e-02, 3.7696e-01, 7.9571e-02,\n",
       "                    1.2907e-01, 1.0634e-01, 2.6129e-02, 2.2988e-02, 2.7980e-02, 5.8236e-03,\n",
       "                    1.4965e-01, 1.0316e-02, 8.7225e-01, 2.2459e-01, 1.0644e-01, 3.7594e-02,\n",
       "                    1.6706e-03, 6.8985e-03, 6.8958e-03, 2.9441e-02, 7.2698e-02, 1.4998e-01,\n",
       "                    5.2826e-02, 1.7824e-02, 2.4088e-04, 1.2180e-02, 6.6280e-03, 1.7873e-02,\n",
       "                    6.7537e-02, 1.1165e-01, 1.2405e-02, 3.7352e-02, 6.6889e-03, 4.4348e-02,\n",
       "                    1.5097e-01, 1.4771e-01, 2.4875e-02, 2.5384e-02, 5.9919e-03, 2.9788e-02,\n",
       "                    8.6750e-03, 1.2066e-01, 7.1998e-02, 7.2531e-02, 7.2436e-02, 9.4202e-02,\n",
       "                    3.5161e-02, 2.1250e-02, 1.0901e-01, 6.7177e-02, 1.1031e-01, 2.5726e-02,\n",
       "                    1.7762e-02, 8.4921e-03, 9.8624e-02, 9.5716e-03, 5.1928e-02, 7.9420e-02,\n",
       "                    6.4954e-02, 1.5816e-02, 9.8182e-02, 7.1607e-02, 1.0367e-01, 3.0511e-02,\n",
       "                    2.7485e-02, 6.5970e-02, 2.5137e-03, 1.3356e-03, 5.1108e-01, 1.4047e-01,\n",
       "                    9.7962e-03, 3.5870e-01, 1.0849e-01, 2.9923e-02, 7.8975e-03, 2.2612e-01,\n",
       "                    8.9324e-02, 1.7290e-02, 1.6298e-01, 6.1762e-02, 9.0471e-03, 1.0979e-01,\n",
       "                    6.8670e-02, 7.0928e-02, 5.0812e-03, 9.3188e-03, 1.4195e-02, 3.1387e-02,\n",
       "                    2.1485e-01, 4.8521e-02, 3.0654e-02, 3.9950e-03, 3.1950e-01, 1.3160e-01,\n",
       "                    8.0963e-03, 3.9335e-02, 1.4061e-01, 1.7383e-02, 7.5366e-03, 3.6598e-02,\n",
       "                    1.9720e-02, 2.1732e-01, 7.7983e-02, 2.0702e-02, 3.0864e-01, 2.2165e-02,\n",
       "                    1.2864e-01, 2.4995e-03, 3.2172e-02, 5.7458e-02, 1.3440e-03, 4.1862e-02,\n",
       "                    4.2925e-02, 2.8010e-01, 4.4458e-02, 5.1573e-01, 7.0186e-02, 5.3205e-02,\n",
       "                    5.6682e-02, 6.9227e-02, 5.7166e-02, 1.3634e-01, 1.8815e-02, 3.0621e-03,\n",
       "                    3.5253e-02, 5.0849e-02, 4.6314e-02, 5.0191e-02, 2.4234e-02, 1.3401e-01,\n",
       "                    1.3048e-02, 8.4104e-02, 1.7676e-01, 2.3997e-02, 6.6985e-02, 5.0005e-02,\n",
       "                    2.5874e-01, 5.1889e-03, 1.0582e-02, 3.3152e-02, 8.8231e-03, 6.8475e-02,\n",
       "                    5.5492e-02, 8.6506e-02, 8.3165e-02, 8.7459e-02, 8.8198e-02, 6.8718e-02,\n",
       "                    7.9366e-02, 5.7050e-03, 1.2713e-01, 8.7671e-02, 1.3915e-02, 2.2153e-02,\n",
       "                    3.6021e-02, 2.7791e-02, 3.0489e-01, 7.6434e-02, 2.6294e-01, 7.3419e-02,\n",
       "                    8.8089e-02, 8.8904e-03, 2.3225e-03, 8.5905e-02, 2.4080e-01, 8.2619e-02,\n",
       "                    1.1137e-02, 8.1550e-02, 4.7074e-02, 3.0823e-02, 6.3378e-02, 9.6252e-02,\n",
       "                    4.2355e-02, 1.6547e-02, 4.6822e-02, 6.9413e-02, 1.4808e-01, 3.3778e-02,\n",
       "                    7.6775e-02, 2.4143e-02, 1.7807e-03, 6.1768e-03, 2.6689e-02, 1.4396e-01,\n",
       "                    3.0819e-03, 3.9429e-02, 1.2872e-01, 1.5905e-02, 2.7880e-02, 7.7629e-02,\n",
       "                    2.3947e-02, 4.4589e-03, 1.8066e-02, 4.2523e-02, 3.1763e-01, 1.5445e-01,\n",
       "                    1.5414e-02, 4.8021e-02, 4.4980e-02, 2.5452e-02, 2.6121e-03, 1.3762e-02,\n",
       "                    8.3633e-02, 5.8752e-03, 2.8794e-02, 7.2982e-02], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.0149, -0.0375, -0.0238, -0.0387, -0.0376, -0.0266, -0.0356, -0.0370,\n",
       "                    -0.0205, -0.0324, -0.0545, -0.0397, -0.0255, -0.0281, -0.0156, -0.0459,\n",
       "                    -0.0170, -0.0232, -0.0226, -0.0621, -0.0378, -0.0170, -0.0596, -0.0193,\n",
       "                    -0.0410, -0.0161, -0.0188, -0.0103, -0.0232, -0.0168, -0.0399, -0.0229,\n",
       "                    -0.0138, -0.0227, -0.0419, -0.0873, -0.0221, -0.0632, -0.0321, -0.0266,\n",
       "                    -0.0631, -0.0111, -0.0576, -0.0196, -0.0176, -0.0172, -0.0201, -0.0422,\n",
       "                    -0.0325, -0.0318, -0.0406, -0.0416, -0.0230, -0.0283, -0.0526, -0.0115,\n",
       "                    -0.0338, -0.0335, -0.0421, -0.0527, -0.0122, -0.0435, -0.0200, -0.0213,\n",
       "                    -0.0223, -0.0325, -0.0507, -0.0203, -0.0263, -0.0281, -0.0229, -0.0297,\n",
       "                    -0.0143, -0.0208, -0.0204, -0.0149, -0.0205, -0.0149, -0.0657, -0.0283,\n",
       "                    -0.0252, -0.0201, -0.0339, -0.0176, -0.0356, -0.0233, -0.0103, -0.0301,\n",
       "                    -0.0259, -0.0187, -0.0310, -0.0568, -0.0447, -0.0261, -0.0259, -0.0353,\n",
       "                    -0.0430, -0.0570, -0.0539, -0.0187, -0.0338, -0.0144, -0.0288, -0.0389,\n",
       "                    -0.0191, -0.0186, -0.0156, -0.0599, -0.0434, -0.0155, -0.0108, -0.0767,\n",
       "                    -0.0244, -0.0226, -0.0184, -0.0151, -0.0201, -0.0285, -0.0370, -0.0310,\n",
       "                    -0.0125, -0.0211, -0.0255, -0.0420, -0.0315, -0.0506, -0.0230, -0.0142,\n",
       "                    -0.0460, -0.0291, -0.0304, -0.0667, -0.0122, -0.0172, -0.0196, -0.0745,\n",
       "                    -0.0178, -0.0550, -0.0297, -0.0206, -0.0377, -0.0182, -0.0249, -0.0181,\n",
       "                    -0.0295, -0.0161, -0.0297, -0.0138, -0.0375, -0.0231, -0.0073, -0.0128,\n",
       "                    -0.0273, -0.0210, -0.0178, -0.0119, -0.0227, -0.0220, -0.0730, -0.0234,\n",
       "                    -0.0558, -0.0510, -0.0257, -0.0415, -0.0342, -0.0223, -0.0183, -0.0177,\n",
       "                    -0.0167, -0.0175, -0.0238, -0.0146, -0.0173, -0.0289, -0.0208, -0.0104,\n",
       "                    -0.0487, -0.0192, -0.0106, -0.0117, -0.0145, -0.0214, -0.0318, -0.0465,\n",
       "                    -0.0217, -0.0273, -0.0200, -0.0360, -0.0176, -0.0225, -0.0166, -0.0299,\n",
       "                    -0.0443, -0.0112, -0.0220, -0.0248, -0.0262, -0.0401, -0.0398, -0.0240,\n",
       "                    -0.0176, -0.0104, -0.0242, -0.0430, -0.0387, -0.0437, -0.0198, -0.0111,\n",
       "                    -0.0328, -0.0341, -0.0741, -0.0241, -0.0238, -0.0440, -0.0159, -0.0281,\n",
       "                    -0.0166, -0.0423, -0.0302, -0.0229, -0.0145, -0.0286, -0.0067, -0.0383,\n",
       "                    -0.0511, -0.0120, -0.0177, -0.0134, -0.0302, -0.0201, -0.0132, -0.0492,\n",
       "                    -0.0081, -0.0167, -0.0277, -0.0171, -0.0172, -0.0247, -0.0377, -0.0013,\n",
       "                    -0.0504, -0.0390, -0.0352, -0.0182, -0.0441, -0.0681, -0.0083, -0.0217,\n",
       "                    -0.0179, -0.0084, -0.0501, -0.0269, -0.0208, -0.0331, -0.0191, -0.0262,\n",
       "                    -0.0132, -0.0476, -0.0395, -0.0386, -0.0304, -0.0485, -0.0243, -0.0171,\n",
       "                    -0.0159, -0.0159, -0.0209, -0.0307, -0.0313, -0.0296, -0.0261, -0.0138,\n",
       "                    -0.0390, -0.0247, -0.0055, -0.0266, -0.0345, -0.0333, -0.0274, -0.0429,\n",
       "                    -0.0335, -0.0194, -0.0163, -0.0461, -0.0160, -0.0156, -0.0219, -0.0128,\n",
       "                    -0.0082, -0.0402, -0.0640, -0.0307, -0.0362, -0.0691, -0.0170, -0.0165,\n",
       "                    -0.0289, -0.0410, -0.0184, -0.0400, -0.0500, -0.0235, -0.0114, -0.0685,\n",
       "                    -0.0320, -0.0082, -0.0102, -0.0317, -0.0180, -0.0319, -0.0299, -0.0266,\n",
       "                    -0.0498, -0.0253, -0.0474, -0.0197, -0.0180, -0.0329, -0.0393, -0.0118,\n",
       "                    -0.0425, -0.0330, -0.0308, -0.0354, -0.0576, -0.0361, -0.0348, -0.0152,\n",
       "                    -0.0158, -0.0776, -0.0342, -0.0284, -0.0298, -0.0872, -0.0223, -0.0142,\n",
       "                    -0.0198, -0.0270, -0.0209, -0.0373, -0.0283, -0.0598, -0.0200, -0.0217,\n",
       "                    -0.0224, -0.0149, -0.0092, -0.0335, -0.0543, -0.0303, -0.0289, -0.0126,\n",
       "                    -0.0170, -0.0227, -0.0237, -0.0262, -0.0480, -0.0300, -0.0287, -0.0381,\n",
       "                    -0.0560, -0.0220, -0.0209, -0.0270, -0.0284, -0.0288, -0.0254, -0.0377,\n",
       "                    -0.0333, -0.0178, -0.0152, -0.0530, -0.0231, -0.0134, -0.0231, -0.0422,\n",
       "                    -0.0385, -0.0190, -0.0371, -0.0296, -0.0358, -0.0809, -0.0520, -0.0254,\n",
       "                    -0.0354, -0.0315, -0.0088, -0.0312, -0.0154, -0.0592, -0.0390, -0.0190,\n",
       "                    -0.0268, -0.0155, -0.0362, -0.0306, -0.0109, -0.0240, -0.0150, -0.0688,\n",
       "                    -0.0117, -0.0289, -0.0306, -0.0188, -0.0481, -0.0868, -0.0225, -0.0381,\n",
       "                    -0.0636, -0.0484, -0.0619, -0.0195, -0.0313, -0.0355, -0.0218, -0.0249,\n",
       "                    -0.0216, -0.0207, -0.0260, -0.0245, -0.0357, -0.0246, -0.0099, -0.0239,\n",
       "                    -0.0505, -0.0160, -0.0266, -0.0203, -0.0245, -0.0154, -0.0361, -0.0294,\n",
       "                    -0.0175, -0.0443, -0.0575, -0.0101, -0.0498, -0.0430, -0.0194, -0.0295,\n",
       "                    -0.0173, -0.0688, -0.0164, -0.0379, -0.0864, -0.0524, -0.0307, -0.0161,\n",
       "                    -0.0125, -0.0265, -0.0412, -0.0512, -0.0489, -0.0298, -0.0341, -0.0267,\n",
       "                    -0.0392, -0.0451, -0.0058, -0.0272, -0.0190, -0.0157, -0.0376, -0.0323,\n",
       "                    -0.0271, -0.0228, -0.0328, -0.0257, -0.0162, -0.0149, -0.0512, -0.0224,\n",
       "                    -0.0164, -0.0098, -0.0324, -0.0440, -0.0734, -0.0193, -0.0240, -0.0216,\n",
       "                    -0.0297, -0.0257, -0.0820, -0.0272, -0.0285, -0.0360, -0.0465, -0.0246,\n",
       "                    -0.0292, -0.0305, -0.0358, -0.0158, -0.0157, -0.0456, -0.0132, -0.0491,\n",
       "                    -0.0267, -0.0182, -0.0372, -0.0171, -0.0476, -0.0327, -0.0561, -0.0100,\n",
       "                    -0.0491, -0.0194, -0.0192, -0.0537, -0.0379, -0.0227, -0.0353, -0.0342],\n",
       "                   device='cuda:0'), max_val=tensor([0.0147, 0.0408, 0.0230, 0.0252, 0.0621, 0.0230, 0.0388, 0.0403, 0.0389,\n",
       "                    0.0251, 0.0590, 0.0439, 0.0187, 0.0441, 0.0168, 0.0504, 0.0116, 0.0170,\n",
       "                    0.0165, 0.0703, 0.0491, 0.0182, 0.0700, 0.0149, 0.0651, 0.0296, 0.0189,\n",
       "                    0.0101, 0.0202, 0.0111, 0.0513, 0.0181, 0.0120, 0.0143, 0.0373, 0.1139,\n",
       "                    0.0235, 0.0610, 0.0261, 0.0236, 0.0607, 0.0270, 0.0417, 0.0374, 0.0253,\n",
       "                    0.0160, 0.0130, 0.0791, 0.0255, 0.0273, 0.0245, 0.0298, 0.0166, 0.0196,\n",
       "                    0.0384, 0.0179, 0.0380, 0.0315, 0.0261, 0.0400, 0.0154, 0.0365, 0.0176,\n",
       "                    0.0155, 0.0307, 0.0212, 0.0318, 0.0228, 0.0308, 0.0209, 0.0251, 0.0333,\n",
       "                    0.0316, 0.0287, 0.0164, 0.0196, 0.0162, 0.0133, 0.1394, 0.0284, 0.0265,\n",
       "                    0.0198, 0.0257, 0.0176, 0.0187, 0.0461, 0.0195, 0.0354, 0.0302, 0.0280,\n",
       "                    0.0240, 0.0595, 0.0287, 0.0316, 0.0338, 0.0302, 0.0484, 0.0388, 0.0403,\n",
       "                    0.0181, 0.0326, 0.0120, 0.0304, 0.0468, 0.0230, 0.0186, 0.0249, 0.0752,\n",
       "                    0.0353, 0.0116, 0.0118, 0.1070, 0.0216, 0.0206, 0.0203, 0.0337, 0.0248,\n",
       "                    0.0498, 0.0469, 0.0307, 0.0080, 0.0144, 0.0202, 0.0455, 0.0240, 0.0502,\n",
       "                    0.0301, 0.0218, 0.0498, 0.0178, 0.0480, 0.1166, 0.0216, 0.0161, 0.0120,\n",
       "                    0.1098, 0.0241, 0.0548, 0.0378, 0.0205, 0.0464, 0.0141, 0.0168, 0.0170,\n",
       "                    0.0309, 0.0158, 0.0216, 0.0123, 0.0444, 0.0216, 0.0213, 0.0151, 0.0253,\n",
       "                    0.0270, 0.0246, 0.0124, 0.0102, 0.0226, 0.0875, 0.0370, 0.0819, 0.0523,\n",
       "                    0.0294, 0.0533, 0.0215, 0.0261, 0.0223, 0.0180, 0.0155, 0.0276, 0.0250,\n",
       "                    0.0119, 0.0146, 0.0362, 0.0308, 0.0108, 0.0380, 0.0170, 0.0233, 0.0182,\n",
       "                    0.0122, 0.0182, 0.0374, 0.0555, 0.0220, 0.0284, 0.0194, 0.0349, 0.0179,\n",
       "                    0.0273, 0.0167, 0.0429, 0.0662, 0.0149, 0.0322, 0.0388, 0.0426, 0.0560,\n",
       "                    0.0276, 0.0287, 0.0208, 0.0165, 0.0152, 0.0684, 0.0321, 0.0451, 0.0152,\n",
       "                    0.0356, 0.0311, 0.0428, 0.0871, 0.0159, 0.0401, 0.0320, 0.0132, 0.0247,\n",
       "                    0.0078, 0.0551, 0.0362, 0.0154, 0.0202, 0.0372, 0.0068, 0.0394, 0.0490,\n",
       "                    0.0162, 0.0140, 0.0269, 0.0197, 0.0184, 0.0235, 0.0444, 0.0226, 0.0106,\n",
       "                    0.0215, 0.0196, 0.0268, 0.0274, 0.0424, 0.0017, 0.0367, 0.0437, 0.0631,\n",
       "                    0.0266, 0.0351, 0.0499, 0.0083, 0.0190, 0.0162, 0.0117, 0.0443, 0.0322,\n",
       "                    0.0160, 0.0310, 0.0170, 0.0311, 0.0112, 0.0671, 0.0262, 0.0337, 0.0250,\n",
       "                    0.0766, 0.0239, 0.0172, 0.0083, 0.0241, 0.0201, 0.0224, 0.0227, 0.0241,\n",
       "                    0.0213, 0.0141, 0.0573, 0.0199, 0.0080, 0.0450, 0.0407, 0.0288, 0.0222,\n",
       "                    0.0359, 0.0280, 0.0171, 0.0135, 0.0425, 0.0151, 0.0151, 0.0181, 0.0083,\n",
       "                    0.0236, 0.0283, 0.0437, 0.0239, 0.0338, 0.0531, 0.0212, 0.0239, 0.0209,\n",
       "                    0.0292, 0.0287, 0.0280, 0.0433, 0.0160, 0.0141, 0.0801, 0.0207, 0.0049,\n",
       "                    0.0194, 0.0352, 0.0113, 0.0225, 0.0345, 0.0254, 0.0450, 0.0338, 0.0487,\n",
       "                    0.0271, 0.0418, 0.0441, 0.0385, 0.0109, 0.0436, 0.0366, 0.0376, 0.0241,\n",
       "                    0.0918, 0.0239, 0.0312, 0.0151, 0.0196, 0.0651, 0.0474, 0.0372, 0.0346,\n",
       "                    0.0897, 0.0244, 0.0121, 0.0257, 0.0241, 0.0173, 0.0622, 0.0245, 0.0603,\n",
       "                    0.0214, 0.0183, 0.0152, 0.0152, 0.0075, 0.0484, 0.0538, 0.0490, 0.0313,\n",
       "                    0.0117, 0.0156, 0.0242, 0.0249, 0.0297, 0.0534, 0.0299, 0.0182, 0.0341,\n",
       "                    0.0487, 0.0239, 0.0224, 0.0172, 0.0326, 0.0148, 0.0317, 0.0276, 0.0292,\n",
       "                    0.0160, 0.0100, 0.0596, 0.0254, 0.0238, 0.0390, 0.0453, 0.0474, 0.0164,\n",
       "                    0.0303, 0.0356, 0.0421, 0.0952, 0.0469, 0.0191, 0.0366, 0.0406, 0.0061,\n",
       "                    0.0249, 0.0176, 0.0724, 0.0521, 0.0217, 0.0212, 0.0204, 0.0267, 0.0317,\n",
       "                    0.0181, 0.0283, 0.0129, 0.0789, 0.0113, 0.0232, 0.0569, 0.0190, 0.0509,\n",
       "                    0.0936, 0.0183, 0.0275, 0.0727, 0.0688, 0.0483, 0.0344, 0.0308, 0.0321,\n",
       "                    0.0204, 0.0273, 0.0307, 0.0178, 0.0359, 0.0206, 0.0355, 0.0445, 0.0073,\n",
       "                    0.0340, 0.0529, 0.0127, 0.0270, 0.0282, 0.0194, 0.0178, 0.0276, 0.0454,\n",
       "                    0.0241, 0.0322, 0.0978, 0.0098, 0.0383, 0.0619, 0.0251, 0.0446, 0.0153,\n",
       "                    0.0744, 0.0200, 0.0299, 0.1214, 0.0435, 0.0364, 0.0103, 0.0331, 0.0262,\n",
       "                    0.0555, 0.0420, 0.0738, 0.0291, 0.0299, 0.0323, 0.0321, 0.0537, 0.0191,\n",
       "                    0.0174, 0.0204, 0.0205, 0.0223, 0.0494, 0.0229, 0.0244, 0.0738, 0.0188,\n",
       "                    0.0154, 0.0229, 0.0577, 0.0180, 0.0157, 0.0120, 0.0316, 0.0588, 0.0530,\n",
       "                    0.0287, 0.0298, 0.0230, 0.0327, 0.0164, 0.0854, 0.0330, 0.0259, 0.0373,\n",
       "                    0.0510, 0.0168, 0.0472, 0.0448, 0.0246, 0.0201, 0.0114, 0.0575, 0.0146,\n",
       "                    0.0348, 0.0182, 0.0173, 0.0449, 0.0224, 0.0513, 0.0425, 0.0544, 0.0212,\n",
       "                    0.0464, 0.0175, 0.0213, 0.0814, 0.0300, 0.0239, 0.0371, 0.0290],\n",
       "                   device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-8.0513e-02, -4.7765e-02, -1.0356e-01, -3.8210e-02, -1.5748e-01,\n",
       "                    -2.9623e-02, -8.4123e-02, -1.7510e-01, -2.8982e-02, -4.5394e-02,\n",
       "                    -4.1325e-02, -3.4889e-03, -4.2529e-02, -1.3716e-01, -6.2069e-02,\n",
       "                    -2.4258e-02, -1.5720e-01, -6.8992e-02, -5.0196e-02, -1.9900e-02,\n",
       "                    -7.9451e-02, -7.7997e-02, -1.2330e-01, -4.7549e-02, -6.2575e-02,\n",
       "                    -8.6188e-02, -3.8635e-02, -2.2682e-02, -1.0370e-01, -1.1125e-01,\n",
       "                    -1.0997e-01, -1.6548e-01, -1.3609e-01, -4.7414e-02, -6.0871e-02,\n",
       "                    -2.5236e-01, -8.1475e-02, -6.4165e-02, -1.0219e-01, -1.3428e-01,\n",
       "                    -3.2392e-02, -8.3240e-02, -1.6994e-04, -6.5742e-02, -1.8135e-01,\n",
       "                    -2.5079e-02, -1.0787e-01, -3.5476e-02, -5.8508e-02, -8.2879e-02,\n",
       "                    -8.3221e-02, -2.0128e-01, -6.3651e-02, -2.9193e-01, -1.8922e-01,\n",
       "                    -1.8918e-01, -3.2589e-02, -3.6858e-02, -4.0327e-02, -5.3121e-01,\n",
       "                    -1.2683e-01, -6.6776e-02, -5.0344e-02, -2.1235e-01, -4.8269e-02,\n",
       "                    -4.1765e-03, -7.1849e-02, -4.0897e-02, -2.1077e-01, -3.0904e-02,\n",
       "                    -7.3659e-02, -3.8002e-02, -2.3897e-01, -1.7594e-01, -3.4174e-02,\n",
       "                    -1.1560e-01, -1.1128e-02, -2.8290e-01, -9.6107e-02, -3.6142e-02,\n",
       "                    -2.5058e-01, -1.6279e-01, -2.7936e-02, -9.0295e-02, -1.4373e-01,\n",
       "                    -7.7631e-02, -1.0652e-01, -5.0571e-02, -3.9325e-01, -1.0304e-01,\n",
       "                    -1.1361e-01, -1.6266e-01, -4.8675e-02, -1.6987e-01, -4.6265e-02,\n",
       "                    -3.6627e-02, -2.8073e-01, -4.5635e-02, -1.4625e-01, -5.7164e-02,\n",
       "                    -1.2169e-01, -3.7247e-02, -8.4990e-02, -2.1188e-01, -2.7817e-02,\n",
       "                    -1.7459e-01, -1.2628e-01, -7.4429e-02, -2.0530e-01, -1.7035e-01,\n",
       "                    -2.2078e-02, -4.0592e-01, -2.7182e-02, -1.9223e-01, -7.7319e-02,\n",
       "                    -8.8318e-02, -2.7925e-01, -5.6615e-02, -7.4638e-02, -1.0225e-01,\n",
       "                    -1.2422e-01, -4.3911e-02, -1.5622e-01, -1.6382e-02, -1.3691e-02,\n",
       "                    -8.2644e-02, -7.8024e-02, -5.6916e-02, -9.0291e-02, -3.4133e-02,\n",
       "                    -1.0768e-01, -5.6036e-02, -2.1254e-01, -1.1899e-01, -4.2277e-02,\n",
       "                    -5.8394e-02, -8.9739e-02, -6.0625e-02, -3.7329e-02, -4.5575e-02,\n",
       "                    -1.9724e-01, -1.5513e-01, -4.4954e-02, -2.1711e-02, -6.1444e-02,\n",
       "                    -2.1347e-01, -7.0957e-02, -1.9477e-01, -1.8515e-01, -8.6698e-02,\n",
       "                    -1.1571e-02, -1.3632e-01, -4.4618e-01, -1.8060e-01, -2.8796e-02,\n",
       "                    -7.9856e-02, -4.9778e-02, -2.6772e-02, -4.0636e-02, -3.0975e-02,\n",
       "                    -5.0800e-02, -5.4651e-02, -6.3243e-02, -6.1945e-02, -1.8021e-02,\n",
       "                    -3.7891e-03, -1.1934e-01, -1.3004e-01, -2.4900e-01, -1.4545e-01,\n",
       "                    -2.4652e-01, -2.6286e-01, -8.1631e-02, -3.8656e-02, -8.1433e-02,\n",
       "                    -2.8747e-02, -4.8286e-02, -1.0855e-01, -1.7646e-01, -1.8480e-01,\n",
       "                    -5.0263e-02, -1.3136e-02, -4.4635e-02, -1.9179e-01, -5.5468e-02,\n",
       "                    -1.7092e-01, -1.3967e-01, -2.0953e-02, -1.8287e-02, -9.6625e-02,\n",
       "                    -1.9950e-03, -1.6076e-01, -2.0730e-02, -3.9683e-02, -8.7496e-02,\n",
       "                    -2.2518e-01, -7.0147e-02, -4.5604e-02, -6.5066e-02, -4.6040e-02,\n",
       "                    -5.2114e-03, -1.4973e-01, -5.6445e-02, -1.4999e-01, -2.1210e-01,\n",
       "                    -1.5690e-01, -7.3883e-02, -4.4980e-02, -1.8247e-01, -2.1252e-01,\n",
       "                    -2.4739e-02, -2.3343e-01, -8.5927e-02, -3.2394e-02, -3.6094e-02,\n",
       "                    -8.1590e-02, -3.0330e-01, -7.0966e-02, -7.1406e-02, -1.3017e-01,\n",
       "                    -1.8413e-01, -4.9723e-02, -1.6087e-01, -6.9890e-02, -1.5086e-01,\n",
       "                    -7.9476e-02, -1.4908e-01, -1.8962e-01, -2.1407e-01, -1.2265e-01,\n",
       "                    -1.7797e-01, -8.8941e-02, -1.3752e-01, -1.8396e-01, -1.1969e-01,\n",
       "                    -1.0500e-02, -1.1907e-01, -1.3737e-01, -1.3458e-01, -3.1358e-02,\n",
       "                    -2.0016e-01, -4.6302e-02, -6.0817e-02, -7.7823e-02, -1.4423e-01,\n",
       "                    -5.2840e-02, -1.9345e-01, -1.0141e-01, -2.2686e-01, -2.1359e-01,\n",
       "                    -3.2960e-02, -1.6033e-01, -1.7303e-02, -5.6995e-02, -2.4420e-01,\n",
       "                    -1.1115e-01, -3.0079e-01, -1.4365e-02, -1.1529e-01, -3.4346e-02,\n",
       "                    -1.3957e-01, -3.4185e-02, -4.7701e-02, -1.8455e-01, -8.0248e-02,\n",
       "                    -2.1346e-02, -5.9203e-02, -3.1462e-01, -1.6793e-01, -5.4885e-02,\n",
       "                    -8.6498e-02, -1.7024e-01, -1.1943e-01, -4.0786e-02, -2.0741e-02,\n",
       "                    -2.2780e-03, -7.7697e-02, -9.3697e-02, -3.8835e-02, -2.7843e-02,\n",
       "                    -7.7303e-02, -1.2509e-01, -8.3151e-02, -1.1996e-01, -7.4605e-02,\n",
       "                    -1.0634e-01, -1.6510e-01, -1.0101e-01, -4.2446e-02, -1.1400e-01,\n",
       "                    -4.1852e-02, -7.1908e-02, -2.4254e-01, -2.7691e-02, -1.9001e-01,\n",
       "                    -4.3972e-02, -1.8273e-01, -7.9659e-02, -1.8591e-01, -2.0581e-01,\n",
       "                    -2.0826e-01, -1.2063e-02, -2.6968e-02, -1.2816e-01, -4.2092e-02,\n",
       "                    -3.1568e-02, -1.5700e-02, -5.1822e-02, -3.5207e-02, -1.0597e-01,\n",
       "                    -3.5939e-02, -1.1745e-01, -3.0079e-02, -2.7767e-02, -2.7934e-02,\n",
       "                    -1.7470e-01, -7.9372e-02, -1.6163e-01, -3.3971e-02, -3.3827e-02,\n",
       "                    -1.6879e-01, -1.2173e-01, -2.3270e-02, -4.7783e-02, -1.8809e-02,\n",
       "                    -1.5476e-01, -5.8785e-02, -7.5284e-02, -2.0168e-01, -4.4105e-02,\n",
       "                    -1.4658e-01, -1.7117e-01, -2.5859e-01, -9.1233e-02, -5.9980e-02,\n",
       "                    -2.0508e-01, -2.6274e-02, -3.1963e-01, -2.7312e-02, -4.9768e-02,\n",
       "                    -5.8298e-02, -1.6795e-02, -5.8838e-02, -1.6216e-01, -4.2777e-02,\n",
       "                    -2.8594e-02, -4.0106e-02, -6.4540e-02, -5.8686e-02, -4.6880e-02,\n",
       "                    -7.3155e-02, -3.2460e-02, -1.2845e-01, -1.0659e-01, -1.4467e-01,\n",
       "                    -6.1230e-02, -1.1895e-01, -1.1679e-01, -2.5410e-01, -1.9498e-01,\n",
       "                    -3.4718e-02, -1.5625e-01, -1.0929e-01, -5.6584e-02, -5.4080e-02,\n",
       "                    -6.3265e-02, -2.2242e-01, -7.9933e-02, -2.3949e-01, -6.0983e-02,\n",
       "                    -4.8080e-02, -7.7586e-02, -1.4796e-02, -7.8205e-02, -7.2121e-02,\n",
       "                    -9.5779e-02, -1.0517e-01, -5.3463e-02, -4.1700e-02, -8.5971e-02,\n",
       "                    -1.7705e-01, -2.3400e-01, -2.5248e-01, -6.5130e-02, -1.4978e-01,\n",
       "                    -3.8227e-02, -1.6926e-02, -9.9310e-02, -1.1145e-01, -2.8744e-01,\n",
       "                    -5.3456e-02, -3.1524e-01, -1.3975e-01, -9.2269e-02, -7.5275e-02,\n",
       "                    -6.6580e-02, -1.5635e-01, -5.8382e-02, -7.2032e-02, -7.0872e-02,\n",
       "                    -2.6085e-01, -2.0340e-01, -1.6416e-01, -4.1384e-02, -3.7117e-02,\n",
       "                    -1.4214e-01, -7.0380e-02, -4.2539e-02, -1.4317e-01, -1.1542e-01,\n",
       "                    -8.0714e-02, -2.1977e-01, -1.5491e-02, -1.1645e-01, -1.3453e-01,\n",
       "                    -1.7232e-01, -1.0316e-01, -2.3792e-01, -2.0762e-01, -6.4271e-02,\n",
       "                    -1.4040e-01, -6.2132e-02, -7.4276e-02, -6.0105e-02, -4.2374e-02,\n",
       "                    -6.2618e-02, -2.8194e-01, -1.3545e-01, -1.4507e-01, -7.3788e-02,\n",
       "                    -1.8140e-02, -1.7100e-01, -6.9810e-02, -2.7533e-02, -2.9849e-03,\n",
       "                    -2.0603e-01, -3.5865e-02, -8.3838e-02, -1.0743e-01, -1.1932e-01,\n",
       "                    -3.9173e-01, -2.6339e-01, -2.3955e-01, -7.3173e-02, -3.6673e-02,\n",
       "                    -6.7971e-02, -2.4392e-02, -3.5657e-02, -1.7249e-01, -1.9794e-01,\n",
       "                    -2.4495e-02, -1.2363e-01, -1.7814e-02, -1.1364e-01, -1.2999e-01,\n",
       "                    -1.9796e-02, -3.4144e-02, -2.7914e-01, -4.2617e-02, -3.1594e-02,\n",
       "                    -3.0926e-02, -4.5518e-02, -8.6533e-02, -6.9511e-02, -4.2352e-01,\n",
       "                    -9.8852e-02, -1.0118e-01, -8.5725e-02, -5.6502e-02, -3.5084e-02,\n",
       "                    -2.0658e-01, -6.0738e-02, -8.9798e-02, -1.4594e-01, -9.1736e-02,\n",
       "                    -3.3436e-01, -3.8404e-02, -8.6521e-02, -8.4039e-02, -1.1521e-01,\n",
       "                    -2.1340e-02, -5.2566e-02, -2.6695e-02, -2.5170e-01, -2.2328e-01,\n",
       "                    -4.0158e-02, -9.5545e-02, -1.5960e-01, -9.8931e-02, -3.7940e-02,\n",
       "                    -3.1848e-02, -2.5424e-01, -7.0350e-02, -4.4307e-02, -1.5615e-01,\n",
       "                    -1.1608e-01, -8.3578e-02, -3.7011e-02, -2.8074e-01, -6.2913e-02,\n",
       "                    -1.7084e-01, -6.6560e-02, -9.8882e-02, -3.0229e-01, -1.0513e-02,\n",
       "                    -1.1556e-01, -1.5025e-01, -8.4478e-02, -1.2203e-01, -1.1949e-01,\n",
       "                    -4.5831e-02, -1.0761e-01], device='cuda:0'), max_val=tensor([1.0929e-01, 4.8307e-02, 2.1222e-01, 5.1371e-02, 2.3440e-01, 4.1708e-02,\n",
       "                    1.1894e-01, 1.8416e-01, 4.3616e-02, 1.0770e-01, 4.1399e-02, 6.4423e-03,\n",
       "                    8.5473e-02, 2.0544e-01, 7.9189e-02, 3.3926e-02, 1.7725e-01, 2.0058e-01,\n",
       "                    4.2442e-02, 2.5279e-02, 9.1749e-02, 2.3872e-01, 1.1877e-01, 5.4326e-02,\n",
       "                    6.9366e-02, 1.2918e-01, 4.1727e-02, 2.8157e-02, 1.4135e-01, 1.4070e-01,\n",
       "                    1.5297e-01, 2.5772e-01, 1.2941e-01, 4.3392e-02, 1.5666e-01, 3.3115e-01,\n",
       "                    8.9982e-02, 9.9188e-02, 1.0053e-01, 2.1333e-01, 4.0021e-02, 9.0520e-02,\n",
       "                    2.0843e-04, 7.7410e-02, 1.4473e-01, 3.8033e-02, 1.0739e-01, 1.0365e-01,\n",
       "                    7.3052e-02, 1.9137e-01, 1.1360e-01, 1.8925e-01, 8.2107e-02, 3.8162e-01,\n",
       "                    1.6886e-01, 1.5947e-01, 4.4744e-02, 3.5387e-02, 8.0124e-02, 4.2477e-01,\n",
       "                    1.7452e-01, 6.7728e-02, 8.4947e-02, 2.4103e-01, 5.5002e-02, 5.9755e-03,\n",
       "                    9.0649e-02, 6.6587e-02, 2.3939e-01, 5.6846e-02, 8.1625e-02, 4.4378e-02,\n",
       "                    2.0565e-01, 1.4070e-01, 5.3090e-02, 2.7836e-01, 1.9085e-02, 2.2072e-01,\n",
       "                    1.2615e-01, 4.5720e-02, 3.1845e-01, 1.7630e-01, 5.5343e-02, 8.0757e-02,\n",
       "                    2.3273e-01, 2.1786e-01, 7.0636e-02, 6.0182e-02, 2.5816e-01, 1.2613e-01,\n",
       "                    1.6130e-01, 1.7435e-01, 7.5559e-02, 2.3455e-01, 5.0725e-02, 5.8487e-02,\n",
       "                    2.5955e-01, 8.0372e-02, 2.4442e-01, 5.2910e-02, 2.1701e-01, 7.8529e-02,\n",
       "                    1.1467e-01, 1.5167e-01, 5.1246e-02, 2.5881e-01, 2.0447e-01, 8.0264e-02,\n",
       "                    3.0697e-01, 1.6390e-01, 3.4684e-02, 4.0138e-01, 4.1089e-02, 2.7382e-01,\n",
       "                    5.8717e-02, 1.3367e-01, 2.3148e-01, 7.0454e-02, 1.3798e-01, 1.4305e-01,\n",
       "                    1.2646e-01, 4.0162e-02, 2.1146e-01, 3.0983e-02, 4.2200e-02, 1.3709e-01,\n",
       "                    7.9777e-02, 6.2407e-02, 1.8784e-01, 5.6787e-02, 1.9975e-01, 1.0210e-01,\n",
       "                    3.4578e-01, 1.9045e-01, 3.5891e-02, 1.0075e-01, 1.2712e-01, 6.1349e-02,\n",
       "                    6.6407e-02, 1.3637e-01, 2.4801e-01, 2.5760e-01, 8.9197e-02, 2.1653e-02,\n",
       "                    6.3580e-02, 2.0353e-01, 1.2916e-01, 2.6060e-01, 2.8492e-01, 6.0455e-02,\n",
       "                    2.8256e-02, 1.9277e-01, 3.7437e-01, 1.6755e-01, 4.2082e-02, 1.7749e-01,\n",
       "                    8.4470e-02, 4.7813e-02, 4.0303e-02, 4.0003e-02, 7.5130e-02, 9.7387e-02,\n",
       "                    7.6809e-02, 8.9027e-02, 4.1441e-02, 7.2080e-03, 2.7081e-01, 3.2244e-01,\n",
       "                    2.2922e-01, 2.1555e-01, 3.2053e-01, 2.0348e-01, 1.3790e-01, 8.8555e-02,\n",
       "                    1.2698e-01, 2.9101e-02, 7.6224e-02, 2.8891e-01, 2.1161e-01, 2.8903e-01,\n",
       "                    7.1563e-02, 2.9305e-02, 5.9278e-02, 1.4761e-01, 5.1350e-02, 1.8780e-01,\n",
       "                    1.6688e-01, 2.5503e-02, 3.8468e-02, 1.2899e-01, 4.3203e-03, 1.5623e-01,\n",
       "                    2.6077e-02, 9.2380e-02, 9.1855e-02, 3.4450e-01, 1.3742e-01, 5.9684e-02,\n",
       "                    8.7429e-02, 8.4190e-02, 4.8821e-03, 1.5526e-01, 6.9872e-02, 2.6824e-01,\n",
       "                    2.0389e-01, 2.4929e-01, 2.0740e-01, 9.6055e-02, 1.6450e-01, 2.4502e-01,\n",
       "                    3.4565e-02, 2.6984e-01, 2.2903e-01, 7.1205e-02, 8.3210e-02, 1.2821e-01,\n",
       "                    2.5299e-01, 7.1497e-02, 6.5964e-02, 2.9749e-01, 2.9303e-01, 8.9040e-02,\n",
       "                    1.4655e-01, 1.7935e-01, 1.1914e-01, 1.1478e-01, 1.5226e-01, 2.2208e-01,\n",
       "                    3.0857e-01, 1.8911e-01, 2.4367e-01, 1.2359e-01, 2.0447e-01, 2.2978e-01,\n",
       "                    1.3495e-01, 2.0627e-02, 1.1735e-01, 1.2242e-01, 2.2784e-01, 1.1694e-02,\n",
       "                    1.4339e-01, 6.8019e-02, 1.0726e-01, 1.1966e-01, 1.6700e-01, 5.8891e-02,\n",
       "                    1.6976e-01, 3.1100e-01, 3.7847e-01, 2.6319e-01, 8.3602e-02, 1.9382e-01,\n",
       "                    2.2318e-02, 6.8169e-02, 3.3686e-01, 1.1405e-01, 4.6442e-01, 2.0056e-02,\n",
       "                    1.2260e-01, 3.1095e-02, 2.5569e-01, 6.1952e-02, 8.1387e-02, 2.0911e-01,\n",
       "                    2.1919e-01, 3.6910e-02, 6.4362e-02, 2.8882e-01, 1.7601e-01, 6.0114e-02,\n",
       "                    1.5173e-01, 3.5142e-01, 2.1276e-01, 5.8468e-02, 3.7593e-02, 4.8932e-03,\n",
       "                    6.0799e-02, 1.0355e-01, 4.7803e-02, 2.4181e-02, 6.8529e-02, 1.8027e-01,\n",
       "                    5.6725e-02, 1.0135e-01, 1.0781e-01, 2.1163e-01, 1.5152e-01, 1.0589e-01,\n",
       "                    5.9950e-02, 1.3502e-01, 4.9241e-02, 1.3425e-01, 2.5013e-01, 3.2498e-02,\n",
       "                    2.6155e-01, 9.4562e-02, 3.3962e-01, 7.3780e-02, 1.5811e-01, 3.0691e-01,\n",
       "                    2.6642e-01, 2.6300e-02, 2.9175e-02, 1.8254e-01, 5.2493e-02, 3.9750e-02,\n",
       "                    1.7319e-02, 6.9785e-02, 8.4347e-02, 1.6217e-01, 5.7914e-02, 1.3685e-01,\n",
       "                    5.9982e-02, 4.9773e-02, 5.3717e-02, 1.6975e-01, 5.8520e-02, 3.5591e-01,\n",
       "                    7.1855e-02, 4.2340e-02, 1.9860e-01, 1.9847e-01, 1.8558e-02, 9.6676e-02,\n",
       "                    3.3871e-02, 1.9364e-01, 5.3104e-02, 9.4425e-02, 1.7506e-01, 6.1287e-02,\n",
       "                    2.1144e-01, 3.3133e-01, 2.4861e-01, 1.1545e-01, 7.0860e-02, 2.0123e-01,\n",
       "                    4.7163e-02, 2.9659e-01, 3.9866e-02, 1.1046e-01, 8.4973e-02, 2.4641e-02,\n",
       "                    9.3174e-02, 3.1027e-01, 7.0603e-02, 4.9673e-02, 5.3616e-02, 8.7668e-02,\n",
       "                    1.4344e-01, 6.5525e-02, 9.8865e-02, 6.7638e-02, 2.4299e-01, 7.2453e-02,\n",
       "                    1.3615e-01, 6.8725e-02, 1.8258e-01, 1.9900e-01, 2.3908e-01, 2.5648e-01,\n",
       "                    4.1417e-02, 8.6779e-02, 1.3426e-01, 6.1415e-02, 5.9023e-02, 1.0859e-01,\n",
       "                    4.4502e-01, 1.7900e-01, 2.8705e-01, 3.5604e-02, 1.0961e-01, 1.0188e-01,\n",
       "                    3.0469e-02, 1.1928e-01, 1.0955e-01, 1.1992e-01, 1.5318e-01, 5.4330e-02,\n",
       "                    4.4951e-02, 1.1621e-01, 2.1365e-01, 3.9608e-01, 2.4301e-01, 8.8921e-02,\n",
       "                    2.7662e-01, 4.6753e-02, 5.0444e-02, 1.4573e-01, 1.4571e-01, 3.7019e-01,\n",
       "                    1.0515e-01, 2.0833e-01, 1.4758e-01, 1.5531e-01, 1.3259e-01, 1.3647e-01,\n",
       "                    2.3204e-01, 6.2877e-02, 1.2137e-01, 4.0056e-02, 1.7496e-01, 2.6755e-01,\n",
       "                    1.7690e-01, 7.2113e-02, 5.5549e-02, 1.4317e-01, 6.2798e-02, 6.5730e-02,\n",
       "                    1.6986e-01, 2.9245e-01, 1.6167e-01, 2.2487e-01, 3.3055e-02, 1.1165e-01,\n",
       "                    1.5697e-01, 2.5959e-01, 2.2513e-01, 2.5288e-01, 4.1028e-01, 9.0775e-02,\n",
       "                    1.0977e-01, 1.2575e-01, 1.0483e-01, 9.0701e-02, 7.8092e-02, 1.0646e-01,\n",
       "                    2.1172e-01, 1.5162e-01, 1.3548e-01, 7.7797e-02, 3.1813e-02, 1.8620e-01,\n",
       "                    1.0349e-01, 4.2141e-02, 6.0170e-03, 2.9887e-01, 5.0266e-02, 2.4376e-01,\n",
       "                    3.1195e-01, 1.8158e-01, 4.5815e-01, 3.0760e-01, 3.2738e-01, 1.2571e-01,\n",
       "                    8.0927e-02, 1.3436e-01, 2.1211e-02, 3.3128e-02, 4.2799e-01, 2.1072e-01,\n",
       "                    6.5911e-02, 1.3216e-01, 2.5739e-02, 1.2829e-01, 2.2121e-01, 3.2815e-02,\n",
       "                    6.7292e-02, 2.9688e-01, 3.9221e-02, 3.8727e-02, 4.8415e-02, 6.9188e-02,\n",
       "                    1.4452e-01, 9.8425e-02, 1.8574e-01, 5.6613e-02, 1.4008e-01, 1.2262e-01,\n",
       "                    1.5735e-01, 6.8382e-02, 2.1613e-01, 3.0248e-02, 9.9036e-02, 2.0897e-01,\n",
       "                    1.5064e-01, 2.6353e-01, 7.4327e-02, 1.4209e-01, 9.9929e-02, 2.5594e-01,\n",
       "                    3.9935e-02, 8.2558e-02, 4.0819e-02, 2.0006e-01, 5.3526e-01, 4.6939e-02,\n",
       "                    1.0680e-01, 1.9406e-01, 1.1687e-01, 6.5103e-02, 7.3253e-02, 2.9140e-01,\n",
       "                    9.7010e-02, 1.0469e-01, 1.6991e-01, 1.4129e-01, 1.3933e-01, 6.7152e-02,\n",
       "                    1.8733e-01, 1.3434e-01, 2.0953e-01, 8.1949e-02, 1.8580e-01, 1.9894e-01,\n",
       "                    2.1334e-02, 1.3718e-01, 1.3011e-01, 9.8328e-02, 3.4495e-01, 1.6773e-01,\n",
       "                    5.5762e-02, 1.0759e-01], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "              min_val=tensor([-5.4708e-02, -1.4324e-02, -1.6922e-01, -1.2557e-02, -1.1783e-01,\n",
       "                      -3.8090e-02, -6.6496e-02, -9.9677e-02, -7.5905e-02, -7.2891e-02,\n",
       "                      -4.3758e-02, -4.8784e-03, -8.5791e-02, -1.5236e-01, -3.2635e-02,\n",
       "                      -1.5238e-02, -1.8702e-01, -1.0574e-01, -3.0109e-02, -1.5859e-02,\n",
       "                      -1.2127e-01, -8.9233e-02, -6.0286e-02, -3.6876e-02, -1.1185e-01,\n",
       "                      -7.5583e-02, -2.6358e-02, -2.1922e-02, -9.8595e-02, -1.2235e-01,\n",
       "                      -8.5401e-02, -2.9248e-01, -1.1212e-01, -3.1792e-02, -9.0072e-02,\n",
       "                      -2.6252e-01, -6.2883e-02, -5.6151e-02, -1.5567e-01, -2.2617e-01,\n",
       "                      -3.3652e-02, -1.6404e-01, -1.2877e-04, -8.9298e-02, -2.6545e-01,\n",
       "                      -9.7111e-03, -8.8967e-02, -3.9463e-02, -2.1587e-02, -6.9005e-02,\n",
       "                      -5.4076e-02, -3.6245e-01, -6.0474e-02, -3.7608e-01, -1.6414e-01,\n",
       "                      -2.0879e-01, -1.4647e-02, -4.1003e-02, -2.1179e-02, -2.8409e-01,\n",
       "                      -1.1170e-01, -6.4927e-02, -2.6090e-01, -1.5484e-01, -8.7388e-02,\n",
       "                      -3.1748e-03, -4.7578e-02, -4.0300e-02, -2.2569e-01, -3.9388e-02,\n",
       "                      -4.8045e-02, -3.7710e-02, -2.5502e-01, -2.4032e-01, -1.3576e-02,\n",
       "                      -7.0239e-02, -5.1877e-03, -3.2374e-01, -7.1373e-02, -2.4621e-02,\n",
       "                      -1.8494e-01, -1.2361e-01, -2.7508e-02, -6.7882e-02, -1.2716e-01,\n",
       "                      -1.6416e-01, -1.0555e-01, -2.4981e-02, -1.3319e-01, -1.0575e-01,\n",
       "                      -1.0542e-01, -2.1250e-01, -4.8139e-02, -1.1317e-01, -4.4998e-02,\n",
       "                      -1.1831e-02, -2.5646e-01, -4.6526e-02, -2.2260e-01, -5.1664e-02,\n",
       "                      -1.0031e-01, -4.5698e-02, -1.1874e-01, -1.7991e-01, -2.4035e-02,\n",
       "                      -1.8017e-01, -6.8478e-02, -5.2351e-02, -3.5657e-01, -3.0844e-01,\n",
       "                      -2.5518e-02, -2.4656e-01, -4.1648e-02, -1.3342e-01, -6.1862e-02,\n",
       "                      -1.4068e-01, -2.6929e-01, -4.1406e-02, -5.2605e-02, -8.3738e-03,\n",
       "                      -1.7725e-01, -3.6239e-02, -2.3878e-01, -8.0004e-03, -1.3482e-02,\n",
       "                      -1.4102e-01, -8.1067e-02, -4.8240e-02, -1.1537e-01, -1.3401e-02,\n",
       "                      -1.0398e-01, -4.9272e-02, -2.1136e-01, -5.1909e-02, -8.0120e-02,\n",
       "                      -2.5742e-02, -8.8730e-02, -7.7296e-02, -1.4180e-02, -6.7242e-02,\n",
       "                      -2.6436e-01, -7.9024e-02, -1.2316e-02, -3.0812e-02, -6.3493e-02,\n",
       "                      -2.1018e-01, -8.2743e-02, -2.0901e-01, -3.0424e-01, -4.7242e-02,\n",
       "                      -8.7395e-03, -1.4848e-01, -3.2978e-01, -1.2761e-01, -4.4191e-02,\n",
       "                      -1.2817e-01, -3.5182e-02, -5.0245e-02, -4.5558e-02, -1.4487e-02,\n",
       "                      -4.6172e-02, -3.7414e-02, -3.9975e-02, -4.1196e-02, -1.7169e-02,\n",
       "                      -1.0068e-02, -1.2551e-01, -1.2908e-01, -2.3067e-01, -9.9356e-02,\n",
       "                      -1.9643e-01, -1.7940e-01, -9.0061e-02, -7.6274e-02, -1.8116e-01,\n",
       "                      -4.2177e-02, -1.1178e-02, -5.2700e-02, -2.6013e-01, -1.1659e-01,\n",
       "                      -4.8055e-03, -1.9382e-02, -5.3599e-02, -2.1239e-01, -3.3236e-02,\n",
       "                      -1.8478e-01, -1.3434e-01, -3.7954e-02, -1.5588e-02, -7.6906e-02,\n",
       "                      -3.3489e-03, -7.6505e-02, -1.2563e-02, -6.8632e-02, -7.4015e-02,\n",
       "                      -1.6253e-01, -1.0550e-01, -1.0727e-01, -7.3443e-02, -5.3727e-02,\n",
       "                      -6.6180e-03, -2.0335e-01, -1.5501e-01, -1.5600e-01, -3.1904e-01,\n",
       "                      -1.8165e-01, -7.1360e-02, -1.2790e-02, -1.2333e-01, -1.7970e-01,\n",
       "                      -2.4818e-02, -2.4331e-01, -1.3353e-01, -3.1411e-02, -2.2920e-02,\n",
       "                      -8.0427e-02, -2.1185e-01, -9.4859e-02, -8.8178e-02, -1.5194e-01,\n",
       "                      -3.0784e-01, -4.8265e-02, -1.6265e-01, -4.4882e-02, -1.2783e-01,\n",
       "                      -1.7211e-01, -1.2864e-01, -2.3790e-01, -2.1303e-01, -9.5396e-02,\n",
       "                      -3.8014e-01, -1.0205e-01, -2.0363e-01, -7.7364e-02, -2.6939e-01,\n",
       "                      -1.5483e-02, -9.1856e-02, -2.1863e-01, -9.3280e-02, -2.1616e-02,\n",
       "                      -1.9524e-01, -5.2396e-02, -4.8777e-02, -6.9394e-02, -8.4926e-02,\n",
       "                      -7.2650e-02, -1.3475e-01, -1.5304e-01, -2.0735e-01, -1.5722e-01,\n",
       "                      -5.0292e-02, -2.0944e-01, -3.1969e-02, -5.1540e-02, -1.5108e-01,\n",
       "                      -8.0766e-02, -1.9276e-01, -2.3731e-02, -1.1199e-01, -3.6624e-02,\n",
       "                      -1.5502e-01, -1.9389e-02, -4.5974e-02, -1.7069e-01, -1.3213e-01,\n",
       "                      -2.5382e-02, -7.0658e-02, -2.5183e-01, -1.0504e-01, -1.0930e-01,\n",
       "                      -8.4299e-02, -1.6195e-01, -9.4886e-02, -1.5040e-02, -8.8144e-03,\n",
       "                      -2.5026e-04, -5.5866e-02, -8.8609e-02, -4.6230e-02, -4.3271e-02,\n",
       "                      -1.0356e-01, -9.9697e-02, -8.2611e-02, -1.5106e-01, -9.7471e-02,\n",
       "                      -4.8792e-02, -1.5705e-01, -9.4869e-02, -5.5841e-02, -9.1202e-02,\n",
       "                      -4.3684e-02, -7.0744e-02, -2.3140e-01, -3.6937e-02, -3.2032e-01,\n",
       "                      -4.0294e-02, -1.7678e-01, -1.3210e-01, -1.5294e-01, -1.6206e-01,\n",
       "                      -1.7707e-01, -1.6629e-02, -4.1632e-02, -7.1090e-02, -2.3719e-02,\n",
       "                      -1.5502e-02, -2.2040e-02, -3.9136e-02, -1.3305e-02, -9.6573e-02,\n",
       "                      -1.9731e-02, -9.1460e-02, -1.8153e-02, -9.2537e-03, -3.7398e-02,\n",
       "                      -1.1295e-01, -5.8645e-02, -2.0746e-01, -4.1559e-02, -2.6914e-02,\n",
       "                      -1.9779e-01, -1.1835e-01, -2.9900e-02, -2.2516e-02, -1.7081e-02,\n",
       "                      -2.1133e-01, -4.2774e-02, -6.7810e-02, -3.6341e-01, -6.8805e-02,\n",
       "                      -1.4891e-01, -1.3244e-01, -2.2255e-01, -8.0870e-02, -3.2149e-02,\n",
       "                      -2.4201e-01, -5.5926e-02, -2.2853e-01, -1.3024e-02, -1.9184e-03,\n",
       "                      -6.5038e-02, -1.0321e-02, -1.2090e-02, -1.8998e-01, -1.9388e-02,\n",
       "                      -1.6625e-01, -5.8133e-02, -6.8987e-02, -1.5095e-01, -5.9590e-02,\n",
       "                      -1.1009e-02, -2.7829e-02, -1.2830e-01, -3.7584e-02, -3.5337e-02,\n",
       "                      -6.8151e-02, -3.3804e-01, -2.5311e-01, -1.9410e-01, -2.4835e-01,\n",
       "                      -6.1672e-02, -1.0231e-01, -1.0601e-01, -1.4552e-01, -1.4152e-02,\n",
       "                      -4.0794e-02, -1.6371e-01, -2.4925e-01, -2.9213e-01, -4.6708e-02,\n",
       "                      -1.0842e-01, -2.5442e-01, -2.4087e-02, -1.7847e-01, -7.6386e-02,\n",
       "                      -9.4667e-02, -2.1205e-01, -7.0892e-02, -4.6566e-02, -8.0424e-02,\n",
       "                      -1.7370e-01, -1.3002e-01, -9.7972e-02, -1.7794e-01, -2.2895e-01,\n",
       "                      -1.9638e-02, -8.9316e-03, -2.3637e-01, -7.6852e-02, -2.7021e-01,\n",
       "                      -4.6234e-02, -2.4825e-01, -2.4770e-01, -9.2857e-02, -6.7366e-02,\n",
       "                      -5.6543e-02, -1.2101e-01, -6.9331e-02, -4.2341e-02, -1.8864e-02,\n",
       "                      -1.9233e-01, -2.0910e-01, -1.9819e-01, -6.1488e-03, -3.7931e-02,\n",
       "                      -1.4015e-01, -5.3390e-02, -2.5140e-02, -4.6510e-01, -6.3077e-02,\n",
       "                      -6.8755e-02, -1.4941e-01, -6.4998e-04, -1.1617e-01, -2.2842e-01,\n",
       "                      -1.0142e-01, -6.2067e-02, -1.9788e-01, -1.8188e-01, -3.6383e-02,\n",
       "                      -5.4327e-02, -3.1547e-01, -7.7701e-02, -2.6809e-02, -4.6081e-02,\n",
       "                      -9.0986e-03, -2.9510e-01, -9.2775e-02, -7.5083e-02, -3.5450e-02,\n",
       "                      -2.3343e-02, -1.4853e-01, -7.0423e-02, -3.4169e-02, -4.3400e-03,\n",
       "                      -1.1862e-01, -1.7838e-02, -1.7156e-01, -1.2920e-01, -8.1925e-02,\n",
       "                      -1.8400e-01, -1.4063e-01, -1.8320e-01, -7.5500e-02, -4.5341e-02,\n",
       "                      -3.8976e-02, -3.0221e-02, -4.4532e-02, -1.2150e-01, -1.8149e-01,\n",
       "                      -1.1890e-01, -1.9813e-01, -1.3010e-02, -1.1359e-01, -1.0171e-01,\n",
       "                      -2.4817e-02, -5.6789e-03, -1.7289e-01, -2.5351e-02, -2.5388e-02,\n",
       "                      -2.8218e-02, -1.0731e-01, -1.2736e-01, -7.8345e-02, -2.2269e-01,\n",
       "                      -6.1365e-02, -8.1792e-02, -1.3069e-01, -1.2291e-01, -6.4499e-02,\n",
       "                      -2.4306e-01, -5.6532e-02, -5.2635e-02, -1.3019e-01, -6.8496e-02,\n",
       "                      -3.5004e-01, -6.4242e-02, -3.5182e-03, -1.1655e-01, -1.8472e-01,\n",
       "                      -5.2908e-02, -9.4189e-02, -3.2297e-02, -1.9436e-01, -1.2440e-01,\n",
       "                      -5.2090e-02, -6.8583e-02, -1.8243e-01, -1.2097e-01, -1.3418e-02,\n",
       "                      -3.1251e-02, -2.3692e-01, -6.6766e-02, -4.0870e-02, -3.7771e-01,\n",
       "                      -1.2313e-01, -1.4533e-01, -3.7880e-02, -2.8674e-01, -1.8347e-01,\n",
       "                      -1.0889e-01, -5.6237e-02, -6.4056e-02, -2.3712e-01, -1.4122e-02,\n",
       "                      -8.8994e-02, -6.8299e-02, -7.6422e-02, -7.0709e-02, -1.0880e-01,\n",
       "                      -3.9120e-02, -1.1008e-01], device='cuda:0'), max_val=tensor([3.5980e-02, 2.2921e-02, 2.1145e-01, 2.9582e-02, 1.2787e-01, 3.5325e-02,\n",
       "                      8.3755e-02, 2.0926e-01, 1.2197e-01, 1.7525e-01, 4.5809e-02, 5.9014e-03,\n",
       "                      1.3523e-01, 3.4770e-01, 4.7321e-02, 2.6452e-02, 1.9700e-01, 1.6488e-01,\n",
       "                      5.0453e-02, 1.8611e-02, 2.4582e-01, 1.0858e-01, 1.0592e-01, 3.6375e-02,\n",
       "                      4.1666e-01, 1.4740e-01, 2.2670e-02, 5.0359e-02, 1.2286e-01, 1.1895e-01,\n",
       "                      6.6471e-02, 4.7306e-01, 1.5033e-01, 2.9456e-02, 1.6747e-01, 3.1404e-01,\n",
       "                      9.0489e-02, 8.1959e-02, 1.2456e-01, 2.7850e-01, 3.3279e-02, 2.3222e-01,\n",
       "                      1.2916e-04, 1.3877e-01, 7.1181e-01, 2.2623e-02, 1.1526e-01, 3.7711e-02,\n",
       "                      4.6812e-02, 1.0787e-01, 1.4651e-01, 5.0111e-01, 8.7979e-02, 2.3618e-01,\n",
       "                      3.8099e-01, 1.8337e-01, 2.4222e-02, 7.8221e-02, 3.8688e-02, 5.8530e-01,\n",
       "                      1.7757e-01, 7.7897e-02, 2.6125e-01, 1.8640e-01, 9.6100e-02, 4.6144e-03,\n",
       "                      7.3982e-02, 4.6813e-02, 2.6132e-01, 6.2712e-02, 5.8779e-02, 7.9592e-02,\n",
       "                      5.2075e-01, 2.3144e-01, 1.2725e-02, 7.3967e-02, 1.0392e-02, 8.4013e-01,\n",
       "                      1.2934e-01, 3.3229e-02, 3.0543e-01, 8.7552e-02, 3.2702e-02, 1.5866e-01,\n",
       "                      1.8163e-01, 3.1994e-01, 1.2384e-01, 2.2671e-02, 2.6992e-01, 1.5003e-01,\n",
       "                      1.5740e-01, 3.7305e-01, 7.2421e-02, 1.9059e-01, 8.0038e-02, 8.9395e-03,\n",
       "                      3.3464e-01, 4.5422e-02, 3.5852e-01, 4.2201e-02, 1.1659e-01, 9.7597e-02,\n",
       "                      1.1089e-01, 1.7449e-01, 4.5757e-02, 2.3852e-01, 9.1938e-02, 1.7735e-01,\n",
       "                      3.6707e-01, 3.3330e-01, 4.2343e-02, 2.8942e-01, 2.9208e-02, 1.5046e-01,\n",
       "                      1.1968e-01, 2.1320e-01, 3.3788e-01, 7.5325e-02, 6.7307e-02, 1.4254e-02,\n",
       "                      2.9239e-01, 3.1450e-02, 4.9767e-01, 1.4414e-02, 1.3129e-02, 2.5790e-01,\n",
       "                      8.2054e-02, 5.7571e-02, 1.6737e-01, 1.4581e-02, 1.6940e-01, 6.5296e-02,\n",
       "                      2.5145e-01, 9.9983e-02, 8.2462e-02, 4.7418e-02, 1.1062e-01, 1.3651e-01,\n",
       "                      1.5020e-02, 1.0500e-01, 2.4262e-01, 1.7838e-01, 1.2978e-02, 2.7274e-02,\n",
       "                      1.4416e-01, 2.3728e-01, 1.0825e-01, 2.8050e-01, 2.1978e-01, 7.6332e-02,\n",
       "                      1.1385e-02, 2.3369e-01, 2.7047e-01, 1.8082e-01, 4.6840e-02, 1.4402e-01,\n",
       "                      7.1573e-02, 7.4798e-02, 2.7795e-02, 1.5068e-02, 5.2271e-02, 4.9222e-02,\n",
       "                      6.1294e-02, 8.6467e-02, 3.3069e-02, 1.6348e-02, 1.3395e-01, 2.6517e-01,\n",
       "                      5.7993e-01, 1.2631e-01, 1.8198e-01, 1.5438e-01, 1.1311e-01, 1.0978e-01,\n",
       "                      2.2753e-01, 5.8761e-02, 1.8871e-02, 6.2290e-02, 3.8774e-01, 1.7549e-01,\n",
       "                      6.2709e-03, 6.1746e-02, 7.8210e-02, 3.8013e-01, 4.2336e-02, 2.7165e-01,\n",
       "                      1.8050e-01, 4.5414e-02, 3.0863e-02, 9.1888e-02, 4.3382e-03, 1.4877e-01,\n",
       "                      2.5582e-02, 6.9990e-02, 9.5508e-02, 2.7363e-01, 2.2254e-01, 1.5770e-01,\n",
       "                      7.3491e-02, 6.4371e-02, 5.1380e-03, 2.4521e-01, 1.1674e-01, 2.0952e-01,\n",
       "                      5.8740e-01, 4.6475e-01, 1.7511e-01, 1.9426e-02, 1.7182e-01, 2.8199e-01,\n",
       "                      4.1027e-02, 2.0669e-01, 1.4836e-01, 3.0293e-02, 2.6501e-02, 1.3557e-01,\n",
       "                      3.3475e-01, 1.0601e-01, 7.6537e-02, 1.3570e-01, 2.2538e-01, 5.4389e-02,\n",
       "                      4.8763e-01, 9.2882e-02, 1.5827e-01, 3.5006e-01, 1.9561e-01, 1.9794e-01,\n",
       "                      3.2727e-01, 2.3654e-01, 4.9769e-01, 1.3963e-01, 3.0548e-01, 1.1488e-01,\n",
       "                      6.4141e-01, 3.3551e-02, 1.2410e-01, 3.4044e-01, 1.5115e-01, 2.7481e-02,\n",
       "                      4.3680e-01, 5.2374e-02, 8.8699e-02, 1.2789e-01, 1.4828e-01, 1.8862e-01,\n",
       "                      4.0422e-01, 2.1190e-01, 3.1130e-01, 1.7203e-01, 1.2357e-01, 2.6279e-01,\n",
       "                      5.2673e-02, 6.4267e-02, 1.8696e-01, 9.4762e-02, 2.2709e-01, 2.1677e-02,\n",
       "                      1.9711e-01, 3.6270e-02, 1.8814e-01, 2.9435e-02, 5.1044e-02, 1.2212e-01,\n",
       "                      2.3363e-01, 3.2327e-02, 8.4277e-02, 3.2442e-01, 1.8705e-01, 1.7353e-01,\n",
       "                      1.0099e-01, 2.4635e-01, 1.0177e-01, 5.2159e-02, 1.5012e-02, 2.2522e-04,\n",
       "                      5.9276e-02, 1.2743e-01, 3.5803e-02, 5.7279e-02, 9.1548e-02, 1.1005e-01,\n",
       "                      1.3348e-01, 3.1212e-01, 1.5276e-01, 5.1474e-02, 2.4629e-01, 1.3975e-01,\n",
       "                      7.9161e-02, 1.5632e-01, 6.5372e-02, 9.8335e-02, 3.0284e-01, 4.5997e-02,\n",
       "                      4.8766e-01, 8.6222e-02, 2.0567e-01, 1.4511e-01, 1.5145e-01, 1.8130e-01,\n",
       "                      1.7131e-01, 2.9773e-02, 4.8657e-02, 1.1368e-01, 4.0307e-02, 1.5454e-02,\n",
       "                      2.2256e-02, 4.4961e-02, 2.4383e-02, 1.5416e-01, 2.2923e-02, 5.4032e-02,\n",
       "                      5.3083e-02, 1.1345e-02, 4.4804e-02, 2.2533e-01, 7.0318e-02, 3.7798e-01,\n",
       "                      7.0901e-02, 2.7322e-02, 2.2547e-01, 1.5195e-01, 2.2392e-02, 1.8926e-02,\n",
       "                      3.3064e-02, 3.5757e-01, 6.1225e-02, 1.3323e-01, 4.6790e-01, 9.6653e-02,\n",
       "                      1.2344e-01, 1.6065e-01, 2.8322e-01, 1.2407e-01, 3.7718e-02, 1.7359e-01,\n",
       "                      6.6920e-02, 2.4076e-01, 2.7849e-02, 1.5713e-03, 9.6584e-02, 2.2875e-02,\n",
       "                      1.4345e-02, 2.8024e-01, 2.0887e-02, 2.6828e-01, 9.9655e-02, 7.7155e-02,\n",
       "                      2.0567e-01, 7.0203e-02, 1.7970e-02, 3.9357e-02, 1.4779e-01, 8.3577e-02,\n",
       "                      1.3012e-01, 1.0843e-01, 4.4490e-01, 2.0147e-01, 4.1891e-01, 2.1326e-01,\n",
       "                      6.9573e-02, 1.3127e-01, 1.3179e-01, 2.6724e-01, 2.3386e-02, 7.4589e-02,\n",
       "                      2.3562e-01, 3.3411e-01, 4.6704e-01, 6.1480e-02, 1.9244e-01, 2.5709e-01,\n",
       "                      4.9816e-02, 1.4124e-01, 4.9062e-02, 1.5724e-01, 2.3673e-01, 1.5203e-01,\n",
       "                      6.5641e-02, 1.0020e-01, 3.3160e-01, 2.1986e-01, 2.0186e-01, 1.8132e-01,\n",
       "                      2.3876e-01, 3.6973e-02, 1.3619e-02, 3.5616e-01, 1.0814e-01, 5.6535e-01,\n",
       "                      4.4686e-02, 1.7489e-01, 3.0673e-01, 1.0998e-01, 9.2345e-02, 1.2203e-01,\n",
       "                      1.8099e-01, 7.3747e-02, 7.3594e-02, 3.3465e-02, 2.0036e-01, 2.0954e-01,\n",
       "                      1.9265e-01, 6.6800e-03, 4.6647e-02, 2.4024e-01, 7.5216e-02, 4.9727e-02,\n",
       "                      2.8194e-01, 1.4711e-01, 8.3628e-02, 1.8275e-01, 4.3760e-04, 1.6321e-01,\n",
       "                      3.7262e-01, 1.5706e-01, 1.1956e-01, 3.2650e-01, 2.9791e-01, 3.3252e-02,\n",
       "                      1.3809e-01, 3.5822e-01, 7.1554e-02, 4.4618e-02, 4.3403e-02, 1.0332e-02,\n",
       "                      2.4346e-01, 9.7095e-02, 9.4488e-02, 3.6650e-02, 3.8583e-02, 1.9100e-01,\n",
       "                      1.2272e-01, 5.6109e-02, 3.0336e-03, 1.9588e-01, 3.6098e-02, 4.4351e-01,\n",
       "                      1.9998e-01, 2.8464e-01, 2.1850e-01, 2.0244e-01, 1.8802e-01, 1.1335e-01,\n",
       "                      3.5468e-02, 3.3853e-02, 5.4788e-02, 6.1393e-02, 1.3452e-01, 3.1025e-01,\n",
       "                      1.8885e-01, 4.7587e-01, 2.0205e-02, 1.3029e-01, 1.4247e-01, 3.9589e-02,\n",
       "                      6.3824e-03, 1.9799e-01, 8.6769e-02, 4.6333e-02, 6.7891e-02, 1.1047e-01,\n",
       "                      1.9075e-01, 1.2163e-01, 2.4685e-01, 8.7375e-02, 1.2117e-01, 7.6237e-02,\n",
       "                      2.7133e-01, 1.4137e-01, 1.8910e-01, 5.5876e-02, 7.0997e-02, 2.1144e-01,\n",
       "                      8.0290e-02, 5.7418e-01, 4.5179e-02, 7.4053e-03, 1.7431e-01, 2.0189e-01,\n",
       "                      4.8188e-02, 1.1883e-01, 6.2058e-02, 1.3835e-01, 1.4401e-01, 7.6232e-02,\n",
       "                      9.7642e-02, 2.8043e-01, 1.7943e-01, 2.3553e-02, 4.9222e-02, 3.4270e-01,\n",
       "                      1.1641e-01, 3.6746e-02, 2.6107e-01, 1.2980e-01, 2.2522e-01, 5.5962e-02,\n",
       "                      3.4208e-01, 2.2629e-01, 1.7418e-01, 8.5535e-02, 1.1420e-01, 3.4023e-01,\n",
       "                      2.3373e-02, 1.2520e-01, 1.9195e-01, 9.3304e-02, 1.2489e-01, 1.3150e-01,\n",
       "                      6.1432e-02, 1.5402e-01], device='cuda:0')\n",
       "            )\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.0070, -0.0053, -0.0047, -0.0055, -0.0024, -0.0053, -0.0066, -0.0231,\n",
       "                    -0.0036, -0.0040, -0.0036, -0.0031, -0.0045, -0.0012, -0.0024, -0.0031,\n",
       "                    -0.0031, -0.0036, -0.0065, -0.0048, -0.0053, -0.0020, -0.0045, -0.0046,\n",
       "                    -0.0023, -0.0057, -0.0028, -0.0046, -0.0079, -0.0012, -0.0028, -0.0091,\n",
       "                    -0.0071, -0.0022, -0.0019, -0.0066, -0.0030, -0.0032, -0.0074, -0.0024,\n",
       "                    -0.0079, -0.0055, -0.0034, -0.0026, -0.0029, -0.0023, -0.0026, -0.0041,\n",
       "                    -0.0023, -0.0030, -0.0025, -0.0025, -0.0032, -0.0106, -0.0063, -0.0031,\n",
       "                    -0.0025, -0.0032, -0.0073, -0.0032, -0.0020, -0.0030, -0.0062, -0.0012,\n",
       "                    -0.0022, -0.0087, -0.0025, -0.0034, -0.0023, -0.0040, -0.0061, -0.0022,\n",
       "                    -0.0109, -0.0064, -0.0014, -0.0035, -0.0035, -0.0029, -0.0030, -0.0033,\n",
       "                    -0.0020, -0.0037, -0.0030, -0.0015, -0.0059, -0.0019, -0.0057, -0.0019,\n",
       "                    -0.0039, -0.0029, -0.0022, -0.0040, -0.0033, -0.0054, -0.0028, -0.0045,\n",
       "                    -0.0030, -0.0030, -0.0058, -0.0010, -0.0018, -0.0032, -0.0080, -0.0079,\n",
       "                    -0.0027, -0.0037, -0.0137, -0.0041, -0.0054, -0.0035, -0.0028, -0.0027,\n",
       "                    -0.0027, -0.0039, -0.0016, -0.0029, -0.0025, -0.0094, -0.0031, -0.0024,\n",
       "                    -0.0027, -0.0029, -0.0022, -0.0029, -0.0020, -0.0041, -0.0020, -0.0039,\n",
       "                    -0.0054, -0.0020, -0.0040, -0.0076, -0.0019, -0.0022, -0.0027, -0.0037,\n",
       "                    -0.0123, -0.0022, -0.0043, -0.0037, -0.0111, -0.0015, -0.0024, -0.0090,\n",
       "                    -0.0039, -0.0028, -0.0022, -0.0063, -0.0091, -0.0027, -0.0062, -0.0035,\n",
       "                    -0.0025, -0.0015, -0.0034, -0.0032, -0.0026, -0.0029, -0.0029, -0.0030,\n",
       "                    -0.0042, -0.0052, -0.0018, -0.0110, -0.0014, -0.0059, -0.0022, -0.0025,\n",
       "                    -0.0037, -0.0015, -0.0023, -0.0037, -0.0062, -0.0020, -0.0033, -0.0016,\n",
       "                    -0.0032, -0.0027, -0.0026, -0.0022, -0.0055, -0.0038, -0.0025, -0.0016,\n",
       "                    -0.0028, -0.0021, -0.0044, -0.0018, -0.0023, -0.0015, -0.0007, -0.0015,\n",
       "                    -0.0098, -0.0021, -0.0034, -0.0056, -0.0176, -0.0029, -0.0081, -0.0032,\n",
       "                    -0.0164, -0.0019, -0.0058, -0.0070, -0.0027, -0.0035, -0.0013, -0.0034,\n",
       "                    -0.0029, -0.0035, -0.0027, -0.0026, -0.0041, -0.0063, -0.0020, -0.0023,\n",
       "                    -0.0035, -0.0052, -0.0043, -0.0050, -0.0038, -0.0024, -0.0077, -0.0059,\n",
       "                    -0.0040, -0.0066, -0.0017, -0.0030, -0.0016, -0.0029, -0.0047, -0.0076,\n",
       "                    -0.0022, -0.0015, -0.0037, -0.0115, -0.0029, -0.0042, -0.0027, -0.0018,\n",
       "                    -0.0041, -0.0043, -0.0019, -0.0011, -0.0022, -0.0030, -0.0029, -0.0059,\n",
       "                    -0.0052, -0.0025, -0.0028, -0.0034, -0.0032, -0.0038, -0.0054, -0.0059,\n",
       "                    -0.0054, -0.0033, -0.0039, -0.0025, -0.0068, -0.0075, -0.0067, -0.0033,\n",
       "                    -0.0025, -0.0032, -0.0056, -0.0034, -0.0027, -0.0030, -0.0009, -0.0032,\n",
       "                    -0.0018, -0.0087, -0.0040, -0.0080, -0.0028, -0.0020, -0.0042, -0.0018,\n",
       "                    -0.0026, -0.0037, -0.0020, -0.0012, -0.0030, -0.0044, -0.0187, -0.0119,\n",
       "                    -0.0049, -0.0018, -0.0019, -0.0046, -0.0029, -0.0021, -0.0014, -0.0029,\n",
       "                    -0.0033, -0.0008, -0.0031, -0.0025, -0.0022, -0.0023, -0.0054, -0.0023,\n",
       "                    -0.0028, -0.0034, -0.0036, -0.0026, -0.0027, -0.0040, -0.0025, -0.0054,\n",
       "                    -0.0040, -0.0035, -0.0022, -0.0030, -0.0049, -0.0028, -0.0029, -0.0033,\n",
       "                    -0.0029, -0.0030, -0.0037, -0.0026, -0.0038, -0.0023, -0.0028, -0.0041,\n",
       "                    -0.0016, -0.0070, -0.0016, -0.0030, -0.0034, -0.0019, -0.0043, -0.0063,\n",
       "                    -0.0031, -0.0029, -0.0029, -0.0042, -0.0046, -0.0019, -0.0037, -0.0031,\n",
       "                    -0.0034, -0.0017, -0.0036, -0.0042, -0.0050, -0.0105, -0.0027, -0.0024,\n",
       "                    -0.0024, -0.0033, -0.0044, -0.0026, -0.0134, -0.0024, -0.0016, -0.0016,\n",
       "                    -0.0060, -0.0026, -0.0029, -0.0036, -0.0031, -0.0039, -0.0032, -0.0023,\n",
       "                    -0.0007, -0.0028, -0.0045, -0.0021, -0.0068, -0.0020, -0.0011, -0.0022,\n",
       "                    -0.0067, -0.0033, -0.0076, -0.0015, -0.0101, -0.0019, -0.0055, -0.0032,\n",
       "                    -0.0026, -0.0024, -0.0029, -0.0026, -0.0009, -0.0012, -0.0021, -0.0015,\n",
       "                    -0.0038, -0.0031, -0.0060, -0.0031, -0.0019, -0.0052, -0.0030, -0.0063,\n",
       "                    -0.0019, -0.0036, -0.0027, -0.0103, -0.0019, -0.0081, -0.0260, -0.0019,\n",
       "                    -0.0084, -0.0043, -0.0046, -0.0038, -0.0014, -0.0076, -0.0056, -0.0011,\n",
       "                    -0.0035, -0.0055, -0.0048, -0.0087, -0.0151, -0.0073, -0.0036, -0.0036,\n",
       "                    -0.0052, -0.0063, -0.0023, -0.0034, -0.0037, -0.0041, -0.0027, -0.0034,\n",
       "                    -0.0027, -0.0071, -0.0087, -0.0039, -0.0021, -0.0041, -0.0010, -0.0024,\n",
       "                    -0.0024, -0.0048, -0.0035, -0.0031, -0.0036, -0.0023, -0.0025, -0.0040,\n",
       "                    -0.0061, -0.0033, -0.0017, -0.0012, -0.0039, -0.0069, -0.0045, -0.0022,\n",
       "                    -0.0029, -0.0023, -0.0023, -0.0029, -0.0058, -0.0028, -0.0033, -0.0026,\n",
       "                    -0.0019, -0.0025, -0.0021, -0.0032, -0.0035, -0.0071, -0.0042, -0.0021,\n",
       "                    -0.0024, -0.0086, -0.0064, -0.0093, -0.0020, -0.0027, -0.0035, -0.0014,\n",
       "                    -0.0063, -0.0068, -0.0051, -0.0044, -0.0046, -0.0024, -0.0031, -0.0023,\n",
       "                    -0.0047, -0.0038, -0.0016, -0.0029, -0.0028, -0.0043, -0.0031, -0.0026,\n",
       "                    -0.0059, -0.0027, -0.0023, -0.0027, -0.0016, -0.0035, -0.0129, -0.0019,\n",
       "                    -0.0015, -0.0065, -0.0036, -0.0036, -0.0058, -0.0043, -0.0033, -0.0020],\n",
       "                   device='cuda:0'), max_val=tensor([0.0040, 0.0037, 0.0060, 0.0024, 0.0015, 0.0036, 0.0085, 0.0094, 0.0035,\n",
       "                    0.0066, 0.0044, 0.0047, 0.0041, 0.0023, 0.0036, 0.0063, 0.0048, 0.0040,\n",
       "                    0.0047, 0.0039, 0.0051, 0.0041, 0.0086, 0.0026, 0.0041, 0.0033, 0.0048,\n",
       "                    0.0040, 0.0052, 0.0006, 0.0056, 0.0038, 0.0048, 0.0056, 0.0032, 0.0038,\n",
       "                    0.0023, 0.0056, 0.0064, 0.0020, 0.0069, 0.0022, 0.0066, 0.0018, 0.0038,\n",
       "                    0.0033, 0.0055, 0.0050, 0.0061, 0.0044, 0.0044, 0.0060, 0.0067, 0.0085,\n",
       "                    0.0036, 0.0065, 0.0040, 0.0052, 0.0071, 0.0021, 0.0012, 0.0058, 0.0023,\n",
       "                    0.0009, 0.0031, 0.0041, 0.0032, 0.0086, 0.0025, 0.0085, 0.0043, 0.0059,\n",
       "                    0.0091, 0.0033, 0.0009, 0.0047, 0.0075, 0.0036, 0.0069, 0.0076, 0.0032,\n",
       "                    0.0067, 0.0047, 0.0038, 0.0036, 0.0012, 0.0041, 0.0032, 0.0044, 0.0060,\n",
       "                    0.0034, 0.0062, 0.0016, 0.0060, 0.0071, 0.0026, 0.0053, 0.0071, 0.0067,\n",
       "                    0.0007, 0.0027, 0.0068, 0.0040, 0.0063, 0.0033, 0.0057, 0.0082, 0.0070,\n",
       "                    0.0105, 0.0043, 0.0051, 0.0013, 0.0027, 0.0062, 0.0013, 0.0064, 0.0044,\n",
       "                    0.0055, 0.0036, 0.0053, 0.0009, 0.0049, 0.0045, 0.0037, 0.0052, 0.0048,\n",
       "                    0.0028, 0.0062, 0.0045, 0.0038, 0.0021, 0.0037, 0.0043, 0.0027, 0.0033,\n",
       "                    0.0022, 0.0052, 0.0029, 0.0024, 0.0027, 0.0067, 0.0020, 0.0025, 0.0058,\n",
       "                    0.0060, 0.0059, 0.0049, 0.0158, 0.0082, 0.0019, 0.0036, 0.0072, 0.0040,\n",
       "                    0.0030, 0.0070, 0.0052, 0.0051, 0.0044, 0.0022, 0.0026, 0.0044, 0.0028,\n",
       "                    0.0042, 0.0063, 0.0018, 0.0037, 0.0025, 0.0061, 0.0053, 0.0026, 0.0014,\n",
       "                    0.0074, 0.0092, 0.0040, 0.0043, 0.0028, 0.0037, 0.0051, 0.0063, 0.0028,\n",
       "                    0.0068, 0.0057, 0.0051, 0.0037, 0.0033, 0.0045, 0.0045, 0.0048, 0.0044,\n",
       "                    0.0014, 0.0006, 0.0011, 0.0098, 0.0035, 0.0043, 0.0035, 0.0091, 0.0024,\n",
       "                    0.0060, 0.0049, 0.0078, 0.0020, 0.0086, 0.0060, 0.0075, 0.0054, 0.0023,\n",
       "                    0.0056, 0.0057, 0.0041, 0.0010, 0.0037, 0.0035, 0.0033, 0.0041, 0.0046,\n",
       "                    0.0051, 0.0028, 0.0047, 0.0074, 0.0042, 0.0039, 0.0032, 0.0066, 0.0070,\n",
       "                    0.0032, 0.0037, 0.0062, 0.0030, 0.0058, 0.0022, 0.0057, 0.0046, 0.0030,\n",
       "                    0.0092, 0.0103, 0.0039, 0.0056, 0.0055, 0.0035, 0.0066, 0.0023, 0.0048,\n",
       "                    0.0014, 0.0045, 0.0059, 0.0041, 0.0018, 0.0100, 0.0050, 0.0037, 0.0040,\n",
       "                    0.0031, 0.0076, 0.0038, 0.0043, 0.0037, 0.0050, 0.0070, 0.0023, 0.0037,\n",
       "                    0.0061, 0.0036, 0.0017, 0.0030, 0.0058, 0.0032, 0.0062, 0.0051, 0.0023,\n",
       "                    0.0020, 0.0056, 0.0041, 0.0057, 0.0044, 0.0032, 0.0047, 0.0038, 0.0054,\n",
       "                    0.0025, 0.0036, 0.0055, 0.0035, 0.0005, 0.0012, 0.0055, 0.0088, 0.0077,\n",
       "                    0.0031, 0.0046, 0.0011, 0.0090, 0.0056, 0.0036, 0.0035, 0.0041, 0.0059,\n",
       "                    0.0006, 0.0057, 0.0058, 0.0044, 0.0045, 0.0025, 0.0038, 0.0048, 0.0080,\n",
       "                    0.0046, 0.0019, 0.0041, 0.0031, 0.0044, 0.0025, 0.0022, 0.0053, 0.0050,\n",
       "                    0.0049, 0.0020, 0.0072, 0.0022, 0.0042, 0.0061, 0.0058, 0.0046, 0.0010,\n",
       "                    0.0029, 0.0040, 0.0045, 0.0021, 0.0018, 0.0059, 0.0031, 0.0063, 0.0037,\n",
       "                    0.0042, 0.0075, 0.0082, 0.0025, 0.0060, 0.0038, 0.0031, 0.0054, 0.0069,\n",
       "                    0.0051, 0.0021, 0.0065, 0.0024, 0.0048, 0.0051, 0.0025, 0.0051, 0.0050,\n",
       "                    0.0028, 0.0012, 0.0046, 0.0065, 0.0038, 0.0099, 0.0041, 0.0011, 0.0035,\n",
       "                    0.0080, 0.0031, 0.0044, 0.0080, 0.0026, 0.0026, 0.0037, 0.0040, 0.0017,\n",
       "                    0.0047, 0.0035, 0.0015, 0.0060, 0.0008, 0.0006, 0.0032, 0.0036, 0.0057,\n",
       "                    0.0043, 0.0026, 0.0053, 0.0044, 0.0024, 0.0085, 0.0049, 0.0034, 0.0084,\n",
       "                    0.0041, 0.0008, 0.0023, 0.0027, 0.0031, 0.0040, 0.0098, 0.0027, 0.0041,\n",
       "                    0.0028, 0.0044, 0.0059, 0.0068, 0.0033, 0.0055, 0.0014, 0.0062, 0.0014,\n",
       "                    0.0033, 0.0117, 0.0019, 0.0033, 0.0025, 0.0047, 0.0052, 0.0026, 0.0031,\n",
       "                    0.0057, 0.0023, 0.0055, 0.0031, 0.0049, 0.0040, 0.0118, 0.0042, 0.0051,\n",
       "                    0.0021, 0.0032, 0.0024, 0.0028, 0.0079, 0.0048, 0.0039, 0.0034, 0.0057,\n",
       "                    0.0066, 0.0039, 0.0032, 0.0080, 0.0024, 0.0070, 0.0005, 0.0054, 0.0036,\n",
       "                    0.0055, 0.0057, 0.0060, 0.0043, 0.0048, 0.0059, 0.0055, 0.0023, 0.0071,\n",
       "                    0.0030, 0.0012, 0.0045, 0.0030, 0.0111, 0.0026, 0.0032, 0.0015, 0.0035,\n",
       "                    0.0013, 0.0026, 0.0057, 0.0068, 0.0035, 0.0025, 0.0079, 0.0013, 0.0066,\n",
       "                    0.0022, 0.0031, 0.0108, 0.0031, 0.0043, 0.0052, 0.0112, 0.0054, 0.0031,\n",
       "                    0.0025, 0.0039, 0.0018, 0.0042, 0.0024, 0.0032, 0.0021, 0.0022, 0.0052,\n",
       "                    0.0047, 0.0026, 0.0090, 0.0043, 0.0027, 0.0055, 0.0034, 0.0063, 0.0050,\n",
       "                    0.0011, 0.0037, 0.0037, 0.0033, 0.0063, 0.0027, 0.0051, 0.0106, 0.0037,\n",
       "                    0.0013, 0.0068, 0.0036, 0.0066, 0.0081, 0.0023, 0.0084, 0.0040],\n",
       "                   device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "            min_val=tensor([-8.2336e-02, -1.0802e-02, -6.7493e-03, -4.4764e-03, -6.4318e-04,\n",
       "                    -1.5397e-02, -2.1780e-02, -1.2516e-02, -4.1677e-03, -1.6603e-02,\n",
       "                    -2.7699e-02, -9.3762e-02, -1.4146e-02, -4.1975e-02, -1.2440e-02,\n",
       "                    -4.7260e-02, -1.4458e-03, -1.8499e-03, -3.8426e-05, -1.4998e-02,\n",
       "                    -1.1819e-02, -4.0273e-03, -2.7789e-02, -6.8135e-02, -5.5805e-02,\n",
       "                    -9.1318e-03, -1.2453e-02, -1.0253e-02, -1.1141e-03, -9.0357e-03,\n",
       "                    -9.5337e-03, -2.2305e-02, -9.9975e-03, -1.0380e-02, -2.5033e-02,\n",
       "                    -1.4233e-01, -1.0965e-01, -1.8634e-02, -1.4098e-02, -4.1856e-02,\n",
       "                    -1.2952e-02, -1.3842e-02, -1.8420e-04, -4.0118e-03, -4.3018e-03,\n",
       "                    -8.9245e-03, -7.5080e-02, -1.5139e-01, -8.7700e-02, -3.4130e-04,\n",
       "                    -3.9703e-02, -3.4230e-02, -1.0972e-02, -1.3249e-01, -2.4153e-04,\n",
       "                    -6.8159e-03, -7.2595e-03, -1.3448e-02, -5.0334e-03, -1.1062e-01,\n",
       "                    -3.7420e-03, -2.3727e-02, -3.0229e-03, -6.9426e-03, -1.3528e-02,\n",
       "                    -1.4941e-01, -5.8698e-03, -1.4937e-03, -7.8526e-03, -7.7294e-04,\n",
       "                    -1.9348e-02, -7.2454e-03, -1.5201e-02, -9.5702e-02, -5.6138e-02,\n",
       "                    -1.4702e-03, -7.4427e-03, -2.3945e-02, -7.8428e-03, -2.3716e-03,\n",
       "                    -1.3780e-01, -6.9925e-04, -1.5528e-02, -3.1366e-03, -4.6413e-03,\n",
       "                    -2.3102e-02, -8.9705e-03, -1.5873e-03, -8.2440e-03, -6.2223e-03,\n",
       "                    -2.5506e-02, -1.3242e-02, -5.1353e-04, -2.1791e-02, -5.1493e-03,\n",
       "                    -1.3812e-02, -5.5537e-02, -3.8971e-03, -2.6355e-02, -6.0026e-04,\n",
       "                    -1.2032e-01, -1.8436e-02, -3.5175e-03, -1.2449e-03, -8.2145e-03,\n",
       "                    -1.1706e-02, -1.7685e-02, -1.9369e-05, -6.4401e-02, -5.5096e-02,\n",
       "                    -1.8162e-02, -6.4431e-02, -1.4657e-02, -7.5035e-03, -2.1136e-02,\n",
       "                    -7.6212e-03, -2.3666e-03, -2.0741e-02, -4.3106e-02, -2.0334e-02,\n",
       "                    -1.3511e-03, -4.1641e-02, -5.8296e-03, -5.8722e-03, -1.3659e-02,\n",
       "                    -1.0262e-03, -2.3093e-01, -9.8559e-04, -1.1263e-02, -1.1840e-02,\n",
       "                    -2.3662e-02, -2.2639e-02, -4.7069e-02, -4.6202e-02, -6.4699e-02,\n",
       "                    -2.6770e-02, -2.5324e-03, -5.4054e-03, -1.1344e-02, -5.4233e-02,\n",
       "                    -1.1799e-01, -4.3332e-02, -1.2100e-02, -2.9244e-03, -9.7896e-03,\n",
       "                    -1.4326e-02, -7.6670e-02, -1.1896e-01, -2.2324e-02, -3.9101e-03,\n",
       "                    -6.3596e-03, -1.2277e-02, -8.6278e-03, -1.0658e-02, -6.1263e-03,\n",
       "                    -5.5144e-02, -5.1011e-03, -5.9610e-03, -4.4050e-03, -9.9926e-03,\n",
       "                    -3.3358e-02, -8.3014e-02, -2.5348e-02, -3.6336e-02, -2.0595e-03,\n",
       "                    -1.6070e-01, -3.4287e-02, -3.3342e-02, -1.7895e-02, -5.9605e-03,\n",
       "                    -6.1645e-02, -7.1834e-02, -2.6489e-03, -1.2419e-01, -7.9349e-03,\n",
       "                    -2.0280e-02, -2.1668e-02, -2.3904e-02, -2.3222e-02, -1.3612e-01,\n",
       "                    -4.4466e-02, -7.3817e-03, -5.3068e-02, -3.3076e-02, -2.2362e-02,\n",
       "                    -6.2480e-03, -1.7699e-02, -9.7977e-03, -6.0473e-03, -3.2279e-02,\n",
       "                    -1.2864e-01, -2.8620e-02, -7.7231e-03, -3.8238e-02, -1.5951e-04,\n",
       "                    -2.2008e-01, -8.0962e-02, -1.1651e-03, -3.5651e-04, -8.7163e-03,\n",
       "                    -1.5586e-01, -6.7890e-04, -3.7423e-03, -3.0017e-02, -3.1545e-02,\n",
       "                    -8.1177e-02, -2.7691e-02, -1.5886e-02, -8.7552e-04, -1.0902e-02,\n",
       "                    -4.9745e-04, -5.1701e-03, -6.0708e-04, -3.6154e-03, -5.6115e-03,\n",
       "                    -1.3872e-01, -1.3833e+00, -1.0961e-01, -1.1299e-02, -4.3861e-02,\n",
       "                    -9.1164e-03, -7.8142e-03, -1.2276e-02, -7.8992e-02, -1.2661e-02,\n",
       "                    -2.0830e-02, -1.9543e-01, -9.9161e-02, -5.5406e-03, -4.9039e-03,\n",
       "                    -1.0357e-02, -5.1513e-04, -4.2819e-03, -4.0658e-04, -2.1597e-01,\n",
       "                    -7.5244e-03, -2.6700e-02, -8.0486e-04, -1.0840e-01, -9.0128e-04,\n",
       "                    -4.6820e-02, -9.5836e-03, -4.3903e-03, -2.7880e-02, -1.8501e-02,\n",
       "                    -7.5238e-02, -2.5470e-01, -1.1664e-01, -8.2424e-04, -2.0330e-03,\n",
       "                    -2.6927e-03, -1.4058e-02, -7.1503e-03, -5.6428e-03, -1.2972e-02,\n",
       "                    -2.1815e-04, -1.8598e-02, -8.8654e-03, -1.2339e-03, -8.5661e-03,\n",
       "                    -2.7714e-02, -6.7790e-03, -4.5411e-02, -1.2672e-02, -3.0964e-02,\n",
       "                    -8.7070e-03, -5.6181e-04, -3.0151e-02, -3.6034e-03, -4.0215e-03,\n",
       "                    -4.6291e-03, -1.0113e-01, -9.7702e-02, -9.4683e-03, -5.2712e-03,\n",
       "                    -8.1221e-02, -4.0699e-03, -3.4796e-02, -6.3273e-04, -1.0087e-03,\n",
       "                    -7.4004e-04, -7.5395e-04, -1.7152e-02, -4.4882e-03, -7.6439e-03,\n",
       "                    -3.4371e-02, -1.6668e-02, -1.2600e-02, -1.1197e-02, -6.3841e-03,\n",
       "                    -3.6064e-02, -3.4842e-03, -4.8746e-03, -3.6482e-02, -5.3867e-01,\n",
       "                    -5.5282e-04, -2.8024e-02, -3.1717e-03, -4.5533e-02, -1.9112e-02,\n",
       "                    -1.0871e-02, -7.3750e-03, -1.4554e-02, -3.4025e-02, -5.2906e-02,\n",
       "                    -1.1411e-02, -7.6240e-03, -2.3536e-01, -9.8434e-03, -9.3568e-03,\n",
       "                    -1.2739e-02, -8.6769e-03, -7.2744e-03, -2.1038e-01, -9.4826e-04,\n",
       "                    -2.6623e-02, -5.4289e-04, -8.3083e-02, -5.1545e-03, -1.0760e-02,\n",
       "                    -8.3318e-03, -1.6114e-02, -2.2210e-02, -1.5500e-02, -1.2198e-02,\n",
       "                    -7.6997e-02, -2.9458e-03, -6.6970e-03, -1.3538e-02, -1.0753e-04,\n",
       "                    -3.7667e-02, -1.1378e-02, -2.9531e-03, -3.0549e-02, -1.1129e-04,\n",
       "                    -7.4828e-03, -4.9550e-03, -2.1630e-02, -4.6721e-03, -7.5276e-03,\n",
       "                    -2.5417e-01, -5.7842e-03, -2.1495e-02, -5.4486e-02, -1.2600e-02,\n",
       "                    -6.6202e-01, -5.2108e-03, -7.1118e-03, -9.2880e-03, -1.5884e-02,\n",
       "                    -1.3396e-02, -8.6177e-03, -2.2135e-01, -4.2869e-03, -3.5963e-02,\n",
       "                    -1.8612e-02, -1.4015e-02, -2.8430e-02, -5.4103e-02, -8.4634e-03,\n",
       "                    -1.5704e-03, -2.4757e-03, -1.6242e-02, -9.4671e-03, -5.8682e-02,\n",
       "                    -3.1700e-02, -6.3469e-03, -1.7161e-02, -2.9911e+00, -9.1946e-03,\n",
       "                    -6.0171e-03, -2.0847e-03, -4.6555e-04, -2.9021e-03, -7.0074e-03,\n",
       "                    -9.6173e-04, -4.0558e-03, -1.0939e-02, -1.4122e-02, -4.2937e-04,\n",
       "                    -9.8475e-04, -2.5634e-03, -1.2241e-02, -5.3389e-04, -2.5745e-02,\n",
       "                    -2.7473e-03, -3.6052e-02, -2.6506e-02, -1.0572e-04, -5.7149e-02,\n",
       "                    -2.3793e-02, -2.3827e-02, -2.0078e-02, -3.6038e-02, -6.9897e-04,\n",
       "                    -2.9056e-02, -2.0563e-02, -1.5308e-02, -3.1487e-02, -4.4516e-03,\n",
       "                    -3.1114e-03, -1.3178e-02, -1.6051e-02, -1.1266e-02, -1.1065e-03,\n",
       "                    -1.3354e-01, -1.3209e-02, -9.8445e-03, -5.5913e-02, -8.9269e-02,\n",
       "                    -2.1962e-02, -2.2572e-02, -1.2183e-01, -5.3555e-03, -7.1265e-03,\n",
       "                    -2.4492e-02, -3.0220e-04, -1.3990e-02, -3.5044e-03, -1.1231e-02,\n",
       "                    -1.8179e-02, -8.8378e-03, -2.0010e-02, -1.5979e-02, -1.1763e-02,\n",
       "                    -1.1988e-02, -2.2069e-02, -1.1117e-02, -8.7290e-04, -2.5819e-02,\n",
       "                    -1.5943e-02, -3.4070e-02, -8.4551e-02, -2.2317e-04, -1.4677e-01,\n",
       "                    -1.9253e-01, -1.1426e-02, -3.7788e-02, -1.0086e-02, -1.2487e-01,\n",
       "                    -7.5136e-03, -8.7765e-02, -1.8874e-02, -8.7278e-02, -8.5182e-04,\n",
       "                    -1.0984e-03, -2.9334e-02, -1.4117e-01, -9.5688e-03, -3.1940e-03,\n",
       "                    -3.3382e-03, -2.1349e-03, -4.9083e-02, -1.0384e-02, -4.5554e-04,\n",
       "                    -2.4074e-02, -1.5484e-02, -2.1373e-02, -2.7401e-02, -8.2415e-03,\n",
       "                    -1.7777e-03, -3.0052e-04, -1.9370e-02, -1.6469e-02, -7.3883e-03,\n",
       "                    -1.0025e-01, -3.8756e-03, -4.5011e-03, -2.8700e-02, -2.4184e-02,\n",
       "                    -1.0102e-01, -2.2616e-02, -6.1231e-02, -7.2917e-03, -5.1887e-02,\n",
       "                    -2.8001e-02, -5.7102e-03, -1.4664e-02, -2.0002e-02, -2.8852e-02,\n",
       "                    -6.6007e-03, -1.2059e-04, -5.7318e-03, -4.0797e-02, -1.8210e-02,\n",
       "                    -2.7265e-02, -1.7570e-02, -2.8915e-02, -2.7353e-03, -3.2675e-03,\n",
       "                    -9.3542e-02, -1.1653e-02, -2.9224e-02, -2.3513e-03, -9.4008e-04,\n",
       "                    -1.0097e-03, -2.6909e-02, -1.6543e-04, -9.0056e-02, -2.3556e-02,\n",
       "                    -2.8476e-02, -4.9341e-03, -1.4157e-02, -3.1562e-01, -1.5501e-03,\n",
       "                    -2.9921e-04, -1.1754e-01, -4.3778e-02, -3.4154e-03, -5.3159e-03,\n",
       "                    -7.9529e-03, -8.0856e-03], device='cuda:0'), max_val=tensor([2.6478e-02, 7.5188e-03, 1.2786e-02, 4.5565e-03, 1.0864e-03, 1.3734e-02,\n",
       "                    7.6803e-03, 1.3268e-02, 3.6352e-03, 1.4624e-02, 2.9549e-02, 2.1231e-01,\n",
       "                    1.3223e-02, 2.0682e-02, 1.1517e-02, 4.9969e-02, 1.2190e-03, 2.3146e-03,\n",
       "                    3.3574e-05, 1.1090e-02, 1.9868e-02, 5.2213e-03, 4.0339e-02, 3.2087e-02,\n",
       "                    8.4964e-02, 1.8277e-02, 1.0775e-02, 1.1486e-02, 1.5063e-03, 9.7668e-03,\n",
       "                    6.4695e-03, 2.5242e-02, 1.4310e-02, 5.7303e-03, 1.5439e-02, 2.0277e-01,\n",
       "                    4.0369e-02, 1.9392e-02, 1.4773e-02, 1.6545e-02, 7.6080e-03, 3.5794e-02,\n",
       "                    2.4301e-04, 3.8258e-03, 4.7865e-03, 6.2580e-03, 8.0827e-02, 2.2080e-01,\n",
       "                    8.9601e-02, 4.1306e-04, 4.1263e-02, 2.8716e-02, 8.9265e-03, 9.8221e-02,\n",
       "                    3.3748e-04, 5.8445e-03, 5.2718e-03, 1.0942e-02, 7.1382e-03, 1.3972e-01,\n",
       "                    4.5509e-03, 1.3302e-02, 4.9382e-03, 7.7810e-03, 6.4186e-03, 3.4723e-01,\n",
       "                    3.4366e-03, 9.5662e-04, 1.8199e-02, 8.7262e-04, 9.3012e-03, 5.7608e-03,\n",
       "                    2.5703e-02, 7.3563e-02, 9.7212e-02, 9.3779e-04, 5.1261e-03, 2.2527e-02,\n",
       "                    6.3418e-03, 1.7457e-03, 9.7638e-02, 9.5263e-04, 1.1592e-02, 2.1159e-03,\n",
       "                    5.7187e-03, 1.8446e-02, 5.1191e-03, 2.2045e-03, 1.3659e-02, 3.4624e-03,\n",
       "                    2.9774e-02, 1.1825e-02, 4.0928e-04, 1.4140e-02, 5.1933e-03, 7.6659e-03,\n",
       "                    4.5713e-02, 3.2579e-03, 3.2548e-02, 3.4800e-04, 9.4698e-02, 1.4638e-02,\n",
       "                    3.1163e-03, 1.9698e-03, 7.6691e-03, 7.4879e-03, 1.2133e-02, 1.9063e-05,\n",
       "                    5.9860e-02, 5.7880e-02, 1.3785e-02, 6.5931e-02, 1.5617e-02, 5.7553e-03,\n",
       "                    1.0843e-02, 6.4150e-03, 1.8951e-03, 2.2145e-02, 1.4861e-02, 1.2988e-02,\n",
       "                    2.2079e-03, 2.3994e-02, 1.1381e-02, 4.3732e-03, 9.1810e-03, 1.4194e-03,\n",
       "                    5.6327e-02, 1.1076e-03, 1.3761e-02, 6.9691e-03, 3.3431e-02, 1.3782e-02,\n",
       "                    4.6409e-02, 2.4394e-02, 6.6386e-02, 2.8799e-02, 3.1980e-03, 5.8594e-03,\n",
       "                    6.8179e-03, 2.8230e-02, 1.0248e-01, 4.0889e-02, 7.6262e-03, 2.1983e-03,\n",
       "                    8.5693e-03, 1.6283e-02, 1.2257e-01, 1.2897e-01, 2.4254e-02, 4.0914e-03,\n",
       "                    6.3174e-03, 1.3663e-02, 4.8646e-03, 9.5479e-03, 5.0621e-03, 6.1930e-02,\n",
       "                    3.6463e-03, 2.5200e-03, 3.1660e-03, 6.5810e-03, 1.3112e-02, 1.8149e-02,\n",
       "                    1.5596e-02, 2.8044e-02, 1.9409e-03, 4.1484e-01, 3.3005e-02, 2.1117e-02,\n",
       "                    1.7002e-02, 4.9785e-03, 1.0111e-01, 3.2032e-02, 5.0429e-03, 1.8656e-01,\n",
       "                    9.4616e-03, 1.8344e-02, 1.8341e-02, 2.9674e-02, 1.0565e-02, 1.6528e-01,\n",
       "                    9.3018e-02, 4.9923e-03, 2.5960e-02, 1.3868e-02, 2.0144e-02, 5.2243e-03,\n",
       "                    1.3776e-02, 8.7045e-03, 4.1388e-03, 1.3856e-02, 3.4223e-01, 6.9635e-02,\n",
       "                    4.8041e-03, 2.4283e-02, 2.2023e-04, 1.7279e-01, 7.4951e-02, 1.4997e-03,\n",
       "                    2.7899e-04, 4.9819e-03, 3.0409e-01, 7.4824e-04, 5.0069e-03, 1.7337e-02,\n",
       "                    2.6564e-02, 4.6880e-02, 2.0103e-02, 1.0479e-02, 1.4441e-03, 7.5159e-03,\n",
       "                    5.6027e-04, 7.9111e-03, 5.8078e-04, 4.2262e-03, 4.4392e-03, 6.4694e-02,\n",
       "                    4.5256e-01, 1.7538e-01, 7.4227e-03, 2.6979e-02, 1.2920e-02, 4.4646e-03,\n",
       "                    6.0607e-03, 7.8621e-02, 1.6372e-02, 4.4093e-02, 2.1743e-01, 6.0761e-02,\n",
       "                    7.2290e-03, 3.7563e-03, 1.5552e-02, 4.0621e-04, 5.0746e-03, 3.3758e-04,\n",
       "                    1.6901e-01, 1.1721e-02, 2.5762e-02, 1.0635e-03, 1.8217e-01, 1.3344e-03,\n",
       "                    3.9580e-02, 7.9366e-03, 3.9156e-03, 3.6771e-02, 1.6188e-02, 4.0841e-02,\n",
       "                    1.2067e-01, 1.0982e-01, 7.1411e-04, 1.4143e-03, 2.5527e-03, 1.4696e-02,\n",
       "                    5.2404e-03, 4.4136e-03, 9.6620e-03, 1.3950e-04, 2.7405e-02, 9.3608e-03,\n",
       "                    7.7714e-04, 1.1321e-02, 1.7244e-02, 6.4682e-03, 4.8220e-02, 1.6698e-02,\n",
       "                    1.5903e-02, 1.0265e-02, 5.2860e-04, 2.5295e-02, 3.2403e-03, 6.3762e-03,\n",
       "                    4.3541e-03, 1.1578e-01, 6.3825e-02, 1.1919e-02, 4.5361e-03, 2.0221e-01,\n",
       "                    4.3343e-03, 4.5128e-02, 6.7749e-04, 1.5809e-03, 7.4705e-04, 8.9260e-04,\n",
       "                    9.0815e-03, 6.2277e-03, 5.7436e-03, 2.3157e-02, 2.0575e-02, 1.4833e-02,\n",
       "                    6.6399e-03, 6.0750e-03, 3.4164e-02, 2.2060e-03, 6.2939e-03, 2.4719e-02,\n",
       "                    2.5004e-01, 3.7783e-04, 4.1185e-02, 2.3655e-03, 6.5099e-02, 1.7923e-02,\n",
       "                    8.1787e-03, 7.8415e-03, 1.4721e-02, 1.6562e-02, 1.1146e-02, 9.9273e-03,\n",
       "                    1.3199e-02, 8.6363e-02, 1.2473e-02, 9.1299e-03, 1.1097e-02, 9.7694e-03,\n",
       "                    4.4597e-03, 4.1077e-01, 1.4558e-03, 2.3782e-02, 8.0873e-04, 5.0077e-02,\n",
       "                    3.8794e-03, 1.1946e-02, 1.6865e-02, 1.9578e-02, 1.0607e-02, 2.6505e-02,\n",
       "                    7.3645e-03, 7.3503e-02, 3.0916e-03, 1.3322e-02, 8.5036e-03, 1.4645e-04,\n",
       "                    4.2661e-02, 1.0202e-02, 1.8606e-03, 1.9850e-02, 1.5392e-04, 9.9221e-03,\n",
       "                    7.2924e-03, 2.8668e-02, 3.1362e-03, 6.5936e-03, 7.6523e-02, 4.7605e-03,\n",
       "                    2.1767e-02, 3.2330e-02, 7.1956e-03, 2.7870e-01, 3.6617e-03, 4.1147e-03,\n",
       "                    6.2391e-03, 1.5957e-02, 7.6085e-03, 9.5045e-03, 1.0114e-01, 3.0390e-03,\n",
       "                    3.3154e-02, 1.9608e-02, 1.4539e-02, 4.3101e-02, 3.8313e-02, 1.0058e-02,\n",
       "                    1.6136e-03, 1.9567e-03, 1.4612e-02, 1.2648e-02, 2.1214e-02, 3.2815e-02,\n",
       "                    8.7098e-03, 2.1619e-02, 9.3213e-01, 7.7519e-03, 6.3353e-03, 5.2323e-03,\n",
       "                    3.5387e-04, 3.1514e-03, 1.0685e-02, 1.8101e-03, 5.1949e-03, 9.1681e-03,\n",
       "                    7.5068e-03, 4.5564e-04, 7.6455e-04, 4.8956e-03, 1.4715e-02, 5.6141e-04,\n",
       "                    4.4490e-02, 2.3435e-03, 6.1211e-02, 4.3440e-02, 1.2708e-04, 2.6511e-02,\n",
       "                    7.4006e-03, 3.6854e-02, 1.2221e-02, 5.5542e-02, 5.4798e-04, 2.8773e-02,\n",
       "                    1.9386e-02, 1.8863e-02, 1.2137e-02, 2.5911e-03, 5.2029e-03, 1.0640e-02,\n",
       "                    2.1909e-02, 7.4183e-03, 1.2059e-03, 9.1888e-02, 1.1643e-02, 5.9015e-03,\n",
       "                    3.2812e-02, 1.0731e-01, 1.4500e-02, 3.8582e-02, 2.9905e-01, 5.0670e-03,\n",
       "                    9.4751e-03, 2.7015e-02, 4.6133e-04, 1.8618e-02, 4.5227e-03, 5.2892e-03,\n",
       "                    1.4081e-02, 9.0401e-03, 1.8108e-02, 1.8158e-02, 1.5674e-02, 7.8913e-03,\n",
       "                    1.5134e-02, 8.8543e-03, 7.9581e-04, 1.9711e-02, 1.1631e-02, 2.4141e-02,\n",
       "                    8.2761e-02, 2.0811e-04, 3.1560e-01, 2.1592e-01, 8.6182e-03, 8.1464e-02,\n",
       "                    7.3292e-03, 1.6965e-01, 1.1448e-02, 9.0755e-02, 1.6851e-02, 7.0932e-02,\n",
       "                    8.1424e-04, 5.4331e-04, 9.2141e-03, 4.0971e-02, 1.0134e-02, 4.2283e-03,\n",
       "                    1.8075e-03, 3.5872e-03, 9.8294e-02, 7.3160e-03, 5.7290e-04, 1.4696e-02,\n",
       "                    2.0119e-02, 2.2283e-02, 1.9188e-02, 7.5593e-03, 1.8416e-03, 1.6814e-04,\n",
       "                    1.1116e-02, 1.3501e-02, 4.9190e-03, 4.2043e-02, 3.7190e-03, 3.6892e-03,\n",
       "                    2.6620e-02, 3.5493e-02, 5.9542e-02, 1.3898e-02, 1.7007e-02, 1.1048e-02,\n",
       "                    3.4124e-02, 1.3123e-02, 6.0974e-03, 8.8271e-03, 2.3248e-02, 1.5419e-02,\n",
       "                    6.5100e-03, 1.8954e-04, 9.4803e-03, 3.5960e-02, 2.2283e-02, 1.6460e-02,\n",
       "                    1.7130e-02, 3.6169e-02, 3.7630e-03, 2.6435e-03, 1.5524e-01, 1.4453e-02,\n",
       "                    3.1198e-02, 2.8062e-03, 1.2552e-03, 1.2582e-03, 3.5742e-02, 2.3953e-04,\n",
       "                    6.8355e-02, 1.8282e-02, 1.2177e-02, 3.8786e-03, 2.0086e-02, 3.7733e-01,\n",
       "                    1.1241e-03, 2.9893e-04, 1.1170e-01, 4.5313e-02, 5.1204e-03, 7.0362e-03,\n",
       "                    6.0633e-03, 6.5585e-03], device='cuda:0')\n",
       "          )\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(\n",
       "      in_features=512, out_features=10, bias=True\n",
       "      (weight_fake_quant): PerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.4439, -0.3981, -0.4470, -0.4510, -0.4506, -0.4700, -0.5349, -0.4495,\n",
       "                -0.4932, -0.3722], device='cuda:0'), max_val=tensor([0.4115, 0.6245, 0.3466, 0.3969, 0.4739, 0.5003, 0.4885, 0.4887, 0.4440,\n",
       "                0.5191], device='cuda:0')\n",
       "      )\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. QAT를 하기 위한 quantization 모델\n",
    "# 8. CUDA 상태로 적용 후, QAT 모델 학습\n",
    "print(\"Training QAT Model...\")\n",
    "quantized_model.train()\n",
    "train_model(model=quantized_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-3, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 다시 CPU 상태로 두고 \n",
    "#    QAT가 적용된 floating point 모델을 quantized integer model로 변환    \n",
    "quantized_model.to(cpu_device)\n",
    "quantized_model = torch.quantization.convert(quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.eval()\n",
    "save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quantized model.\n",
    "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 evaluation accuracy: 0.874\n",
      "INT8 evaluation accuracy: 0.873\n",
      "FP32 CPU Inference Latency: 4.00 ms / sample\n",
      "FP32 CUDA Inference Latency: 3.48 ms / sample\n",
      "INT8 CPU Inference Latency: 3.11 ms / sample\n",
      "INT8 JIT CPU Inference Latency: 1.62 ms / sample\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
    "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
    "\n",
    "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
    "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
    "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
